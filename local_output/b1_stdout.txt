[08-12 15:58:59] (inity_circuit_breaker.py, line 522)=> ============================================================
[08-12 15:58:59] (inity_circuit_breaker.py, line 523)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[08-12 15:58:59] (inity_circuit_breaker.py, line 524)=> Circuit Breaker Alpha: 0.1
[08-12 15:58:59] (inity_circuit_breaker.py, line 525)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[08-12 15:58:59] (inity_circuit_breaker.py, line 526)=> Circuit Breaker Enabled: True
[08-12 15:58:59] (inity_circuit_breaker.py, line 527)=> Number of Examples: 1000
[08-12 15:58:59] (inity_circuit_breaker.py, line 528)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[08-12 15:58:59] (inity_circuit_breaker.py, line 529)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[08-12 15:58:59] (inity_circuit_breaker.py, line 530)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[08-12 15:58:59] (inity_circuit_breaker.py, line 531)=> Validation Ratio: 0.1
[08-12 15:58:59] (inity_circuit_breaker.py, line 532)=> Category: hate
[08-12 15:58:59] (inity_circuit_breaker.py, line 533)=> Batch Size: 4
[08-12 15:58:59] (inity_circuit_breaker.py, line 534)=> Workers: 4
[08-12 15:58:59] (inity_circuit_breaker.py, line 535)=> Device: cuda:0
[08-12 15:58:59] (inity_circuit_breaker.py, line 536)=> Model: infinity_/home/gs285/VAR/my_model/weights/infinity_8b_weights
[08-12 15:58:59] (inity_circuit_breaker.py, line 537)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-12 15:58:59] (inity_circuit_breaker.py, line 538)=> Rush Resume: 
[08-12 15:58:59] (inity_circuit_breaker.py, line 539)=> Selective Layers: 0,1,2,3,4,5
[08-12 15:58:59] (inity_circuit_breaker.py, line 540)=> ============================================================
[08-12 15:58:59] (inity_circuit_breaker.py, line 326)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[08-12 15:58:59] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-12 15:58:59] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-12 15:58:59] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[08-12 15:58:59] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-12 15:58:59] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-12 15:58:59] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[08-12 15:58:59] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[08-12 15:58:59] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[08-12 15:58:59] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[08-12 15:58:59] (inity_circuit_breaker.py, line 344)=> args.batch_size=4, vbs=6







=======================================================   RESTART [08-12 16:39:45]   =======================================================
[08-12 16:39:45] (inity_circuit_breaker.py, line 522)=> ============================================================
[08-12 16:39:45] (inity_circuit_breaker.py, line 523)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[08-12 16:39:45] (inity_circuit_breaker.py, line 524)=> Circuit Breaker Alpha: 0.1
[08-12 16:39:45] (inity_circuit_breaker.py, line 525)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[08-12 16:39:45] (inity_circuit_breaker.py, line 526)=> Circuit Breaker Enabled: True
[08-12 16:39:45] (inity_circuit_breaker.py, line 527)=> Number of Examples: 1000
[08-12 16:39:45] (inity_circuit_breaker.py, line 528)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[08-12 16:39:45] (inity_circuit_breaker.py, line 529)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[08-12 16:39:45] (inity_circuit_breaker.py, line 530)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[08-12 16:39:45] (inity_circuit_breaker.py, line 531)=> Validation Ratio: 0.1
[08-12 16:39:45] (inity_circuit_breaker.py, line 532)=> Category: hate
[08-12 16:39:45] (inity_circuit_breaker.py, line 533)=> Batch Size: 4
[08-12 16:39:45] (inity_circuit_breaker.py, line 534)=> Workers: 4
[08-12 16:39:45] (inity_circuit_breaker.py, line 535)=> Device: cuda:0
[08-12 16:39:45] (inity_circuit_breaker.py, line 536)=> Model: infinity_/home/gs285/VAR/my_model/weights/infinity_8b_weights
[08-12 16:39:45] (inity_circuit_breaker.py, line 537)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-12 16:39:45] (inity_circuit_breaker.py, line 538)=> Rush Resume: 
[08-12 16:39:45] (inity_circuit_breaker.py, line 539)=> Selective Layers: 0,1,2,3,4,5
[08-12 16:39:45] (inity_circuit_breaker.py, line 540)=> ============================================================
[08-12 16:39:45] (inity_circuit_breaker.py, line 326)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[08-12 16:39:45] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-12 16:39:45] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-12 16:39:45] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[08-12 16:39:45] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-12 16:39:45] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-12 16:39:45] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[08-12 16:39:45] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[08-12 16:39:45] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[08-12 16:39:45] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[08-12 16:39:45] (inity_circuit_breaker.py, line 344)=> args.batch_size=4, vbs=6







=======================================================   RESTART [08-12 16:44:19]   =======================================================
[08-12 16:44:19] (inity_circuit_breaker.py, line 526)=> ============================================================
[08-12 16:44:19] (inity_circuit_breaker.py, line 527)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[08-12 16:44:19] (inity_circuit_breaker.py, line 528)=> Circuit Breaker Alpha: 0.1
[08-12 16:44:19] (inity_circuit_breaker.py, line 529)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[08-12 16:44:19] (inity_circuit_breaker.py, line 530)=> Circuit Breaker Enabled: True
[08-12 16:44:19] (inity_circuit_breaker.py, line 531)=> Number of Examples: 1000
[08-12 16:44:19] (inity_circuit_breaker.py, line 532)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[08-12 16:44:19] (inity_circuit_breaker.py, line 533)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[08-12 16:44:19] (inity_circuit_breaker.py, line 534)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[08-12 16:44:19] (inity_circuit_breaker.py, line 535)=> Validation Ratio: 0.1
[08-12 16:44:19] (inity_circuit_breaker.py, line 536)=> Category: hate
[08-12 16:44:19] (inity_circuit_breaker.py, line 537)=> Batch Size: 4
[08-12 16:44:19] (inity_circuit_breaker.py, line 538)=> Workers: 4
[08-12 16:44:19] (inity_circuit_breaker.py, line 539)=> Device: cuda:0
[08-12 16:44:19] (inity_circuit_breaker.py, line 540)=> Model: infinity_/home/gs285/VAR/my_model/weights/infinity_8b_weights
[08-12 16:44:19] (inity_circuit_breaker.py, line 541)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-12 16:44:19] (inity_circuit_breaker.py, line 542)=> Rush Resume: 
[08-12 16:44:19] (inity_circuit_breaker.py, line 543)=> Selective Layers: 0,1,2,3,4,5
[08-12 16:44:19] (inity_circuit_breaker.py, line 544)=> ============================================================
[08-12 16:44:19] (inity_circuit_breaker.py, line 326)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[08-12 16:44:19] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-12 16:44:19] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-12 16:44:19] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[08-12 16:44:19] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-12 16:44:19] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-12 16:44:20] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[08-12 16:44:20] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[08-12 16:44:20] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[08-12 16:44:20] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[08-12 16:44:20] (inity_circuit_breaker.py, line 344)=> args.batch_size=4, vbs=6
[08-12 16:44:20] (inity_circuit_breaker.py, line 367)=> len(dataloader): 100, len(dataset): 100
[08-12 16:44:20] (inity_circuit_breaker.py, line 369)=> total_samples: 200
[08-12 16:44:20] (inity_circuit_breaker.py, line 370)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[08-12 16:44:20] (inity_circuit_breaker.py, line 100)=> train_h_div_w_list=['1:1', '4:3', '3:4', '16:9', '9:16']
[08-12 16:44:20] (inity_circuit_breaker.py, line 104)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth







=======================================================   RESTART [08-12 16:47:34]   =======================================================
[08-12 16:47:34] (inity_circuit_breaker.py, line 526)=> ============================================================
[08-12 16:47:34] (inity_circuit_breaker.py, line 527)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[08-12 16:47:34] (inity_circuit_breaker.py, line 528)=> Circuit Breaker Alpha: 0.1
[08-12 16:47:34] (inity_circuit_breaker.py, line 529)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[08-12 16:47:34] (inity_circuit_breaker.py, line 530)=> Circuit Breaker Enabled: True
[08-12 16:47:34] (inity_circuit_breaker.py, line 531)=> Number of Examples: 1000
[08-12 16:47:34] (inity_circuit_breaker.py, line 532)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[08-12 16:47:34] (inity_circuit_breaker.py, line 533)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[08-12 16:47:34] (inity_circuit_breaker.py, line 534)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[08-12 16:47:34] (inity_circuit_breaker.py, line 535)=> Validation Ratio: 0.1
[08-12 16:47:34] (inity_circuit_breaker.py, line 536)=> Category: hate
[08-12 16:47:34] (inity_circuit_breaker.py, line 537)=> Batch Size: 4
[08-12 16:47:34] (inity_circuit_breaker.py, line 538)=> Workers: 4
[08-12 16:47:34] (inity_circuit_breaker.py, line 539)=> Device: cuda:0
[08-12 16:47:34] (inity_circuit_breaker.py, line 540)=> Model: infinity_/home/gs285/VAR/my_model/weights/infinity_8b_weights
[08-12 16:47:34] (inity_circuit_breaker.py, line 541)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-12 16:47:34] (inity_circuit_breaker.py, line 542)=> Rush Resume: 
[08-12 16:47:34] (inity_circuit_breaker.py, line 543)=> Selective Layers: 0,1,2,3,4,5
[08-12 16:47:34] (inity_circuit_breaker.py, line 544)=> ============================================================
[08-12 16:47:34] (inity_circuit_breaker.py, line 326)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[08-12 16:47:34] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-12 16:47:34] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-12 16:47:34] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[08-12 16:47:34] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-12 16:47:34] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-12 16:47:35] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[08-12 16:47:35] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[08-12 16:47:35] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[08-12 16:47:35] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[08-12 16:47:35] (inity_circuit_breaker.py, line 344)=> args.batch_size=4, vbs=6
[08-12 16:47:35] (inity_circuit_breaker.py, line 367)=> len(dataloader): 100, len(dataset): 100
[08-12 16:47:35] (inity_circuit_breaker.py, line 369)=> total_samples: 200
[08-12 16:47:35] (inity_circuit_breaker.py, line 370)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[08-12 16:47:35] (inity_circuit_breaker.py, line 100)=> train_h_div_w_list=['1:1', '4:3', '3:4', '16:9', '9:16']
[08-12 16:47:35] (inity_circuit_breaker.py, line 104)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth







=======================================================   RESTART [08-12 16:48:05]   =======================================================
[08-12 16:48:05] (inity_circuit_breaker.py, line 526)=> ============================================================
[08-12 16:48:05] (inity_circuit_breaker.py, line 527)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[08-12 16:48:05] (inity_circuit_breaker.py, line 528)=> Circuit Breaker Alpha: 0.1
[08-12 16:48:05] (inity_circuit_breaker.py, line 529)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[08-12 16:48:05] (inity_circuit_breaker.py, line 530)=> Circuit Breaker Enabled: True
[08-12 16:48:05] (inity_circuit_breaker.py, line 531)=> Number of Examples: 1000
[08-12 16:48:05] (inity_circuit_breaker.py, line 532)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[08-12 16:48:05] (inity_circuit_breaker.py, line 533)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[08-12 16:48:05] (inity_circuit_breaker.py, line 534)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[08-12 16:48:05] (inity_circuit_breaker.py, line 535)=> Validation Ratio: 0.1
[08-12 16:48:05] (inity_circuit_breaker.py, line 536)=> Category: hate
[08-12 16:48:05] (inity_circuit_breaker.py, line 537)=> Batch Size: 4
[08-12 16:48:05] (inity_circuit_breaker.py, line 538)=> Workers: 4
[08-12 16:48:05] (inity_circuit_breaker.py, line 539)=> Device: cuda:0
[08-12 16:48:05] (inity_circuit_breaker.py, line 540)=> Model: infinity_/home/gs285/VAR/my_model/weights/infinity_8b_weights
[08-12 16:48:05] (inity_circuit_breaker.py, line 541)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-12 16:48:05] (inity_circuit_breaker.py, line 542)=> Rush Resume: 
[08-12 16:48:05] (inity_circuit_breaker.py, line 543)=> Selective Layers: 0,1,2,3,4,5
[08-12 16:48:05] (inity_circuit_breaker.py, line 544)=> ============================================================
[08-12 16:48:05] (inity_circuit_breaker.py, line 326)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[08-12 16:48:05] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-12 16:48:05] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-12 16:48:05] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[08-12 16:48:05] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-12 16:48:05] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-12 16:48:06] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[08-12 16:48:06] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[08-12 16:48:06] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[08-12 16:48:06] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[08-12 16:48:06] (inity_circuit_breaker.py, line 344)=> args.batch_size=4, vbs=6
[08-12 16:48:06] (inity_circuit_breaker.py, line 367)=> len(dataloader): 100, len(dataset): 100
[08-12 16:48:06] (inity_circuit_breaker.py, line 369)=> total_samples: 200
[08-12 16:48:06] (inity_circuit_breaker.py, line 370)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[08-12 16:48:06] (inity_circuit_breaker.py, line 100)=> train_h_div_w_list=['1:1', '4:3', '3:4', '16:9', '9:16']
[08-12 16:48:06] (inity_circuit_breaker.py, line 104)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth







=======================================================   RESTART [08-12 16:50:18]   =======================================================
[08-12 16:50:18] (inity_circuit_breaker.py, line 526)=> ============================================================
[08-12 16:50:18] (inity_circuit_breaker.py, line 527)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[08-12 16:50:18] (inity_circuit_breaker.py, line 528)=> Circuit Breaker Alpha: 0.1
[08-12 16:50:18] (inity_circuit_breaker.py, line 529)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[08-12 16:50:18] (inity_circuit_breaker.py, line 530)=> Circuit Breaker Enabled: True
[08-12 16:50:18] (inity_circuit_breaker.py, line 531)=> Number of Examples: 1000
[08-12 16:50:18] (inity_circuit_breaker.py, line 532)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[08-12 16:50:18] (inity_circuit_breaker.py, line 533)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[08-12 16:50:18] (inity_circuit_breaker.py, line 534)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[08-12 16:50:18] (inity_circuit_breaker.py, line 535)=> Validation Ratio: 0.1
[08-12 16:50:18] (inity_circuit_breaker.py, line 536)=> Category: hate
[08-12 16:50:18] (inity_circuit_breaker.py, line 537)=> Batch Size: 4
[08-12 16:50:18] (inity_circuit_breaker.py, line 538)=> Workers: 4
[08-12 16:50:18] (inity_circuit_breaker.py, line 539)=> Device: cuda:0
[08-12 16:50:18] (inity_circuit_breaker.py, line 540)=> Model: infinity_/home/gs285/VAR/my_model/weights/infinity_8b_weights
[08-12 16:50:18] (inity_circuit_breaker.py, line 541)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-12 16:50:18] (inity_circuit_breaker.py, line 542)=> Rush Resume: 
[08-12 16:50:18] (inity_circuit_breaker.py, line 543)=> Selective Layers: 0,1,2,3,4,5
[08-12 16:50:18] (inity_circuit_breaker.py, line 544)=> ============================================================
[08-12 16:50:18] (inity_circuit_breaker.py, line 326)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[08-12 16:50:18] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-12 16:50:18] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-12 16:50:18] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[08-12 16:50:18] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-12 16:50:18] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-12 16:50:19] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[08-12 16:50:19] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[08-12 16:50:19] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[08-12 16:50:19] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[08-12 16:50:19] (inity_circuit_breaker.py, line 344)=> args.batch_size=4, vbs=6
[08-12 16:50:19] (inity_circuit_breaker.py, line 367)=> len(dataloader): 100, len(dataset): 100
[08-12 16:50:19] (inity_circuit_breaker.py, line 369)=> total_samples: 200
[08-12 16:50:19] (inity_circuit_breaker.py, line 370)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[08-12 16:50:19] (inity_circuit_breaker.py, line 100)=> train_h_div_w_list=['1:1', '4:3', '3:4', '16:9', '9:16']
[08-12 16:50:19] (inity_circuit_breaker.py, line 104)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth







=======================================================   RESTART [08-19 09:25:21]   =======================================================
[08-19 09:25:21] (inity_circuit_breaker.py, line 526)=> ============================================================
[08-19 09:25:21] (inity_circuit_breaker.py, line 527)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[08-19 09:25:21] (inity_circuit_breaker.py, line 528)=> Circuit Breaker Alpha: 0.1
[08-19 09:25:21] (inity_circuit_breaker.py, line 529)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[08-19 09:25:21] (inity_circuit_breaker.py, line 530)=> Circuit Breaker Enabled: True
[08-19 09:25:21] (inity_circuit_breaker.py, line 531)=> Number of Examples: 1000
[08-19 09:25:21] (inity_circuit_breaker.py, line 532)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[08-19 09:25:21] (inity_circuit_breaker.py, line 533)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[08-19 09:25:21] (inity_circuit_breaker.py, line 534)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[08-19 09:25:21] (inity_circuit_breaker.py, line 535)=> Validation Ratio: 0.1
[08-19 09:25:21] (inity_circuit_breaker.py, line 536)=> Category: hate
[08-19 09:25:21] (inity_circuit_breaker.py, line 537)=> Batch Size: 4
[08-19 09:25:21] (inity_circuit_breaker.py, line 538)=> Workers: 4
[08-19 09:25:21] (inity_circuit_breaker.py, line 539)=> Device: cuda:0
[08-19 09:25:21] (inity_circuit_breaker.py, line 540)=> Model: infinity_/home/gs285/VAR/my_model/weights/infinity_8b_weights
[08-19 09:25:21] (inity_circuit_breaker.py, line 541)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-19 09:25:21] (inity_circuit_breaker.py, line 542)=> Rush Resume: 
[08-19 09:25:21] (inity_circuit_breaker.py, line 543)=> Selective Layers: 0,1,2,3,4,5
[08-19 09:25:21] (inity_circuit_breaker.py, line 544)=> ============================================================
[08-19 09:25:21] (inity_circuit_breaker.py, line 326)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[08-19 09:25:21] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-19 09:25:21] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-19 09:25:21] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[08-19 09:25:21] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-19 09:25:21] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-19 09:25:22] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[08-19 09:25:22] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[08-19 09:25:22] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[08-19 09:25:22] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[08-19 09:25:22] (inity_circuit_breaker.py, line 344)=> args.batch_size=4, vbs=6
[08-19 09:25:22] (inity_circuit_breaker.py, line 367)=> len(dataloader): 100, len(dataset): 100
[08-19 09:25:22] (inity_circuit_breaker.py, line 369)=> total_samples: 200
[08-19 09:25:22] (inity_circuit_breaker.py, line 370)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[08-19 09:25:22] (inity_circuit_breaker.py, line 100)=> train_h_div_w_list=['1:1', '4:3', '3:4', '16:9', '9:16']
[08-19 09:25:22] (inity_circuit_breaker.py, line 104)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth







=======================================================   RESTART [08-19 10:18:59]   =======================================================
[08-19 10:18:59] (inity_circuit_breaker.py, line 526)=> ============================================================
[08-19 10:18:59] (inity_circuit_breaker.py, line 527)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[08-19 10:18:59] (inity_circuit_breaker.py, line 528)=> Circuit Breaker Alpha: 0.1
[08-19 10:18:59] (inity_circuit_breaker.py, line 529)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[08-19 10:18:59] (inity_circuit_breaker.py, line 530)=> Circuit Breaker Enabled: True
[08-19 10:18:59] (inity_circuit_breaker.py, line 531)=> Number of Examples: 1000
[08-19 10:18:59] (inity_circuit_breaker.py, line 532)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[08-19 10:18:59] (inity_circuit_breaker.py, line 533)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[08-19 10:18:59] (inity_circuit_breaker.py, line 534)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[08-19 10:18:59] (inity_circuit_breaker.py, line 535)=> Validation Ratio: 0.1
[08-19 10:18:59] (inity_circuit_breaker.py, line 536)=> Category: hate
[08-19 10:18:59] (inity_circuit_breaker.py, line 537)=> Batch Size: 4
[08-19 10:18:59] (inity_circuit_breaker.py, line 538)=> Workers: 4
[08-19 10:18:59] (inity_circuit_breaker.py, line 539)=> Device: cuda:0
[08-19 10:18:59] (inity_circuit_breaker.py, line 540)=> Model: infinity_/home/gs285/VAR/my_model/weights/infinity_8b_weights
[08-19 10:18:59] (inity_circuit_breaker.py, line 541)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-19 10:18:59] (inity_circuit_breaker.py, line 542)=> Rush Resume: 
[08-19 10:18:59] (inity_circuit_breaker.py, line 543)=> Selective Layers: 0,1,2,3,4,5
[08-19 10:18:59] (inity_circuit_breaker.py, line 544)=> ============================================================
[08-19 10:18:59] (inity_circuit_breaker.py, line 326)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[08-19 10:18:59] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-19 10:18:59] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-19 10:18:59] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[08-19 10:18:59] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-19 10:18:59] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-19 10:19:00] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[08-19 10:19:00] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[08-19 10:19:00] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[08-19 10:19:00] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[08-19 10:19:00] (inity_circuit_breaker.py, line 344)=> args.batch_size=4, vbs=6
[08-19 10:19:00] (inity_circuit_breaker.py, line 367)=> len(dataloader): 100, len(dataset): 100
[08-19 10:19:00] (inity_circuit_breaker.py, line 369)=> total_samples: 200
[08-19 10:19:00] (inity_circuit_breaker.py, line 370)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[08-19 10:19:00] (inity_circuit_breaker.py, line 100)=> train_h_div_w_list=['1:1', '4:3', '3:4', '16:9', '9:16']
[08-19 10:19:00] (inity_circuit_breaker.py, line 104)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-19 10:19:04] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': ['1:1', '4:3', '3:4', '16:9', '9:16'], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[08-19 10:19:04] (l/infinity/utils/load.py, line  76)=> model_str='infinity_/home/gs285/VAR/my_model/weights/infinity_8b_weights'







=======================================================   RESTART [08-19 10:24:56]   =======================================================
[08-19 10:24:56] (inity_circuit_breaker.py, line 526)=> ============================================================
[08-19 10:24:56] (inity_circuit_breaker.py, line 527)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[08-19 10:24:56] (inity_circuit_breaker.py, line 528)=> Circuit Breaker Alpha: 0.1
[08-19 10:24:56] (inity_circuit_breaker.py, line 529)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[08-19 10:24:56] (inity_circuit_breaker.py, line 530)=> Circuit Breaker Enabled: True
[08-19 10:24:56] (inity_circuit_breaker.py, line 531)=> Number of Examples: 1000
[08-19 10:24:56] (inity_circuit_breaker.py, line 532)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[08-19 10:24:56] (inity_circuit_breaker.py, line 533)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[08-19 10:24:56] (inity_circuit_breaker.py, line 534)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[08-19 10:24:56] (inity_circuit_breaker.py, line 535)=> Validation Ratio: 0.1
[08-19 10:24:56] (inity_circuit_breaker.py, line 536)=> Category: hate
[08-19 10:24:56] (inity_circuit_breaker.py, line 537)=> Batch Size: 4
[08-19 10:24:56] (inity_circuit_breaker.py, line 538)=> Workers: 4
[08-19 10:24:56] (inity_circuit_breaker.py, line 539)=> Device: cuda:0
[08-19 10:24:56] (inity_circuit_breaker.py, line 540)=> Model: infinity_infinity_8b
[08-19 10:24:56] (inity_circuit_breaker.py, line 541)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-19 10:24:56] (inity_circuit_breaker.py, line 542)=> Rush Resume: 
[08-19 10:24:56] (inity_circuit_breaker.py, line 543)=> Selective Layers: 0,1,2,3,4,5
[08-19 10:24:56] (inity_circuit_breaker.py, line 544)=> ============================================================
[08-19 10:24:56] (inity_circuit_breaker.py, line 326)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[08-19 10:24:56] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-19 10:24:56] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-19 10:24:56] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[08-19 10:24:56] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-19 10:24:56] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-19 10:24:57] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[08-19 10:24:57] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[08-19 10:24:57] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[08-19 10:24:57] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[08-19 10:24:57] (inity_circuit_breaker.py, line 344)=> args.batch_size=4, vbs=6
[08-19 10:24:57] (inity_circuit_breaker.py, line 367)=> len(dataloader): 100, len(dataset): 100
[08-19 10:24:57] (inity_circuit_breaker.py, line 369)=> total_samples: 200
[08-19 10:24:57] (inity_circuit_breaker.py, line 370)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[08-19 10:24:57] (inity_circuit_breaker.py, line 100)=> train_h_div_w_list=['1:1', '4:3', '3:4', '16:9', '9:16']
[08-19 10:24:57] (inity_circuit_breaker.py, line 104)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-19 10:25:01] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': ['1:1', '4:3', '3:4', '16:9', '9:16'], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[08-19 10:25:01] (l/infinity/utils/load.py, line  76)=> model_str='infinity_infinity_8b'







=======================================================   RESTART [08-19 10:30:18]   =======================================================
[08-19 10:30:18] (inity_circuit_breaker.py, line 526)=> ============================================================
[08-19 10:30:18] (inity_circuit_breaker.py, line 527)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[08-19 10:30:18] (inity_circuit_breaker.py, line 528)=> Circuit Breaker Alpha: 0.1
[08-19 10:30:18] (inity_circuit_breaker.py, line 529)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[08-19 10:30:18] (inity_circuit_breaker.py, line 530)=> Circuit Breaker Enabled: True
[08-19 10:30:18] (inity_circuit_breaker.py, line 531)=> Number of Examples: 1000
[08-19 10:30:18] (inity_circuit_breaker.py, line 532)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[08-19 10:30:18] (inity_circuit_breaker.py, line 533)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[08-19 10:30:18] (inity_circuit_breaker.py, line 534)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[08-19 10:30:18] (inity_circuit_breaker.py, line 535)=> Validation Ratio: 0.1
[08-19 10:30:18] (inity_circuit_breaker.py, line 536)=> Category: hate
[08-19 10:30:18] (inity_circuit_breaker.py, line 537)=> Batch Size: 4
[08-19 10:30:18] (inity_circuit_breaker.py, line 538)=> Workers: 4
[08-19 10:30:18] (inity_circuit_breaker.py, line 539)=> Device: cuda:0
[08-19 10:30:18] (inity_circuit_breaker.py, line 540)=> Model: infinity_8b
[08-19 10:30:18] (inity_circuit_breaker.py, line 541)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-19 10:30:18] (inity_circuit_breaker.py, line 542)=> Rush Resume: 
[08-19 10:30:18] (inity_circuit_breaker.py, line 543)=> Selective Layers: 0,1,2,3,4,5
[08-19 10:30:18] (inity_circuit_breaker.py, line 544)=> ============================================================
[08-19 10:30:18] (inity_circuit_breaker.py, line 326)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[08-19 10:30:18] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-19 10:30:18] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-19 10:30:18] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[08-19 10:30:18] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-19 10:30:18] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-19 10:30:19] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[08-19 10:30:19] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[08-19 10:30:19] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[08-19 10:30:19] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[08-19 10:30:19] (inity_circuit_breaker.py, line 344)=> args.batch_size=4, vbs=6
[08-19 10:30:19] (inity_circuit_breaker.py, line 367)=> len(dataloader): 100, len(dataset): 100
[08-19 10:30:19] (inity_circuit_breaker.py, line 369)=> total_samples: 200
[08-19 10:30:19] (inity_circuit_breaker.py, line 370)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[08-19 10:30:19] (inity_circuit_breaker.py, line 100)=> train_h_div_w_list=['1:1', '4:3', '3:4', '16:9', '9:16']
[08-19 10:30:19] (inity_circuit_breaker.py, line 104)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-19 10:30:23] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': ['1:1', '4:3', '3:4', '16:9', '9:16'], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[08-19 10:30:23] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'







=======================================================   RESTART [08-19 12:21:04]   =======================================================
[08-19 12:21:04] (inity_circuit_breaker.py, line 526)=> ============================================================
[08-19 12:21:04] (inity_circuit_breaker.py, line 527)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[08-19 12:21:04] (inity_circuit_breaker.py, line 528)=> Circuit Breaker Alpha: 0.1
[08-19 12:21:04] (inity_circuit_breaker.py, line 529)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[08-19 12:21:04] (inity_circuit_breaker.py, line 530)=> Circuit Breaker Enabled: True
[08-19 12:21:04] (inity_circuit_breaker.py, line 531)=> Number of Examples: 1000
[08-19 12:21:04] (inity_circuit_breaker.py, line 532)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[08-19 12:21:04] (inity_circuit_breaker.py, line 533)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[08-19 12:21:04] (inity_circuit_breaker.py, line 534)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[08-19 12:21:04] (inity_circuit_breaker.py, line 535)=> Validation Ratio: 0.1
[08-19 12:21:04] (inity_circuit_breaker.py, line 536)=> Category: hate
[08-19 12:21:04] (inity_circuit_breaker.py, line 537)=> Batch Size: 4
[08-19 12:21:04] (inity_circuit_breaker.py, line 538)=> Workers: 4
[08-19 12:21:04] (inity_circuit_breaker.py, line 539)=> Device: cuda:0
[08-19 12:21:04] (inity_circuit_breaker.py, line 540)=> Model: infinity_8b
[08-19 12:21:04] (inity_circuit_breaker.py, line 541)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-19 12:21:04] (inity_circuit_breaker.py, line 542)=> Rush Resume: 
[08-19 12:21:04] (inity_circuit_breaker.py, line 543)=> Selective Layers: 0,1,2,3,4,5
[08-19 12:21:04] (inity_circuit_breaker.py, line 544)=> ============================================================
[08-19 12:21:04] (inity_circuit_breaker.py, line 326)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[08-19 12:21:04] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-19 12:21:04] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-19 12:21:04] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[08-19 12:21:04] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-19 12:21:04] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-19 12:21:05] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[08-19 12:21:05] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[08-19 12:21:05] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[08-19 12:21:05] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[08-19 12:21:05] (inity_circuit_breaker.py, line 344)=> args.batch_size=4, vbs=6
[08-19 12:21:05] (inity_circuit_breaker.py, line 367)=> len(dataloader): 100, len(dataset): 100
[08-19 12:21:05] (inity_circuit_breaker.py, line 369)=> total_samples: 200
[08-19 12:21:05] (inity_circuit_breaker.py, line 370)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[08-19 12:21:05] (inity_circuit_breaker.py, line 100)=> train_h_div_w_list=['1:1', '4:3', '3:4', '16:9', '9:16']
[08-19 12:21:05] (inity_circuit_breaker.py, line 104)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-19 12:21:09] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': ['1:1', '4:3', '3:4', '16:9', '9:16'], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[08-19 12:21:09] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[08-19 12:21:09] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2







=======================================================   RESTART [08-19 12:37:15]   =======================================================
[08-19 12:37:15] (inity_circuit_breaker.py, line 543)=> ============================================================
[08-19 12:37:15] (inity_circuit_breaker.py, line 544)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[08-19 12:37:15] (inity_circuit_breaker.py, line 545)=> Circuit Breaker Alpha: 0.1
[08-19 12:37:15] (inity_circuit_breaker.py, line 546)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[08-19 12:37:15] (inity_circuit_breaker.py, line 547)=> Circuit Breaker Enabled: True
[08-19 12:37:15] (inity_circuit_breaker.py, line 548)=> Number of Examples: 1000
[08-19 12:37:15] (inity_circuit_breaker.py, line 549)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[08-19 12:37:15] (inity_circuit_breaker.py, line 550)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[08-19 12:37:15] (inity_circuit_breaker.py, line 551)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[08-19 12:37:15] (inity_circuit_breaker.py, line 552)=> Validation Ratio: 0.1
[08-19 12:37:15] (inity_circuit_breaker.py, line 553)=> Category: hate
[08-19 12:37:15] (inity_circuit_breaker.py, line 554)=> Batch Size: 4
[08-19 12:37:15] (inity_circuit_breaker.py, line 555)=> Workers: 4
[08-19 12:37:15] (inity_circuit_breaker.py, line 556)=> Device: cuda:0
[08-19 12:37:15] (inity_circuit_breaker.py, line 557)=> Model: infinity_8b
[08-19 12:37:15] (inity_circuit_breaker.py, line 558)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-19 12:37:15] (inity_circuit_breaker.py, line 559)=> Rush Resume: 
[08-19 12:37:15] (inity_circuit_breaker.py, line 560)=> Selective Layers: 0,1,2,3,4,5
[08-19 12:37:15] (inity_circuit_breaker.py, line 561)=> ============================================================
[08-19 12:37:15] (inity_circuit_breaker.py, line 343)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[08-19 12:37:15] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-19 12:37:15] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-19 12:37:15] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[08-19 12:37:15] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-19 12:37:15] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-19 12:37:15] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[08-19 12:37:15] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[08-19 12:37:15] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[08-19 12:37:15] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[08-19 12:37:15] (inity_circuit_breaker.py, line 361)=> args.batch_size=4, vbs=6
[08-19 12:37:15] (inity_circuit_breaker.py, line 384)=> len(dataloader): 100, len(dataset): 100
[08-19 12:37:15] (inity_circuit_breaker.py, line 386)=> total_samples: 200
[08-19 12:37:15] (inity_circuit_breaker.py, line 387)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[08-19 12:37:15] (inity_circuit_breaker.py, line 115)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[08-19 12:37:15] (inity_circuit_breaker.py, line 117)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[08-19 12:37:15] (inity_circuit_breaker.py, line 121)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-19 12:37:20] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[08-19 12:37:20] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[08-19 12:37:20] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[08-19 12:37:20] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[08-19 12:37:28] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[08-19 12:37:32] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[08-19 12:37:32] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[08-19 12:37:32] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[08-19 12:37:32] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[08-19 12:37:32] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

[08-19 12:37:54] (ity/models/init_param.py, line  15)=> [init_weights] Infinity with std=0.02
[08-19 12:37:55] (inity_circuit_breaker.py, line 219)=> [PT] GPT model = Infinity(
  drop_path_rate=0.2
  (norm0_cond): Identity()
  (text_norm): FastRMSNorm(C=2048, eps=1e-06, elementwise_affine=True)
  (text_proj_for_sos): TextAttentivePool(
    (ca): CrossAttention(
      Cq=3072, Ckv=2048, cos_attn=False
      (mat_kv): Linear(in_features=2048, out_features=6144, bias=False)
      (proj): Linear(in_features=3072, out_features=3072, bias=True)
      (proj_drop): Identity()
    )
  )
  (text_proj_for_ca): Sequential(
    (0): Linear(in_features=2048, out_features=3072, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=3072, out_features=3072, bias=True)
  )
  (lvl_embed): Embedding(15, 3072)
  (norm0_ve): Identity()
  (word_embed): Linear(in_features=56, out_features=3072, bias=True)
  (shared_ada_lin): Sequential(
    (0): SiLU()
    (1): SharedAdaLin(in_features=3072, out_features=18432, bias=True)
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=3072, out_features=6144, bias=True)
    )
  )
  (head): Linear(in_features=3072, out_features=112, bias=True)
  (blocks): ModuleList(
    (0): CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): Identity()
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
    (1-55): 55 x CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): DropPath(...)
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
  )
)


[08-19 12:37:55] (inity_circuit_breaker.py, line 221)=> [PT][#para] VAE=81.62, VAE.quant=0.00
[08-19 12:37:55] (inity_circuit_breaker.py, line 224)=> [PT][#para] GPT=8573.89


[08-19 12:37:55] (inity_circuit_breaker.py, line 252)=> >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>      FSDP INIT with args.zero=2 sharding_strategy=<ShardingStrategy.SHARD_GRAD_OP: 2> auto_wrap_policy=<torch.distributed.fsdp.wrap.ModuleWrapPolicy object at 0x7f20106ff400>({<class 'infinity.models.basic.CrossAttnBlock'>})      <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<







=======================================================   RESTART [08-19 12:49:13]   =======================================================
[08-19 12:49:13] (inity_circuit_breaker.py, line 548)=> ============================================================
[08-19 12:49:13] (inity_circuit_breaker.py, line 549)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[08-19 12:49:13] (inity_circuit_breaker.py, line 550)=> Circuit Breaker Alpha: 0.1
[08-19 12:49:13] (inity_circuit_breaker.py, line 551)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[08-19 12:49:13] (inity_circuit_breaker.py, line 552)=> Circuit Breaker Enabled: True
[08-19 12:49:13] (inity_circuit_breaker.py, line 553)=> Number of Examples: 1000
[08-19 12:49:13] (inity_circuit_breaker.py, line 554)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[08-19 12:49:13] (inity_circuit_breaker.py, line 555)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[08-19 12:49:13] (inity_circuit_breaker.py, line 556)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[08-19 12:49:13] (inity_circuit_breaker.py, line 557)=> Validation Ratio: 0.1
[08-19 12:49:13] (inity_circuit_breaker.py, line 558)=> Category: hate
[08-19 12:49:13] (inity_circuit_breaker.py, line 559)=> Batch Size: 4
[08-19 12:49:13] (inity_circuit_breaker.py, line 560)=> Workers: 4
[08-19 12:49:13] (inity_circuit_breaker.py, line 561)=> Device: cuda:0
[08-19 12:49:13] (inity_circuit_breaker.py, line 562)=> Model: infinity_8b
[08-19 12:49:13] (inity_circuit_breaker.py, line 563)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-19 12:49:13] (inity_circuit_breaker.py, line 564)=> Rush Resume: 
[08-19 12:49:13] (inity_circuit_breaker.py, line 565)=> Selective Layers: 0,1,2,3,4,5
[08-19 12:49:13] (inity_circuit_breaker.py, line 566)=> ============================================================
[08-19 12:49:13] (inity_circuit_breaker.py, line 348)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[08-19 12:49:13] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-19 12:49:13] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-19 12:49:13] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[08-19 12:49:13] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-19 12:49:13] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-19 12:49:13] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[08-19 12:49:13] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[08-19 12:49:13] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[08-19 12:49:13] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[08-19 12:49:13] (inity_circuit_breaker.py, line 366)=> args.batch_size=4, vbs=6
[08-19 12:49:13] (inity_circuit_breaker.py, line 389)=> len(dataloader): 100, len(dataset): 100
[08-19 12:49:13] (inity_circuit_breaker.py, line 391)=> total_samples: 200
[08-19 12:49:13] (inity_circuit_breaker.py, line 392)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[08-19 12:49:13] (inity_circuit_breaker.py, line 115)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[08-19 12:49:13] (inity_circuit_breaker.py, line 117)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[08-19 12:49:13] (inity_circuit_breaker.py, line 121)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-19 12:49:18] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[08-19 12:49:18] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[08-19 12:49:18] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[08-19 12:49:18] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[08-19 12:49:20] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[08-19 12:49:24] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[08-19 12:49:24] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[08-19 12:49:24] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[08-19 12:49:24] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[08-19 12:49:24] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

[08-19 12:49:42] (ity/models/init_param.py, line  15)=> [init_weights] Infinity with std=0.02
[08-19 12:49:42] (inity_circuit_breaker.py, line 219)=> [PT] GPT model = Infinity(
  drop_path_rate=0.2
  (norm0_cond): Identity()
  (text_norm): FastRMSNorm(C=2048, eps=1e-06, elementwise_affine=True)
  (text_proj_for_sos): TextAttentivePool(
    (ca): CrossAttention(
      Cq=3072, Ckv=2048, cos_attn=False
      (mat_kv): Linear(in_features=2048, out_features=6144, bias=False)
      (proj): Linear(in_features=3072, out_features=3072, bias=True)
      (proj_drop): Identity()
    )
  )
  (text_proj_for_ca): Sequential(
    (0): Linear(in_features=2048, out_features=3072, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=3072, out_features=3072, bias=True)
  )
  (lvl_embed): Embedding(15, 3072)
  (norm0_ve): Identity()
  (word_embed): Linear(in_features=56, out_features=3072, bias=True)
  (shared_ada_lin): Sequential(
    (0): SiLU()
    (1): SharedAdaLin(in_features=3072, out_features=18432, bias=True)
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=3072, out_features=6144, bias=True)
    )
  )
  (head): Linear(in_features=3072, out_features=112, bias=True)
  (blocks): ModuleList(
    (0): CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): Identity()
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
    (1-55): 55 x CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): DropPath(...)
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
  )
)


[08-19 12:49:42] (inity_circuit_breaker.py, line 221)=> [PT][#para] VAE=81.62, VAE.quant=0.00
[08-19 12:49:42] (inity_circuit_breaker.py, line 224)=> [PT][#para] GPT=8573.89


[08-19 12:49:42] (inity_circuit_breaker.py, line 257)=> >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>      FSDP INIT with args.zero=2 sharding_strategy=<ShardingStrategy.SHARD_GRAD_OP: 2> auto_wrap_policy=<torch.distributed.fsdp.wrap.ModuleWrapPolicy object at 0x7f4a04e1b700>({<class 'infinity.models.basic.CrossAttnBlock'>})      <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<







=======================================================   RESTART [08-19 12:55:41]   =======================================================
[08-19 12:55:41] (inity_circuit_breaker.py, line 548)=> ============================================================
[08-19 12:55:41] (inity_circuit_breaker.py, line 549)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[08-19 12:55:41] (inity_circuit_breaker.py, line 550)=> Circuit Breaker Alpha: 0.1
[08-19 12:55:41] (inity_circuit_breaker.py, line 551)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[08-19 12:55:41] (inity_circuit_breaker.py, line 552)=> Circuit Breaker Enabled: True
[08-19 12:55:41] (inity_circuit_breaker.py, line 553)=> Number of Examples: 1000
[08-19 12:55:41] (inity_circuit_breaker.py, line 554)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[08-19 12:55:41] (inity_circuit_breaker.py, line 555)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[08-19 12:55:41] (inity_circuit_breaker.py, line 556)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[08-19 12:55:41] (inity_circuit_breaker.py, line 557)=> Validation Ratio: 0.1
[08-19 12:55:41] (inity_circuit_breaker.py, line 558)=> Category: hate
[08-19 12:55:41] (inity_circuit_breaker.py, line 559)=> Batch Size: 4
[08-19 12:55:41] (inity_circuit_breaker.py, line 560)=> Workers: 4
[08-19 12:55:41] (inity_circuit_breaker.py, line 561)=> Device: cuda:0
[08-19 12:55:41] (inity_circuit_breaker.py, line 562)=> Model: infinity_8b
[08-19 12:55:41] (inity_circuit_breaker.py, line 563)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-19 12:55:41] (inity_circuit_breaker.py, line 564)=> Rush Resume: 
[08-19 12:55:41] (inity_circuit_breaker.py, line 565)=> Selective Layers: 0,1,2,3,4,5
[08-19 12:55:41] (inity_circuit_breaker.py, line 566)=> ============================================================
[08-19 12:55:41] (inity_circuit_breaker.py, line 348)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[08-19 12:55:41] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-19 12:55:41] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-19 12:55:41] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[08-19 12:55:41] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-19 12:55:41] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-19 12:55:42] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[08-19 12:55:42] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[08-19 12:55:42] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[08-19 12:55:42] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[08-19 12:55:42] (inity_circuit_breaker.py, line 366)=> args.batch_size=4, vbs=6
[08-19 12:55:42] (inity_circuit_breaker.py, line 389)=> len(dataloader): 100, len(dataset): 100
[08-19 12:55:42] (inity_circuit_breaker.py, line 391)=> total_samples: 200
[08-19 12:55:42] (inity_circuit_breaker.py, line 392)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[08-19 12:55:42] (inity_circuit_breaker.py, line 115)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[08-19 12:55:42] (inity_circuit_breaker.py, line 117)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[08-19 12:55:42] (inity_circuit_breaker.py, line 121)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-19 12:55:46] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[08-19 12:55:46] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[08-19 12:55:46] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[08-19 12:55:46] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[08-19 12:55:49] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[08-19 12:55:52] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[08-19 12:55:52] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[08-19 12:55:52] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[08-19 12:55:52] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[08-19 12:55:52] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

[08-19 12:56:10] (ity/models/init_param.py, line  15)=> [init_weights] Infinity with std=0.02
[08-19 12:56:10] (inity_circuit_breaker.py, line 219)=> [PT] GPT model = Infinity(
  drop_path_rate=0.2
  (norm0_cond): Identity()
  (text_norm): FastRMSNorm(C=2048, eps=1e-06, elementwise_affine=True)
  (text_proj_for_sos): TextAttentivePool(
    (ca): CrossAttention(
      Cq=3072, Ckv=2048, cos_attn=False
      (mat_kv): Linear(in_features=2048, out_features=6144, bias=False)
      (proj): Linear(in_features=3072, out_features=3072, bias=True)
      (proj_drop): Identity()
    )
  )
  (text_proj_for_ca): Sequential(
    (0): Linear(in_features=2048, out_features=3072, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=3072, out_features=3072, bias=True)
  )
  (lvl_embed): Embedding(15, 3072)
  (norm0_ve): Identity()
  (word_embed): Linear(in_features=56, out_features=3072, bias=True)
  (shared_ada_lin): Sequential(
    (0): SiLU()
    (1): SharedAdaLin(in_features=3072, out_features=18432, bias=True)
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=3072, out_features=6144, bias=True)
    )
  )
  (head): Linear(in_features=3072, out_features=112, bias=True)
  (blocks): ModuleList(
    (0): CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): Identity()
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
    (1-55): 55 x CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): DropPath(...)
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
  )
)


[08-19 12:56:10] (inity_circuit_breaker.py, line 221)=> [PT][#para] VAE=81.62, VAE.quant=0.00
[08-19 12:56:10] (inity_circuit_breaker.py, line 224)=> [PT][#para] GPT=8573.89


[08-19 12:56:10] (inity_circuit_breaker.py, line 257)=> >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>      FSDP INIT with args.zero=2 sharding_strategy=<ShardingStrategy.SHARD_GRAD_OP: 2> auto_wrap_policy=<torch.distributed.fsdp.wrap.ModuleWrapPolicy object at 0x7fc4ac63fd90>({<class 'infinity.models.basic.CrossAttnBlock'>})      <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<







=======================================================   RESTART [08-19 12:57:59]   =======================================================
[08-19 12:57:59] (inity_circuit_breaker.py, line 576)=> ============================================================
[08-19 12:57:59] (inity_circuit_breaker.py, line 577)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[08-19 12:57:59] (inity_circuit_breaker.py, line 578)=> Circuit Breaker Alpha: 0.1
[08-19 12:57:59] (inity_circuit_breaker.py, line 579)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[08-19 12:57:59] (inity_circuit_breaker.py, line 580)=> Circuit Breaker Enabled: True
[08-19 12:57:59] (inity_circuit_breaker.py, line 581)=> Number of Examples: 1000
[08-19 12:57:59] (inity_circuit_breaker.py, line 582)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[08-19 12:57:59] (inity_circuit_breaker.py, line 583)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[08-19 12:57:59] (inity_circuit_breaker.py, line 584)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[08-19 12:57:59] (inity_circuit_breaker.py, line 585)=> Validation Ratio: 0.1
[08-19 12:57:59] (inity_circuit_breaker.py, line 586)=> Category: hate
[08-19 12:57:59] (inity_circuit_breaker.py, line 587)=> Batch Size: 4
[08-19 12:57:59] (inity_circuit_breaker.py, line 588)=> Workers: 4
[08-19 12:57:59] (inity_circuit_breaker.py, line 589)=> Device: cuda:0
[08-19 12:57:59] (inity_circuit_breaker.py, line 590)=> Model: infinity_8b
[08-19 12:57:59] (inity_circuit_breaker.py, line 591)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-19 12:57:59] (inity_circuit_breaker.py, line 592)=> Rush Resume: 
[08-19 12:57:59] (inity_circuit_breaker.py, line 593)=> Selective Layers: 0,1,2,3,4,5
[08-19 12:57:59] (inity_circuit_breaker.py, line 594)=> ============================================================
[08-19 12:57:59] (inity_circuit_breaker.py, line 376)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[08-19 12:57:59] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-19 12:57:59] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-19 12:57:59] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[08-19 12:57:59] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-19 12:57:59] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-19 12:58:00] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[08-19 12:58:00] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[08-19 12:58:00] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[08-19 12:58:00] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[08-19 12:58:00] (inity_circuit_breaker.py, line 394)=> args.batch_size=4, vbs=6
[08-19 12:58:00] (inity_circuit_breaker.py, line 417)=> len(dataloader): 100, len(dataset): 100
[08-19 12:58:00] (inity_circuit_breaker.py, line 419)=> total_samples: 200
[08-19 12:58:00] (inity_circuit_breaker.py, line 420)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[08-19 12:58:00] (inity_circuit_breaker.py, line 140)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[08-19 12:58:00] (inity_circuit_breaker.py, line 142)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[08-19 12:58:00] (inity_circuit_breaker.py, line 146)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-19 12:58:04] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[08-19 12:58:04] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[08-19 12:58:04] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[08-19 12:58:04] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[08-19 12:58:06] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[08-19 12:58:09] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[08-19 12:58:09] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[08-19 12:58:09] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[08-19 12:58:10] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[08-19 12:58:10] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

[08-19 12:58:28] (ity/models/init_param.py, line  15)=> [init_weights] Infinity with std=0.02
[08-19 12:58:28] (inity_circuit_breaker.py, line 246)=> [PT] GPT model = Infinity(
  drop_path_rate=0.2
  (norm0_cond): Identity()
  (text_norm): FastRMSNorm(C=2048, eps=1e-06, elementwise_affine=True)
  (text_proj_for_sos): TextAttentivePool(
    (ca): CrossAttention(
      Cq=3072, Ckv=2048, cos_attn=False
      (mat_kv): Linear(in_features=2048, out_features=6144, bias=False)
      (proj): Linear(in_features=3072, out_features=3072, bias=True)
      (proj_drop): Identity()
    )
  )
  (text_proj_for_ca): Sequential(
    (0): Linear(in_features=2048, out_features=3072, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=3072, out_features=3072, bias=True)
  )
  (lvl_embed): Embedding(15, 3072)
  (norm0_ve): Identity()
  (word_embed): Linear(in_features=56, out_features=3072, bias=True)
  (shared_ada_lin): Sequential(
    (0): SiLU()
    (1): SharedAdaLin(in_features=3072, out_features=18432, bias=True)
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=3072, out_features=6144, bias=True)
    )
  )
  (head): Linear(in_features=3072, out_features=112, bias=True)
  (blocks): ModuleList(
    (0): CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): Identity()
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
    (1-55): 55 x CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): DropPath(...)
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
  )
)


[08-19 12:58:28] (inity_circuit_breaker.py, line 248)=> [PT][#para] VAE=81.62, VAE.quant=0.00
[08-19 12:58:28] (inity_circuit_breaker.py, line 251)=> [PT][#para] GPT=8573.89


[08-19 12:58:28] (inity_circuit_breaker.py, line 285)=> >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>      FSDP INIT with args.zero=2 sharding_strategy=<ShardingStrategy.SHARD_GRAD_OP: 2> auto_wrap_policy=<torch.distributed.fsdp.wrap.ModuleWrapPolicy object at 0x7fd1c143f340>({<class 'infinity.models.basic.CrossAttnBlock'>})      <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<







=======================================================   RESTART [08-20 20:08:38]   =======================================================
[08-20 20:08:38] (inity_circuit_breaker.py, line 576)=> ============================================================
[08-20 20:08:38] (inity_circuit_breaker.py, line 577)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[08-20 20:08:38] (inity_circuit_breaker.py, line 578)=> Circuit Breaker Alpha: 0.1
[08-20 20:08:38] (inity_circuit_breaker.py, line 579)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[08-20 20:08:38] (inity_circuit_breaker.py, line 580)=> Circuit Breaker Enabled: True
[08-20 20:08:38] (inity_circuit_breaker.py, line 581)=> Number of Examples: 1000
[08-20 20:08:38] (inity_circuit_breaker.py, line 582)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[08-20 20:08:38] (inity_circuit_breaker.py, line 583)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[08-20 20:08:38] (inity_circuit_breaker.py, line 584)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[08-20 20:08:38] (inity_circuit_breaker.py, line 585)=> Validation Ratio: 0.1
[08-20 20:08:38] (inity_circuit_breaker.py, line 586)=> Category: hate
[08-20 20:08:38] (inity_circuit_breaker.py, line 587)=> Batch Size: 4
[08-20 20:08:38] (inity_circuit_breaker.py, line 588)=> Workers: 4
[08-20 20:08:38] (inity_circuit_breaker.py, line 589)=> Device: cuda:0
[08-20 20:08:38] (inity_circuit_breaker.py, line 590)=> Model: infinity_8b
[08-20 20:08:38] (inity_circuit_breaker.py, line 591)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-20 20:08:38] (inity_circuit_breaker.py, line 592)=> Rush Resume: 
[08-20 20:08:38] (inity_circuit_breaker.py, line 593)=> Selective Layers: 0,1,2,3,4,5
[08-20 20:08:38] (inity_circuit_breaker.py, line 594)=> ============================================================
[08-20 20:08:38] (inity_circuit_breaker.py, line 376)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[08-20 20:08:38] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-20 20:08:38] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[08-20 20:08:38] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[08-20 20:08:38] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-20 20:08:38] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[08-20 20:08:38] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[08-20 20:08:38] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[08-20 20:08:38] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[08-20 20:08:38] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[08-20 20:08:38] (inity_circuit_breaker.py, line 394)=> args.batch_size=4, vbs=6
[08-20 20:08:38] (inity_circuit_breaker.py, line 417)=> len(dataloader): 100, len(dataset): 100
[08-20 20:08:38] (inity_circuit_breaker.py, line 419)=> total_samples: 200
[08-20 20:08:38] (inity_circuit_breaker.py, line 420)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[08-20 20:08:38] (inity_circuit_breaker.py, line 140)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[08-20 20:08:38] (inity_circuit_breaker.py, line 142)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[08-20 20:08:38] (inity_circuit_breaker.py, line 146)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[08-20 20:08:43] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[08-20 20:08:43] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[08-20 20:08:43] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[08-20 20:08:43] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[08-20 20:08:45] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[08-20 20:08:49] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[08-20 20:08:49] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[08-20 20:08:49] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[08-20 20:08:49] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[08-20 20:08:49] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

[08-20 20:09:07] (ity/models/init_param.py, line  15)=> [init_weights] Infinity with std=0.02
[08-20 20:09:07] (inity_circuit_breaker.py, line 246)=> [PT] GPT model = Infinity(
  drop_path_rate=0.2
  (norm0_cond): Identity()
  (text_norm): FastRMSNorm(C=2048, eps=1e-06, elementwise_affine=True)
  (text_proj_for_sos): TextAttentivePool(
    (ca): CrossAttention(
      Cq=3072, Ckv=2048, cos_attn=False
      (mat_kv): Linear(in_features=2048, out_features=6144, bias=False)
      (proj): Linear(in_features=3072, out_features=3072, bias=True)
      (proj_drop): Identity()
    )
  )
  (text_proj_for_ca): Sequential(
    (0): Linear(in_features=2048, out_features=3072, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=3072, out_features=3072, bias=True)
  )
  (lvl_embed): Embedding(15, 3072)
  (norm0_ve): Identity()
  (word_embed): Linear(in_features=56, out_features=3072, bias=True)
  (shared_ada_lin): Sequential(
    (0): SiLU()
    (1): SharedAdaLin(in_features=3072, out_features=18432, bias=True)
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=3072, out_features=6144, bias=True)
    )
  )
  (head): Linear(in_features=3072, out_features=112, bias=True)
  (blocks): ModuleList(
    (0): CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): Identity()
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
    (1-55): 55 x CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): DropPath(...)
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
  )
)


[08-20 20:09:07] (inity_circuit_breaker.py, line 248)=> [PT][#para] VAE=81.62, VAE.quant=0.00
[08-20 20:09:07] (inity_circuit_breaker.py, line 251)=> [PT][#para] GPT=8573.89


[08-20 20:09:07] (inity_circuit_breaker.py, line 285)=> >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>      FSDP INIT with args.zero=2 sharding_strategy=<ShardingStrategy.SHARD_GRAD_OP: 2> auto_wrap_policy=<torch.distributed.fsdp.wrap.ModuleWrapPolicy object at 0x7fe07ea3f880>({<class 'infinity.models.basic.CrossAttnBlock'>})      <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<







=======================================================   RESTART [09-01 22:43:42]   =======================================================
[09-01 22:43:42] (inity_circuit_breaker.py, line 576)=> ============================================================
[09-01 22:43:42] (inity_circuit_breaker.py, line 577)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[09-01 22:43:42] (inity_circuit_breaker.py, line 578)=> Circuit Breaker Alpha: 0.1
[09-01 22:43:42] (inity_circuit_breaker.py, line 579)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[09-01 22:43:42] (inity_circuit_breaker.py, line 580)=> Circuit Breaker Enabled: True
[09-01 22:43:42] (inity_circuit_breaker.py, line 581)=> Number of Examples: 1000
[09-01 22:43:42] (inity_circuit_breaker.py, line 582)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[09-01 22:43:42] (inity_circuit_breaker.py, line 583)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[09-01 22:43:42] (inity_circuit_breaker.py, line 584)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[09-01 22:43:42] (inity_circuit_breaker.py, line 585)=> Validation Ratio: 0.1
[09-01 22:43:42] (inity_circuit_breaker.py, line 586)=> Category: hate
[09-01 22:43:42] (inity_circuit_breaker.py, line 587)=> Batch Size: 4
[09-01 22:43:42] (inity_circuit_breaker.py, line 588)=> Workers: 4
[09-01 22:43:42] (inity_circuit_breaker.py, line 589)=> Device: cuda:0
[09-01 22:43:42] (inity_circuit_breaker.py, line 590)=> Model: infinity_8b
[09-01 22:43:42] (inity_circuit_breaker.py, line 591)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-01 22:43:42] (inity_circuit_breaker.py, line 592)=> Rush Resume: 
[09-01 22:43:42] (inity_circuit_breaker.py, line 593)=> Selective Layers: 0,1,2,3,4,5
[09-01 22:43:42] (inity_circuit_breaker.py, line 594)=> ============================================================
[09-01 22:43:42] (inity_circuit_breaker.py, line 376)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[09-01 22:43:42] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-01 22:43:42] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-01 22:43:42] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[09-01 22:43:42] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-01 22:43:42] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-01 22:43:43] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[09-01 22:43:43] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[09-01 22:43:43] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[09-01 22:43:43] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[09-01 22:43:43] (inity_circuit_breaker.py, line 394)=> args.batch_size=4, vbs=6
[09-01 22:43:43] (inity_circuit_breaker.py, line 417)=> len(dataloader): 100, len(dataset): 100
[09-01 22:43:43] (inity_circuit_breaker.py, line 419)=> total_samples: 200
[09-01 22:43:43] (inity_circuit_breaker.py, line 420)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[09-01 22:43:43] (inity_circuit_breaker.py, line 140)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[09-01 22:43:43] (inity_circuit_breaker.py, line 142)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[09-01 22:43:43] (inity_circuit_breaker.py, line 146)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-01 22:43:46] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[09-01 22:43:46] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[09-01 22:43:46] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[09-01 22:43:46] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[09-01 22:43:54] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[09-01 22:43:59] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[09-01 22:44:00] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[09-01 22:44:00] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[09-01 22:44:00] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[09-01 22:44:00] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

[09-01 22:44:10] (ity/models/init_param.py, line  15)=> [init_weights] Infinity with std=0.02
[09-01 22:44:10] (inity_circuit_breaker.py, line 246)=> [PT] GPT model = Infinity(
  drop_path_rate=0.2
  (norm0_cond): Identity()
  (text_norm): FastRMSNorm(C=2048, eps=1e-06, elementwise_affine=True)
  (text_proj_for_sos): TextAttentivePool(
    (ca): CrossAttention(
      Cq=3072, Ckv=2048, cos_attn=False
      (mat_kv): Linear(in_features=2048, out_features=6144, bias=False)
      (proj): Linear(in_features=3072, out_features=3072, bias=True)
      (proj_drop): Identity()
    )
  )
  (text_proj_for_ca): Sequential(
    (0): Linear(in_features=2048, out_features=3072, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=3072, out_features=3072, bias=True)
  )
  (lvl_embed): Embedding(15, 3072)
  (norm0_ve): Identity()
  (word_embed): Linear(in_features=56, out_features=3072, bias=True)
  (shared_ada_lin): Sequential(
    (0): SiLU()
    (1): SharedAdaLin(in_features=3072, out_features=18432, bias=True)
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=3072, out_features=6144, bias=True)
    )
  )
  (head): Linear(in_features=3072, out_features=112, bias=True)
  (blocks): ModuleList(
    (0): CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): Identity()
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
    (1-55): 55 x CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): DropPath(...)
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
  )
)


[09-01 22:44:10] (inity_circuit_breaker.py, line 248)=> [PT][#para] VAE=81.62, VAE.quant=0.00
[09-01 22:44:10] (inity_circuit_breaker.py, line 251)=> [PT][#para] GPT=8573.89


[09-01 22:44:11] (nity/utils/lr_control.py, line  71)=> [get_param_groups][lr decay] with_lr_scale=False, lr_scale=0.0
[09-01 22:44:11] (nity/utils/lr_control.py, line 111)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': '[no scale]',
         'params': "('module.text_norm.weight, module.text_proj_for_sos.ca.mat_kv.weight, module.text_proj_for_sos.ca.proj.weight, module.text_proj_for_ca.0.weight, module.text_proj_for_ca.2.weight, '\n"
                   " 'module.word_embed.weight, module.shared_ada_lin.1.weight, module.head_nm.ada_lin.1.weight, module.head.weight, module.blocks.0.sa.mat_qkv.weight, module.blocks.0.sa.proj.weight, '\n"
                   " 'module.blocks.0.ca.mat_q.weight, module.blocks.0.ca.mat_kv.weight, module.blocks.0.ca.proj.weight, module.blocks.0.ffn.fc1.weight, module.blocks.0.ffn.fc2.weight, module.blocks.0.ca_norm.weight, '\n"
                   " 'module.blocks.1.sa.mat_qkv.weight, module.blocks.1.sa.proj.weight, module.blocks.1.ca.mat_q.weight, module.blocks.1.ca.mat_kv.weight, module.blocks.1.ca.proj.weight, '\n"
                   " 'module.blocks.1.ffn.fc1.weight, module.blocks.1.ffn.fc2.weight, module.blocks.1.ca_norm.weight, module.blocks.2.sa.mat_qkv.weight, module.blocks.2.sa.proj.weight, module.blocks.2.ca.mat_q.weight, '\n"
                   " 'module.blocks.2.ca.mat_kv.weight, module.blocks.2.ca.proj.weight, module.blocks.2.ffn.fc1.weight, module.blocks.2.ffn.fc2.weight, module.blocks.2.ca_norm.weight, module.blocks.3.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.3.sa.proj.weight, module.blocks.3.ca.mat_q.weight, module.blocks.3.ca.mat_kv.weight, module.blocks.3.ca.proj.weight, module.blocks.3.ffn.fc1.weight, module.blocks.3.ffn.fc2.weight, '\n"
                   " 'module.blocks.3.ca_norm.weight, module.blocks.4.sa.mat_qkv.weight, module.blocks.4.sa.proj.weight, module.blocks.4.ca.mat_q.weight, module.blocks.4.ca.mat_kv.weight, '\n"
                   " 'module.blocks.4.ca.proj.weight, module.blocks.4.ffn.fc1.weight, module.blocks.4.ffn.fc2.weight, module.blocks.4.ca_norm.weight, module.blocks.5.sa.mat_qkv.weight, module.blocks.5.sa.proj.weight, '\n"
                   " 'module.blocks.5.ca.mat_q.weight, module.blocks.5.ca.mat_kv.weight, module.blocks.5.ca.proj.weight, module.blocks.5.ffn.fc1.weight, module.blocks.5.ffn.fc2.weight, module.blocks.5.ca_norm.weight, '\n"
                   " 'module.blocks.6.sa.mat_qkv.weight, module.blocks.6.sa.proj.weight, module.blocks.6.ca.mat_q.weight, module.blocks.6.ca.mat_kv.weight, module.blocks.6.ca.proj.weight, '\n"
                   " 'module.blocks.6.ffn.fc1.weight, module.blocks.6.ffn.fc2.weight, module.blocks.6.ca_norm.weight, module.blocks.7.sa.mat_qkv.weight, module.blocks.7.sa.proj.weight, module.blocks.7.ca.mat_q.weight, '\n"
                   " 'module.blocks.7.ca.mat_kv.weight, module.blocks.7.ca.proj.weight, module.blocks.7.ffn.fc1.weight, module.blocks.7.ffn.fc2.weight, module.blocks.7.ca_norm.weight, module.blocks.8.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.8.sa.proj.weight, module.blocks.8.ca.mat_q.weight, module.blocks.8.ca.mat_kv.weight, module.blocks.8.ca.proj.weight, module.blocks.8.ffn.fc1.weight, module.blocks.8.ffn.fc2.weight, '\n"
                   " 'module.blocks.8.ca_norm.weight, module.blocks.9.sa.mat_qkv.weight, module.blocks.9.sa.proj.weight, module.blocks.9.ca.mat_q.weight, module.blocks.9.ca.mat_kv.weight, '\n"
                   " 'module.blocks.9.ca.proj.weight, module.blocks.9.ffn.fc1.weight, module.blocks.9.ffn.fc2.weight, module.blocks.9.ca_norm.weight, module.blocks.10.sa.mat_qkv.weight, module.blocks.10.sa.proj.weight, '\n"
                   " 'module.blocks.10.ca.mat_q.weight, module.blocks.10.ca.mat_kv.weight, module.blocks.10.ca.proj.weight, module.blocks.10.ffn.fc1.weight, module.blocks.10.ffn.fc2.weight, '\n"
                   " 'module.blocks.10.ca_norm.weight, module.blocks.11.sa.mat_qkv.weight, module.blocks.11.sa.proj.weight, module.blocks.11.ca.mat_q.weight, module.blocks.11.ca.mat_kv.weight, '\n"
                   " 'module.blocks.11.ca.proj.weight, module.blocks.11.ffn.fc1.weight, module.blocks.11.ffn.fc2.weight, module.blocks.11.ca_norm.weight, module.blocks.12.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.12.sa.proj.weight, module.blocks.12.ca.mat_q.weight, module.blocks.12.ca.mat_kv.weight, module.blocks.12.ca.proj.weight, module.blocks.12.ffn.fc1.weight, '\n"
                   " 'module.blocks.12.ffn.fc2.weight, module.blocks.12.ca_norm.weight, module.blocks.13.sa.mat_qkv.weight, module.blocks.13.sa.proj.weight, module.blocks.13.ca.mat_q.weight, '\n"
                   " 'module.blocks.13.ca.mat_kv.weight, module.blocks.13.ca.proj.weight, module.blocks.13.ffn.fc1.weight, module.blocks.13.ffn.fc2.weight, module.blocks.13.ca_norm.weight, '\n"
                   " 'module.blocks.14.sa.mat_qkv.weight, module.blocks.14.sa.proj.weight, module.blocks.14.ca.mat_q.weight, module.blocks.14.ca.mat_kv.weight, module.blocks.14.ca.proj.weight, '\n"
                   " 'module.blocks.14.ffn.fc1.weight, module.blocks.14.ffn.fc2.weight, module.blocks.14.ca_norm.weight, module.blocks.15.sa.mat_qkv.weight, module.blocks.15.sa.proj.weight, '\n"
                   " 'module.blocks.15.ca.mat_q.weight, module.blocks.15.ca.mat_kv.weight, module.blocks.15.ca.proj.weight, module.blocks.15.ffn.fc1.weight, module.blocks.15.ffn.fc2.weight, '\n"
                   " 'module.blocks.15.ca_norm.weight, module.blocks.16.sa.mat_qkv.weight, module.blocks.16.sa.proj.weight, module.blocks.16.ca.mat_q.weight, module.blocks.16.ca.mat_kv.weight, '\n"
                   " 'module.blocks.16.ca.proj.weight, module.blocks.16.ffn.fc1.weight, module.blocks.16.ffn.fc2.weight, module.blocks.16.ca_norm.weight, module.blocks.17.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.17.sa.proj.weight, module.blocks.17.ca.mat_q.weight, module.blocks.17.ca.mat_kv.weight, module.blocks.17.ca.proj.weight, module.blocks.17.ffn.fc1.weight, '\n"
                   " 'module.blocks.17.ffn.fc2.weight, module.blocks.17.ca_norm.weight, module.blocks.18.sa.mat_qkv.weight, module.blocks.18.sa.proj.weight, module.blocks.18.ca.mat_q.weight, '\n"
                   " 'module.blocks.18.ca.mat_kv.weight, module.blocks.18.ca.proj.weight, module.blocks.18.ffn.fc1.weight, module.blocks.18.ffn.fc2.weight, module.blocks.18.ca_norm.weight, '\n"
                   " 'module.blocks.19.sa.mat_qkv.weight, module.blocks.19.sa.proj.weight, module.blocks.19.ca.mat_q.weight, module.blocks.19.ca.mat_kv.weight, module.blocks.19.ca.proj.weight, '\n"
                   " 'module.blocks.19.ffn.fc1.weight, module.blocks.19.ffn.fc2.weight, module.blocks.19.ca_norm.weight, module.blocks.20.sa.mat_qkv.weight, module.blocks.20.sa.proj.weight, '\n"
                   " 'module.blocks.20.ca.mat_q.weight, module.blocks.20.ca.mat_kv.weight, module.blocks.20.ca.proj.weight, module.blocks.20.ffn.fc1.weight, module.blocks.20.ffn.fc2.weight, '\n"
                   " 'module.blocks.20.ca_norm.weight, module.blocks.21.sa.mat_qkv.weight, module.blocks.21.sa.proj.weight, module.blocks.21.ca.mat_q.weight, module.blocks.21.ca.mat_kv.weight, '\n"
                   " 'module.blocks.21.ca.proj.weight, module.blocks.21.ffn.fc1.weight, module.blocks.21.ffn.fc2.weight, module.blocks.21.ca_norm.weight, module.blocks.22.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.22.sa.proj.weight, module.blocks.22.ca.mat_q.weight, module.blocks.22.ca.mat_kv.weight, module.blocks.22.ca.proj.weight, module.blocks.22.ffn.fc1.weight, '\n"
                   " 'module.blocks.22.ffn.fc2.weight, module.blocks.22.ca_norm.weight, module.blocks.23.sa.mat_qkv.weight, module.blocks.23.sa.proj.weight, module.blocks.23.ca.mat_q.weight, '\n"
                   " 'module.blocks.23.ca.mat_kv.weight, module.blocks.23.ca.proj.weight, module.blocks.23.ffn.fc1.weight, module.blocks.23.ffn.fc2.weight, module.blocks.23.ca_norm.weight, '\n"
                   " 'module.blocks.24.sa.mat_qkv.weight, module.blocks.24.sa.proj.weight, module.blocks.24.ca.mat_q.weight, module.blocks.24.ca.mat_kv.weight, module.blocks.24.ca.proj.weight, '\n"
                   " 'module.blocks.24.ffn.fc1.weight, module.blocks.24.ffn.fc2.weight, module.blocks.24.ca_norm.weight, module.blocks.25.sa.mat_qkv.weight, module.blocks.25.sa.proj.weight, '\n"
                   " 'module.blocks.25.ca.mat_q.weight, module.blocks.25.ca.mat_kv.weight, module.blocks.25.ca.proj.weight, module.blocks.25.ffn.fc1.weight, module.blocks.25.ffn.fc2.weight, '\n"
                   " 'module.blocks.25.ca_norm.weight, module.blocks.26.sa.mat_qkv.weight, module.blocks.26.sa.proj.weight, module.blocks.26.ca.mat_q.weight, module.blocks.26.ca.mat_kv.weight, '\n"
                   " 'module.blocks.26.ca.proj.weight, module.blocks.26.ffn.fc1.weight, module.blocks.26.ffn.fc2.weight, module.blocks.26.ca_norm.weight, module.blocks.27.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.27.sa.proj.weight, module.blocks.27.ca.mat_q.weight, module.blocks.27.ca.mat_kv.weight, module.blocks.27.ca.proj.weight, module.blocks.27.ffn.fc1.weight, '\n"
                   " 'module.blocks.27.ffn.fc2.weight, module.blocks.27.ca_norm.weight, module.blocks.28.sa.mat_qkv.weight, module.blocks.28.sa.proj.weight, module.blocks.28.ca.mat_q.weight, '\n"
                   " 'module.blocks.28.ca.mat_kv.weight, module.blocks.28.ca.proj.weight, module.blocks.28.ffn.fc1.weight, module.blocks.28.ffn.fc2.weight, module.blocks.28.ca_norm.weight, '\n"
                   " 'module.blocks.29.sa.mat_qkv.weight, module.blocks.29.sa.proj.weight, module.blocks.29.ca.mat_q.weight, module.blocks.29.ca.mat_kv.weight, module.blocks.29.ca.proj.weight, '\n"
                   " 'module.blocks.29.ffn.fc1.weight, module.blocks.29.ffn.fc2.weight, module.blocks.29.ca_norm.weight, module.blocks.30.sa.mat_qkv.weight, module.blocks.30.sa.proj.weight, '\n"
                   " 'module.blocks.30.ca.mat_q.weight, module.blocks.30.ca.mat_kv.weight, module.blocks.30.ca.proj.weight, module.blocks.30.ffn.fc1.weight, module.blocks.30.ffn.fc2.weight, '\n"
                   " 'module.blocks.30.ca_norm.weight, module.blocks.31.sa.mat_qkv.weight, module.blocks.31.sa.proj.weight, module.blocks.31.ca.mat_q.weight, module.blocks.31.ca.mat_kv.weight, '\n"
                   " 'module.blocks.31.ca.proj.weight, module.blocks.31.ffn.fc1.weight, module.blocks.31.ffn.fc2.weight, module.blocks.31.ca_norm.weight, module.blocks.32.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.32.sa.proj.weight, module.blocks.32.ca.mat_q.weight, module.blocks.32.ca.mat_kv.weight, module.blocks.32.ca.proj.weight, module.blocks.32.ffn.fc1.weight, '\n"
                   " 'module.blocks.32.ffn.fc2.weight, module.blocks.32.ca_norm.weight, module.blocks.33.sa.mat_qkv.weight, module.blocks.33.sa.proj.weight, module.blocks.33.ca.mat_q.weight, '\n"
                   " 'module.blocks.33.ca.mat_kv.weight, module.blocks.33.ca.proj.weight, module.blocks.33.ffn.fc1.weight, module.blocks.33.ffn.fc2.weight, module.blocks.33.ca_norm.weight, '\n"
                   " 'module.blocks.34.sa.mat_qkv.weight, module.blocks.34.sa.proj.weight, module.blocks.34.ca.mat_q.weight, module.blocks.34.ca.mat_kv.weight, module.blocks.34.ca.proj.weight, '\n"
                   " 'module.blocks.34.ffn.fc1.weight, module.blocks.34.ffn.fc2.weight, module.blocks.34.ca_norm.weight, module.blocks.35.sa.mat_qkv.weight, module.blocks.35.sa.proj.weight, '\n"
                   " 'module.blocks.35.ca.mat_q.weight, module.blocks.35.ca.mat_kv.weight, module.blocks.35.ca.proj.weight, module.blocks.35.ffn.fc1.weight, module.blocks.35.ffn.fc2.weight, '\n"
                   " 'module.blocks.35.ca_norm.weight, module.blocks.36.sa.mat_qkv.weight, module.blocks.36.sa.proj.weight, module.blocks.36.ca.mat_q.weight, module.blocks.36.ca.mat_kv.weight, '\n"
                   " 'module.blocks.36.ca.proj.weight, module.blocks.36.ffn.fc1.weight, module.blocks.36.ffn.fc2.weight, module.blocks.36.ca_norm.weight, module.blocks.37.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.37.sa.proj.weight, module.blocks.37.ca.mat_q.weight, module.blocks.37.ca.mat_kv.weight, module.blocks.37.ca.proj.weight, module.blocks.37.ffn.fc1.weight, '\n"
                   " 'module.blocks.37.ffn.fc2.weight, module.blocks.37.ca_norm.weight, module.blocks.38.sa.mat_qkv.weight, module.blocks.38.sa.proj.weight, module.blocks.38.ca.mat_q.weight, '\n"
                   " 'module.blocks.38.ca.mat_kv.weight, module.blocks.38.ca.proj.weight, module.blocks.38.ffn.fc1.weight, module.blocks.38.ffn.fc2.weight, module.blocks.38.ca_norm.weight, '\n"
                   " 'module.blocks.39.sa.mat_qkv.weight, module.blocks.39.sa.proj.weight, module.blocks.39.ca.mat_q.weight, module.blocks.39.ca.mat_kv.weight, module.blocks.39.ca.proj.weight, '\n"
                   " 'module.blocks.39.ffn.fc1.weight, module.blocks.39.ffn.fc2.weight, module.blocks.39.ca_norm.weight, module.blocks.40.sa.mat_qkv.weight, module.blocks.40.sa.proj.weight, '\n"
                   " 'module.blocks.40.ca.mat_q.weight, module.blocks.40.ca.mat_kv.weight, module.blocks.40.ca.proj.weight, module.blocks.40.ffn.fc1.weight, module.blocks.40.ffn.fc2.weight, '\n"
                   " 'module.blocks.40.ca_norm.weight, module.blocks.41.sa.mat_qkv.weight, module.blocks.41.sa.proj.weight, module.blocks.41.ca.mat_q.weight, module.blocks.41.ca.mat_kv.weight, '\n"
                   " 'module.blocks.41.ca.proj.weight, module.blocks.41.ffn.fc1.weight, module.blocks.41.ffn.fc2.weight, module.blocks.41.ca_norm.weight, module.blocks.42.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.42.sa.proj.weight, module.blocks.42.ca.mat_q.weight, module.blocks.42.ca.mat_kv.weight, module.blocks.42.ca.proj.weight, module.blocks.42.ffn.fc1.weight, '\n"
                   " 'module.blocks.42.ffn.fc2.weight, module.blocks.42.ca_norm.weight, module.blocks.43.sa.mat_qkv.weight, module.blocks.43.sa.proj.weight, module.blocks.43.ca.mat_q.weight, '\n"
                   " 'module.blocks.43.ca.mat_kv.weight, module.blocks.43.ca.proj.weight, module.blocks.43.ffn.fc1.weight, module.blocks.43.ffn.fc2.weight, module.blocks.43.ca_norm.weight, '\n"
                   " 'module.blocks.44.sa.mat_qkv.weight, module.blocks.44.sa.proj.weight, module.blocks.44.ca.mat_q.weight, module.blocks.44.ca.mat_kv.weight, module.blocks.44.ca.proj.weight, '\n"
                   " 'module.blocks.44.ffn.fc1.weight, module.blocks.44.ffn.fc2.weight, module.blocks.44.ca_norm.weight, module.blocks.45.sa.mat_qkv.weight, module.blocks.45.sa.proj.weight, '\n"
                   " 'module.blocks.45.ca.mat_q.weight, module.blocks.45.ca.mat_kv.weight, module.blocks.45.ca.proj.weight, module.blocks.45.ffn.fc1.weight, module.blocks.45.ffn.fc2.weight, '\n"
                   " 'module.blocks.45.ca_norm.weight, module.blocks.46.sa.mat_qkv.weight, module.blocks.46.sa.proj.weight, module.blocks.46.ca.mat_q.weight, module.blocks.46.ca.mat_kv.weight, '\n"
                   " 'module.blocks.46.ca.proj.weight, module.blocks.46.ffn.fc1.weight, module.blocks.46.ffn.fc2.weight, module.blocks.46.ca_norm.weight, module.blocks.47.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.47.sa.proj.weight, module.blocks.47.ca.mat_q.weight, module.blocks.47.ca.mat_kv.weight, module.blocks.47.ca.proj.weight, module.blocks.47.ffn.fc1.weight, '\n"
                   " 'module.blocks.47.ffn.fc2.weight, module.blocks.47.ca_norm.weight, module.blocks.48.sa.mat_qkv.weight, module.blocks.48.sa.proj.weight, module.blocks.48.ca.mat_q.weight, '\n"
                   " 'module.blocks.48.ca.mat_kv.weight, module.blocks.48.ca.proj.weight, module.blocks.48.ffn.fc1.weight, module.blocks.48.ffn.fc2.weight, module.blocks.48.ca_norm.weight, '\n"
                   " 'module.blocks.49.sa.mat_qkv.weight, module.blocks.49.sa.proj.weight, module.blocks.49.ca.mat_q.weight, module.blocks.49.ca.mat_kv.weight, module.blocks.49.ca.proj.weight, '\n"
                   " 'module.blocks.49.ffn.fc1.weight, module.blocks.49.ffn.fc2.weight, module.blocks.49.ca_norm.weight, module.blocks.50.sa.mat_qkv.weight, module.blocks.50.sa.proj.weight, '\n"
                   " 'module.blocks.50.ca.mat_q.weight, module.blocks.50.ca.mat_kv.weight, module.blocks.50.ca.proj.weight, module.blocks.50.ffn.fc1.weight, module.blocks.50.ffn.fc2.weight, '\n"
                   " 'module.blocks.50.ca_norm.weight, module.blocks.51.sa.mat_qkv.weight, module.blocks.51.sa.proj.weight, module.blocks.51.ca.mat_q.weight, module.blocks.51.ca.mat_kv.weight, '\n"
                   " 'module.blocks.51.ca.proj.weight, module.blocks.51.ffn.fc1.weight, module.blocks.51.ffn.fc2.weight, module.blocks.51.ca_norm.weight, module.blocks.52.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.52.sa.proj.weight, module.blocks.52.ca.mat_q.weight, module.blocks.52.ca.mat_kv.weight, module.blocks.52.ca.proj.weight, module.blocks.52.ffn.fc1.weight, '\n"
                   " 'module.blocks.52.ffn.fc2.weight, module.blocks.52.ca_norm.weight, module.blocks.53.sa.mat_qkv.weight, module.blocks.53.sa.proj.weight, module.blocks.53.ca.mat_q.weight, '\n"
                   " 'module.blocks.53.ca.mat_kv.weight, module.blocks.53.ca.proj.weight, module.blocks.53.ffn.fc1.weight, module.blocks.53.ffn.fc2.weight, module.blocks.53.ca_norm.weight, '\n"
                   " 'module.blocks.54.sa.mat_qkv.weight, module.blocks.54.sa.proj.weight, module.blocks.54.ca.mat_q.weight, module.blocks.54.ca.mat_kv.weight, module.blocks.54.ca.proj.weight, '\n"
                   " 'module.blocks.54.ffn.fc1.weight, module.blocks.54.ffn.fc2.weight, module.blocks.54.ca_norm.weight, module.blocks.55.sa.mat_qkv.weight, module.blocks.55.sa.proj.weight, '\n"
                   " 'module.blocks.55.ca.mat_q.weight, module.blocks.55.ca.mat_kv.weight, module.blocks.55.ca.proj.weight, module.blocks.55.ffn.fc1.weight, module.blocks.55.ffn.fc2.weight, '\n"
                   " 'module.blocks.55.ca_norm.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': '[no scale]',
          'params': "('module.cfg_uncond, module.pos_start, module.text_proj_for_sos.ca.mat_q, module.text_proj_for_sos.ca.v_bias, module.text_proj_for_sos.ca.proj.bias, module.text_proj_for_ca.0.bias, '\n"
                    " 'module.text_proj_for_ca.2.bias, module.lvl_embed.weight, module.word_embed.bias, module.shared_ada_lin.1.bias, module.head_nm.ada_lin.1.bias, module.head.bias, module.blocks.0.ada_gss, '\n"
                    " 'module.blocks.0.sa.scale_mul_1H11, module.blocks.0.sa.q_bias, module.blocks.0.sa.v_bias, module.blocks.0.sa.proj.bias, module.blocks.0.ca.v_bias, module.blocks.0.ca.mat_q.bias, '\n"
                    " 'module.blocks.0.ca.proj.bias, module.blocks.0.ffn.fc1.bias, module.blocks.0.ffn.fc2.bias, module.blocks.0.ca_norm.bias, module.blocks.1.ada_gss, module.blocks.1.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.1.sa.q_bias, module.blocks.1.sa.v_bias, module.blocks.1.sa.proj.bias, module.blocks.1.ca.v_bias, module.blocks.1.ca.mat_q.bias, module.blocks.1.ca.proj.bias, '\n"
                    " 'module.blocks.1.ffn.fc1.bias, module.blocks.1.ffn.fc2.bias, module.blocks.1.ca_norm.bias, module.blocks.2.ada_gss, module.blocks.2.sa.scale_mul_1H11, module.blocks.2.sa.q_bias, '\n"
                    " 'module.blocks.2.sa.v_bias, module.blocks.2.sa.proj.bias, module.blocks.2.ca.v_bias, module.blocks.2.ca.mat_q.bias, module.blocks.2.ca.proj.bias, module.blocks.2.ffn.fc1.bias, '\n"
                    " 'module.blocks.2.ffn.fc2.bias, module.blocks.2.ca_norm.bias, module.blocks.3.ada_gss, module.blocks.3.sa.scale_mul_1H11, module.blocks.3.sa.q_bias, module.blocks.3.sa.v_bias, '\n"
                    " 'module.blocks.3.sa.proj.bias, module.blocks.3.ca.v_bias, module.blocks.3.ca.mat_q.bias, module.blocks.3.ca.proj.bias, module.blocks.3.ffn.fc1.bias, module.blocks.3.ffn.fc2.bias, '\n"
                    " 'module.blocks.3.ca_norm.bias, module.blocks.4.ada_gss, module.blocks.4.sa.scale_mul_1H11, module.blocks.4.sa.q_bias, module.blocks.4.sa.v_bias, module.blocks.4.sa.proj.bias, '\n"
                    " 'module.blocks.4.ca.v_bias, module.blocks.4.ca.mat_q.bias, module.blocks.4.ca.proj.bias, module.blocks.4.ffn.fc1.bias, module.blocks.4.ffn.fc2.bias, module.blocks.4.ca_norm.bias, '\n"
                    " 'module.blocks.5.ada_gss, module.blocks.5.sa.scale_mul_1H11, module.blocks.5.sa.q_bias, module.blocks.5.sa.v_bias, module.blocks.5.sa.proj.bias, module.blocks.5.ca.v_bias, '\n"
                    " 'module.blocks.5.ca.mat_q.bias, module.blocks.5.ca.proj.bias, module.blocks.5.ffn.fc1.bias, module.blocks.5.ffn.fc2.bias, module.blocks.5.ca_norm.bias, module.blocks.6.ada_gss, '\n"
                    " 'module.blocks.6.sa.scale_mul_1H11, module.blocks.6.sa.q_bias, module.blocks.6.sa.v_bias, module.blocks.6.sa.proj.bias, module.blocks.6.ca.v_bias, module.blocks.6.ca.mat_q.bias, '\n"
                    " 'module.blocks.6.ca.proj.bias, module.blocks.6.ffn.fc1.bias, module.blocks.6.ffn.fc2.bias, module.blocks.6.ca_norm.bias, module.blocks.7.ada_gss, module.blocks.7.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.7.sa.q_bias, module.blocks.7.sa.v_bias, module.blocks.7.sa.proj.bias, module.blocks.7.ca.v_bias, module.blocks.7.ca.mat_q.bias, module.blocks.7.ca.proj.bias, '\n"
                    " 'module.blocks.7.ffn.fc1.bias, module.blocks.7.ffn.fc2.bias, module.blocks.7.ca_norm.bias, module.blocks.8.ada_gss, module.blocks.8.sa.scale_mul_1H11, module.blocks.8.sa.q_bias, '\n"
                    " 'module.blocks.8.sa.v_bias, module.blocks.8.sa.proj.bias, module.blocks.8.ca.v_bias, module.blocks.8.ca.mat_q.bias, module.blocks.8.ca.proj.bias, module.blocks.8.ffn.fc1.bias, '\n"
                    " 'module.blocks.8.ffn.fc2.bias, module.blocks.8.ca_norm.bias, module.blocks.9.ada_gss, module.blocks.9.sa.scale_mul_1H11, module.blocks.9.sa.q_bias, module.blocks.9.sa.v_bias, '\n"
                    " 'module.blocks.9.sa.proj.bias, module.blocks.9.ca.v_bias, module.blocks.9.ca.mat_q.bias, module.blocks.9.ca.proj.bias, module.blocks.9.ffn.fc1.bias, module.blocks.9.ffn.fc2.bias, '\n"
                    " 'module.blocks.9.ca_norm.bias, module.blocks.10.ada_gss, module.blocks.10.sa.scale_mul_1H11, module.blocks.10.sa.q_bias, module.blocks.10.sa.v_bias, module.blocks.10.sa.proj.bias, '\n"
                    " 'module.blocks.10.ca.v_bias, module.blocks.10.ca.mat_q.bias, module.blocks.10.ca.proj.bias, module.blocks.10.ffn.fc1.bias, module.blocks.10.ffn.fc2.bias, module.blocks.10.ca_norm.bias, '\n"
                    " 'module.blocks.11.ada_gss, module.blocks.11.sa.scale_mul_1H11, module.blocks.11.sa.q_bias, module.blocks.11.sa.v_bias, module.blocks.11.sa.proj.bias, module.blocks.11.ca.v_bias, '\n"
                    " 'module.blocks.11.ca.mat_q.bias, module.blocks.11.ca.proj.bias, module.blocks.11.ffn.fc1.bias, module.blocks.11.ffn.fc2.bias, module.blocks.11.ca_norm.bias, module.blocks.12.ada_gss, '\n"
                    " 'module.blocks.12.sa.scale_mul_1H11, module.blocks.12.sa.q_bias, module.blocks.12.sa.v_bias, module.blocks.12.sa.proj.bias, module.blocks.12.ca.v_bias, module.blocks.12.ca.mat_q.bias, '\n"
                    " 'module.blocks.12.ca.proj.bias, module.blocks.12.ffn.fc1.bias, module.blocks.12.ffn.fc2.bias, module.blocks.12.ca_norm.bias, module.blocks.13.ada_gss, module.blocks.13.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.13.sa.q_bias, module.blocks.13.sa.v_bias, module.blocks.13.sa.proj.bias, module.blocks.13.ca.v_bias, module.blocks.13.ca.mat_q.bias, module.blocks.13.ca.proj.bias, '\n"
                    " 'module.blocks.13.ffn.fc1.bias, module.blocks.13.ffn.fc2.bias, module.blocks.13.ca_norm.bias, module.blocks.14.ada_gss, module.blocks.14.sa.scale_mul_1H11, module.blocks.14.sa.q_bias, '\n"
                    " 'module.blocks.14.sa.v_bias, module.blocks.14.sa.proj.bias, module.blocks.14.ca.v_bias, module.blocks.14.ca.mat_q.bias, module.blocks.14.ca.proj.bias, module.blocks.14.ffn.fc1.bias, '\n"
                    " 'module.blocks.14.ffn.fc2.bias, module.blocks.14.ca_norm.bias, module.blocks.15.ada_gss, module.blocks.15.sa.scale_mul_1H11, module.blocks.15.sa.q_bias, module.blocks.15.sa.v_bias, '\n"
                    " 'module.blocks.15.sa.proj.bias, module.blocks.15.ca.v_bias, module.blocks.15.ca.mat_q.bias, module.blocks.15.ca.proj.bias, module.blocks.15.ffn.fc1.bias, module.blocks.15.ffn.fc2.bias, '\n"
                    " 'module.blocks.15.ca_norm.bias, module.blocks.16.ada_gss, module.blocks.16.sa.scale_mul_1H11, module.blocks.16.sa.q_bias, module.blocks.16.sa.v_bias, module.blocks.16.sa.proj.bias, '\n"
                    " 'module.blocks.16.ca.v_bias, module.blocks.16.ca.mat_q.bias, module.blocks.16.ca.proj.bias, module.blocks.16.ffn.fc1.bias, module.blocks.16.ffn.fc2.bias, module.blocks.16.ca_norm.bias, '\n"
                    " 'module.blocks.17.ada_gss, module.blocks.17.sa.scale_mul_1H11, module.blocks.17.sa.q_bias, module.blocks.17.sa.v_bias, module.blocks.17.sa.proj.bias, module.blocks.17.ca.v_bias, '\n"
                    " 'module.blocks.17.ca.mat_q.bias, module.blocks.17.ca.proj.bias, module.blocks.17.ffn.fc1.bias, module.blocks.17.ffn.fc2.bias, module.blocks.17.ca_norm.bias, module.blocks.18.ada_gss, '\n"
                    " 'module.blocks.18.sa.scale_mul_1H11, module.blocks.18.sa.q_bias, module.blocks.18.sa.v_bias, module.blocks.18.sa.proj.bias, module.blocks.18.ca.v_bias, module.blocks.18.ca.mat_q.bias, '\n"
                    " 'module.blocks.18.ca.proj.bias, module.blocks.18.ffn.fc1.bias, module.blocks.18.ffn.fc2.bias, module.blocks.18.ca_norm.bias, module.blocks.19.ada_gss, module.blocks.19.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.19.sa.q_bias, module.blocks.19.sa.v_bias, module.blocks.19.sa.proj.bias, module.blocks.19.ca.v_bias, module.blocks.19.ca.mat_q.bias, module.blocks.19.ca.proj.bias, '\n"
                    " 'module.blocks.19.ffn.fc1.bias, module.blocks.19.ffn.fc2.bias, module.blocks.19.ca_norm.bias, module.blocks.20.ada_gss, module.blocks.20.sa.scale_mul_1H11, module.blocks.20.sa.q_bias, '\n"
                    " 'module.blocks.20.sa.v_bias, module.blocks.20.sa.proj.bias, module.blocks.20.ca.v_bias, module.blocks.20.ca.mat_q.bias, module.blocks.20.ca.proj.bias, module.blocks.20.ffn.fc1.bias, '\n"
                    " 'module.blocks.20.ffn.fc2.bias, module.blocks.20.ca_norm.bias, module.blocks.21.ada_gss, module.blocks.21.sa.scale_mul_1H11, module.blocks.21.sa.q_bias, module.blocks.21.sa.v_bias, '\n"
                    " 'module.blocks.21.sa.proj.bias, module.blocks.21.ca.v_bias, module.blocks.21.ca.mat_q.bias, module.blocks.21.ca.proj.bias, module.blocks.21.ffn.fc1.bias, module.blocks.21.ffn.fc2.bias, '\n"
                    " 'module.blocks.21.ca_norm.bias, module.blocks.22.ada_gss, module.blocks.22.sa.scale_mul_1H11, module.blocks.22.sa.q_bias, module.blocks.22.sa.v_bias, module.blocks.22.sa.proj.bias, '\n"
                    " 'module.blocks.22.ca.v_bias, module.blocks.22.ca.mat_q.bias, module.blocks.22.ca.proj.bias, module.blocks.22.ffn.fc1.bias, module.blocks.22.ffn.fc2.bias, module.blocks.22.ca_norm.bias, '\n"
                    " 'module.blocks.23.ada_gss, module.blocks.23.sa.scale_mul_1H11, module.blocks.23.sa.q_bias, module.blocks.23.sa.v_bias, module.blocks.23.sa.proj.bias, module.blocks.23.ca.v_bias, '\n"
                    " 'module.blocks.23.ca.mat_q.bias, module.blocks.23.ca.proj.bias, module.blocks.23.ffn.fc1.bias, module.blocks.23.ffn.fc2.bias, module.blocks.23.ca_norm.bias, module.blocks.24.ada_gss, '\n"
                    " 'module.blocks.24.sa.scale_mul_1H11, module.blocks.24.sa.q_bias, module.blocks.24.sa.v_bias, module.blocks.24.sa.proj.bias, module.blocks.24.ca.v_bias, module.blocks.24.ca.mat_q.bias, '\n"
                    " 'module.blocks.24.ca.proj.bias, module.blocks.24.ffn.fc1.bias, module.blocks.24.ffn.fc2.bias, module.blocks.24.ca_norm.bias, module.blocks.25.ada_gss, module.blocks.25.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.25.sa.q_bias, module.blocks.25.sa.v_bias, module.blocks.25.sa.proj.bias, module.blocks.25.ca.v_bias, module.blocks.25.ca.mat_q.bias, module.blocks.25.ca.proj.bias, '\n"
                    " 'module.blocks.25.ffn.fc1.bias, module.blocks.25.ffn.fc2.bias, module.blocks.25.ca_norm.bias, module.blocks.26.ada_gss, module.blocks.26.sa.scale_mul_1H11, module.blocks.26.sa.q_bias, '\n"
                    " 'module.blocks.26.sa.v_bias, module.blocks.26.sa.proj.bias, module.blocks.26.ca.v_bias, module.blocks.26.ca.mat_q.bias, module.blocks.26.ca.proj.bias, module.blocks.26.ffn.fc1.bias, '\n"
                    " 'module.blocks.26.ffn.fc2.bias, module.blocks.26.ca_norm.bias, module.blocks.27.ada_gss, module.blocks.27.sa.scale_mul_1H11, module.blocks.27.sa.q_bias, module.blocks.27.sa.v_bias, '\n"
                    " 'module.blocks.27.sa.proj.bias, module.blocks.27.ca.v_bias, module.blocks.27.ca.mat_q.bias, module.blocks.27.ca.proj.bias, module.blocks.27.ffn.fc1.bias, module.blocks.27.ffn.fc2.bias, '\n"
                    " 'module.blocks.27.ca_norm.bias, module.blocks.28.ada_gss, module.blocks.28.sa.scale_mul_1H11, module.blocks.28.sa.q_bias, module.blocks.28.sa.v_bias, module.blocks.28.sa.proj.bias, '\n"
                    " 'module.blocks.28.ca.v_bias, module.blocks.28.ca.mat_q.bias, module.blocks.28.ca.proj.bias, module.blocks.28.ffn.fc1.bias, module.blocks.28.ffn.fc2.bias, module.blocks.28.ca_norm.bias, '\n"
                    " 'module.blocks.29.ada_gss, module.blocks.29.sa.scale_mul_1H11, module.blocks.29.sa.q_bias, module.blocks.29.sa.v_bias, module.blocks.29.sa.proj.bias, module.blocks.29.ca.v_bias, '\n"
                    " 'module.blocks.29.ca.mat_q.bias, module.blocks.29.ca.proj.bias, module.blocks.29.ffn.fc1.bias, module.blocks.29.ffn.fc2.bias, module.blocks.29.ca_norm.bias, module.blocks.30.ada_gss, '\n"
                    " 'module.blocks.30.sa.scale_mul_1H11, module.blocks.30.sa.q_bias, module.blocks.30.sa.v_bias, module.blocks.30.sa.proj.bias, module.blocks.30.ca.v_bias, module.blocks.30.ca.mat_q.bias, '\n"
                    " 'module.blocks.30.ca.proj.bias, module.blocks.30.ffn.fc1.bias, module.blocks.30.ffn.fc2.bias, module.blocks.30.ca_norm.bias, module.blocks.31.ada_gss, module.blocks.31.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.31.sa.q_bias, module.blocks.31.sa.v_bias, module.blocks.31.sa.proj.bias, module.blocks.31.ca.v_bias, module.blocks.31.ca.mat_q.bias, module.blocks.31.ca.proj.bias, '\n"
                    " 'module.blocks.31.ffn.fc1.bias, module.blocks.31.ffn.fc2.bias, module.blocks.31.ca_norm.bias, module.blocks.32.ada_gss, module.blocks.32.sa.scale_mul_1H11, module.blocks.32.sa.q_bias, '\n"
                    " 'module.blocks.32.sa.v_bias, module.blocks.32.sa.proj.bias, module.blocks.32.ca.v_bias, module.blocks.32.ca.mat_q.bias, module.blocks.32.ca.proj.bias, module.blocks.32.ffn.fc1.bias, '\n"
                    " 'module.blocks.32.ffn.fc2.bias, module.blocks.32.ca_norm.bias, module.blocks.33.ada_gss, module.blocks.33.sa.scale_mul_1H11, module.blocks.33.sa.q_bias, module.blocks.33.sa.v_bias, '\n"
                    " 'module.blocks.33.sa.proj.bias, module.blocks.33.ca.v_bias, module.blocks.33.ca.mat_q.bias, module.blocks.33.ca.proj.bias, module.blocks.33.ffn.fc1.bias, module.blocks.33.ffn.fc2.bias, '\n"
                    " 'module.blocks.33.ca_norm.bias, module.blocks.34.ada_gss, module.blocks.34.sa.scale_mul_1H11, module.blocks.34.sa.q_bias, module.blocks.34.sa.v_bias, module.blocks.34.sa.proj.bias, '\n"
                    " 'module.blocks.34.ca.v_bias, module.blocks.34.ca.mat_q.bias, module.blocks.34.ca.proj.bias, module.blocks.34.ffn.fc1.bias, module.blocks.34.ffn.fc2.bias, module.blocks.34.ca_norm.bias, '\n"
                    " 'module.blocks.35.ada_gss, module.blocks.35.sa.scale_mul_1H11, module.blocks.35.sa.q_bias, module.blocks.35.sa.v_bias, module.blocks.35.sa.proj.bias, module.blocks.35.ca.v_bias, '\n"
                    " 'module.blocks.35.ca.mat_q.bias, module.blocks.35.ca.proj.bias, module.blocks.35.ffn.fc1.bias, module.blocks.35.ffn.fc2.bias, module.blocks.35.ca_norm.bias, module.blocks.36.ada_gss, '\n"
                    " 'module.blocks.36.sa.scale_mul_1H11, module.blocks.36.sa.q_bias, module.blocks.36.sa.v_bias, module.blocks.36.sa.proj.bias, module.blocks.36.ca.v_bias, module.blocks.36.ca.mat_q.bias, '\n"
                    " 'module.blocks.36.ca.proj.bias, module.blocks.36.ffn.fc1.bias, module.blocks.36.ffn.fc2.bias, module.blocks.36.ca_norm.bias, module.blocks.37.ada_gss, module.blocks.37.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.37.sa.q_bias, module.blocks.37.sa.v_bias, module.blocks.37.sa.proj.bias, module.blocks.37.ca.v_bias, module.blocks.37.ca.mat_q.bias, module.blocks.37.ca.proj.bias, '\n"
                    " 'module.blocks.37.ffn.fc1.bias, module.blocks.37.ffn.fc2.bias, module.blocks.37.ca_norm.bias, module.blocks.38.ada_gss, module.blocks.38.sa.scale_mul_1H11, module.blocks.38.sa.q_bias, '\n"
                    " 'module.blocks.38.sa.v_bias, module.blocks.38.sa.proj.bias, module.blocks.38.ca.v_bias, module.blocks.38.ca.mat_q.bias, module.blocks.38.ca.proj.bias, module.blocks.38.ffn.fc1.bias, '\n"
                    " 'module.blocks.38.ffn.fc2.bias, module.blocks.38.ca_norm.bias, module.blocks.39.ada_gss, module.blocks.39.sa.scale_mul_1H11, module.blocks.39.sa.q_bias, module.blocks.39.sa.v_bias, '\n"
                    " 'module.blocks.39.sa.proj.bias, module.blocks.39.ca.v_bias, module.blocks.39.ca.mat_q.bias, module.blocks.39.ca.proj.bias, module.blocks.39.ffn.fc1.bias, module.blocks.39.ffn.fc2.bias, '\n"
                    " 'module.blocks.39.ca_norm.bias, module.blocks.40.ada_gss, module.blocks.40.sa.scale_mul_1H11, module.blocks.40.sa.q_bias, module.blocks.40.sa.v_bias, module.blocks.40.sa.proj.bias, '\n"
                    " 'module.blocks.40.ca.v_bias, module.blocks.40.ca.mat_q.bias, module.blocks.40.ca.proj.bias, module.blocks.40.ffn.fc1.bias, module.blocks.40.ffn.fc2.bias, module.blocks.40.ca_norm.bias, '\n"
                    " 'module.blocks.41.ada_gss, module.blocks.41.sa.scale_mul_1H11, module.blocks.41.sa.q_bias, module.blocks.41.sa.v_bias, module.blocks.41.sa.proj.bias, module.blocks.41.ca.v_bias, '\n"
                    " 'module.blocks.41.ca.mat_q.bias, module.blocks.41.ca.proj.bias, module.blocks.41.ffn.fc1.bias, module.blocks.41.ffn.fc2.bias, module.blocks.41.ca_norm.bias, module.blocks.42.ada_gss, '\n"
                    " 'module.blocks.42.sa.scale_mul_1H11, module.blocks.42.sa.q_bias, module.blocks.42.sa.v_bias, module.blocks.42.sa.proj.bias, module.blocks.42.ca.v_bias, module.blocks.42.ca.mat_q.bias, '\n"
                    " 'module.blocks.42.ca.proj.bias, module.blocks.42.ffn.fc1.bias, module.blocks.42.ffn.fc2.bias, module.blocks.42.ca_norm.bias, module.blocks.43.ada_gss, module.blocks.43.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.43.sa.q_bias, module.blocks.43.sa.v_bias, module.blocks.43.sa.proj.bias, module.blocks.43.ca.v_bias, module.blocks.43.ca.mat_q.bias, module.blocks.43.ca.proj.bias, '\n"
                    " 'module.blocks.43.ffn.fc1.bias, module.blocks.43.ffn.fc2.bias, module.blocks.43.ca_norm.bias, module.blocks.44.ada_gss, module.blocks.44.sa.scale_mul_1H11, module.blocks.44.sa.q_bias, '\n"
                    " 'module.blocks.44.sa.v_bias, module.blocks.44.sa.proj.bias, module.blocks.44.ca.v_bias, module.blocks.44.ca.mat_q.bias, module.blocks.44.ca.proj.bias, module.blocks.44.ffn.fc1.bias, '\n"
                    " 'module.blocks.44.ffn.fc2.bias, module.blocks.44.ca_norm.bias, module.blocks.45.ada_gss, module.blocks.45.sa.scale_mul_1H11, module.blocks.45.sa.q_bias, module.blocks.45.sa.v_bias, '\n"
                    " 'module.blocks.45.sa.proj.bias, module.blocks.45.ca.v_bias, module.blocks.45.ca.mat_q.bias, module.blocks.45.ca.proj.bias, module.blocks.45.ffn.fc1.bias, module.blocks.45.ffn.fc2.bias, '\n"
                    " 'module.blocks.45.ca_norm.bias, module.blocks.46.ada_gss, module.blocks.46.sa.scale_mul_1H11, module.blocks.46.sa.q_bias, module.blocks.46.sa.v_bias, module.blocks.46.sa.proj.bias, '\n"
                    " 'module.blocks.46.ca.v_bias, module.blocks.46.ca.mat_q.bias, module.blocks.46.ca.proj.bias, module.blocks.46.ffn.fc1.bias, module.blocks.46.ffn.fc2.bias, module.blocks.46.ca_norm.bias, '\n"
                    " 'module.blocks.47.ada_gss, module.blocks.47.sa.scale_mul_1H11, module.blocks.47.sa.q_bias, module.blocks.47.sa.v_bias, module.blocks.47.sa.proj.bias, module.blocks.47.ca.v_bias, '\n"
                    " 'module.blocks.47.ca.mat_q.bias, module.blocks.47.ca.proj.bias, module.blocks.47.ffn.fc1.bias, module.blocks.47.ffn.fc2.bias, module.blocks.47.ca_norm.bias, module.blocks.48.ada_gss, '\n"
                    " 'module.blocks.48.sa.scale_mul_1H11, module.blocks.48.sa.q_bias, module.blocks.48.sa.v_bias, module.blocks.48.sa.proj.bias, module.blocks.48.ca.v_bias, module.blocks.48.ca.mat_q.bias, '\n"
                    " 'module.blocks.48.ca.proj.bias, module.blocks.48.ffn.fc1.bias, module.blocks.48.ffn.fc2.bias, module.blocks.48.ca_norm.bias, module.blocks.49.ada_gss, module.blocks.49.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.49.sa.q_bias, module.blocks.49.sa.v_bias, module.blocks.49.sa.proj.bias, module.blocks.49.ca.v_bias, module.blocks.49.ca.mat_q.bias, module.blocks.49.ca.proj.bias, '\n"
                    " 'module.blocks.49.ffn.fc1.bias, module.blocks.49.ffn.fc2.bias, module.blocks.49.ca_norm.bias, module.blocks.50.ada_gss, module.blocks.50.sa.scale_mul_1H11, module.blocks.50.sa.q_bias, '\n"
                    " 'module.blocks.50.sa.v_bias, module.blocks.50.sa.proj.bias, module.blocks.50.ca.v_bias, module.blocks.50.ca.mat_q.bias, module.blocks.50.ca.proj.bias, module.blocks.50.ffn.fc1.bias, '\n"
                    " 'module.blocks.50.ffn.fc2.bias, module.blocks.50.ca_norm.bias, module.blocks.51.ada_gss, module.blocks.51.sa.scale_mul_1H11, module.blocks.51.sa.q_bias, module.blocks.51.sa.v_bias, '\n"
                    " 'module.blocks.51.sa.proj.bias, module.blocks.51.ca.v_bias, module.blocks.51.ca.mat_q.bias, module.blocks.51.ca.proj.bias, module.blocks.51.ffn.fc1.bias, module.blocks.51.ffn.fc2.bias, '\n"
                    " 'module.blocks.51.ca_norm.bias, module.blocks.52.ada_gss, module.blocks.52.sa.scale_mul_1H11, module.blocks.52.sa.q_bias, module.blocks.52.sa.v_bias, module.blocks.52.sa.proj.bias, '\n"
                    " 'module.blocks.52.ca.v_bias, module.blocks.52.ca.mat_q.bias, module.blocks.52.ca.proj.bias, module.blocks.52.ffn.fc1.bias, module.blocks.52.ffn.fc2.bias, module.blocks.52.ca_norm.bias, '\n"
                    " 'module.blocks.53.ada_gss, module.blocks.53.sa.scale_mul_1H11, module.blocks.53.sa.q_bias, module.blocks.53.sa.v_bias, module.blocks.53.sa.proj.bias, module.blocks.53.ca.v_bias, '\n"
                    " 'module.blocks.53.ca.mat_q.bias, module.blocks.53.ca.proj.bias, module.blocks.53.ffn.fc1.bias, module.blocks.53.ffn.fc2.bias, module.blocks.53.ca_norm.bias, module.blocks.54.ada_gss, '\n"
                    " 'module.blocks.54.sa.scale_mul_1H11, module.blocks.54.sa.q_bias, module.blocks.54.sa.v_bias, module.blocks.54.sa.proj.bias, module.blocks.54.ca.v_bias, module.blocks.54.ca.mat_q.bias, '\n"
                    " 'module.blocks.54.ca.proj.bias, module.blocks.54.ffn.fc1.bias, module.blocks.54.ffn.fc2.bias, module.blocks.54.ca_norm.bias, module.blocks.55.ada_gss, module.blocks.55.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.55.sa.q_bias, module.blocks.55.sa.v_bias, module.blocks.55.sa.proj.bias, module.blocks.55.ca.v_bias, module.blocks.55.ca.mat_q.bias, module.blocks.55.ca.proj.bias, '\n"
                    " 'module.blocks.55.ffn.fc1.bias, module.blocks.55.ffn.fc2.bias, module.blocks.55.ca_norm.bias')",
          'wd_sc': 0.0}}

[09-01 22:44:11] (nity/utils/lr_control.py, line 116)=> [get_param_groups][rank0] type(model).__name__='NullDDP' count=1085, numel=8573892016
[09-01 22:44:11] (nity/utils/lr_control.py, line 117)=> 
[09-01 22:44:11] (inity_circuit_breaker.py, line 343)=> [vgpt] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.97), fused=True), opt_kw={'lr': 9.375e-05, 'weight_decay': 0}

[09-01 22:44:11] (inity_circuit_breaker.py, line 348)=> Loading T5 from weights/flan-t5-xl...







=======================================================   RESTART [09-01 22:47:47]   =======================================================
[09-01 22:47:47] (inity_circuit_breaker.py, line 576)=> ============================================================
[09-01 22:47:47] (inity_circuit_breaker.py, line 577)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[09-01 22:47:47] (inity_circuit_breaker.py, line 578)=> Circuit Breaker Alpha: 0.1
[09-01 22:47:47] (inity_circuit_breaker.py, line 579)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[09-01 22:47:47] (inity_circuit_breaker.py, line 580)=> Circuit Breaker Enabled: True
[09-01 22:47:47] (inity_circuit_breaker.py, line 581)=> Number of Examples: 1000
[09-01 22:47:47] (inity_circuit_breaker.py, line 582)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[09-01 22:47:47] (inity_circuit_breaker.py, line 583)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[09-01 22:47:47] (inity_circuit_breaker.py, line 584)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[09-01 22:47:47] (inity_circuit_breaker.py, line 585)=> Validation Ratio: 0.1
[09-01 22:47:47] (inity_circuit_breaker.py, line 586)=> Category: hate
[09-01 22:47:47] (inity_circuit_breaker.py, line 587)=> Batch Size: 4
[09-01 22:47:47] (inity_circuit_breaker.py, line 588)=> Workers: 4
[09-01 22:47:47] (inity_circuit_breaker.py, line 589)=> Device: cuda:0
[09-01 22:47:47] (inity_circuit_breaker.py, line 590)=> Model: infinity_8b
[09-01 22:47:47] (inity_circuit_breaker.py, line 591)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-01 22:47:47] (inity_circuit_breaker.py, line 592)=> Rush Resume: 
[09-01 22:47:47] (inity_circuit_breaker.py, line 593)=> Selective Layers: 0,1,2,3,4,5
[09-01 22:47:47] (inity_circuit_breaker.py, line 594)=> ============================================================
[09-01 22:47:47] (inity_circuit_breaker.py, line 376)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[09-01 22:47:47] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-01 22:47:47] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-01 22:47:47] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[09-01 22:47:47] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-01 22:47:47] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-01 22:47:47] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[09-01 22:47:47] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[09-01 22:47:47] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[09-01 22:47:47] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[09-01 22:47:47] (inity_circuit_breaker.py, line 394)=> args.batch_size=4, vbs=6
[09-01 22:47:47] (inity_circuit_breaker.py, line 417)=> len(dataloader): 100, len(dataset): 100
[09-01 22:47:47] (inity_circuit_breaker.py, line 419)=> total_samples: 200
[09-01 22:47:47] (inity_circuit_breaker.py, line 420)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[09-01 22:47:47] (inity_circuit_breaker.py, line 140)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[09-01 22:47:47] (inity_circuit_breaker.py, line 142)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[09-01 22:47:47] (inity_circuit_breaker.py, line 146)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-01 22:47:50] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[09-01 22:47:50] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[09-01 22:47:50] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[09-01 22:47:50] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[09-01 22:47:52] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[09-01 22:47:54] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[09-01 22:47:54] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[09-01 22:47:54] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[09-01 22:47:54] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[09-01 22:47:54] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

[09-01 22:48:04] (ity/models/init_param.py, line  15)=> [init_weights] Infinity with std=0.02
[09-01 22:48:04] (inity_circuit_breaker.py, line 246)=> [PT] GPT model = Infinity(
  drop_path_rate=0.2
  (norm0_cond): Identity()
  (text_norm): FastRMSNorm(C=2048, eps=1e-06, elementwise_affine=True)
  (text_proj_for_sos): TextAttentivePool(
    (ca): CrossAttention(
      Cq=3072, Ckv=2048, cos_attn=False
      (mat_kv): Linear(in_features=2048, out_features=6144, bias=False)
      (proj): Linear(in_features=3072, out_features=3072, bias=True)
      (proj_drop): Identity()
    )
  )
  (text_proj_for_ca): Sequential(
    (0): Linear(in_features=2048, out_features=3072, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=3072, out_features=3072, bias=True)
  )
  (lvl_embed): Embedding(15, 3072)
  (norm0_ve): Identity()
  (word_embed): Linear(in_features=56, out_features=3072, bias=True)
  (shared_ada_lin): Sequential(
    (0): SiLU()
    (1): SharedAdaLin(in_features=3072, out_features=18432, bias=True)
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=3072, out_features=6144, bias=True)
    )
  )
  (head): Linear(in_features=3072, out_features=112, bias=True)
  (blocks): ModuleList(
    (0): CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): Identity()
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
    (1-55): 55 x CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): DropPath(...)
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
  )
)


[09-01 22:48:04] (inity_circuit_breaker.py, line 248)=> [PT][#para] VAE=81.62, VAE.quant=0.00
[09-01 22:48:04] (inity_circuit_breaker.py, line 251)=> [PT][#para] GPT=8573.89


[09-01 22:48:04] (nity/utils/lr_control.py, line  71)=> [get_param_groups][lr decay] with_lr_scale=False, lr_scale=0.0
[09-01 22:48:04] (nity/utils/lr_control.py, line 111)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': '[no scale]',
         'params': "('module.text_norm.weight, module.text_proj_for_sos.ca.mat_kv.weight, module.text_proj_for_sos.ca.proj.weight, module.text_proj_for_ca.0.weight, module.text_proj_for_ca.2.weight, '\n"
                   " 'module.word_embed.weight, module.shared_ada_lin.1.weight, module.head_nm.ada_lin.1.weight, module.head.weight, module.blocks.0.sa.mat_qkv.weight, module.blocks.0.sa.proj.weight, '\n"
                   " 'module.blocks.0.ca.mat_q.weight, module.blocks.0.ca.mat_kv.weight, module.blocks.0.ca.proj.weight, module.blocks.0.ffn.fc1.weight, module.blocks.0.ffn.fc2.weight, module.blocks.0.ca_norm.weight, '\n"
                   " 'module.blocks.1.sa.mat_qkv.weight, module.blocks.1.sa.proj.weight, module.blocks.1.ca.mat_q.weight, module.blocks.1.ca.mat_kv.weight, module.blocks.1.ca.proj.weight, '\n"
                   " 'module.blocks.1.ffn.fc1.weight, module.blocks.1.ffn.fc2.weight, module.blocks.1.ca_norm.weight, module.blocks.2.sa.mat_qkv.weight, module.blocks.2.sa.proj.weight, module.blocks.2.ca.mat_q.weight, '\n"
                   " 'module.blocks.2.ca.mat_kv.weight, module.blocks.2.ca.proj.weight, module.blocks.2.ffn.fc1.weight, module.blocks.2.ffn.fc2.weight, module.blocks.2.ca_norm.weight, module.blocks.3.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.3.sa.proj.weight, module.blocks.3.ca.mat_q.weight, module.blocks.3.ca.mat_kv.weight, module.blocks.3.ca.proj.weight, module.blocks.3.ffn.fc1.weight, module.blocks.3.ffn.fc2.weight, '\n"
                   " 'module.blocks.3.ca_norm.weight, module.blocks.4.sa.mat_qkv.weight, module.blocks.4.sa.proj.weight, module.blocks.4.ca.mat_q.weight, module.blocks.4.ca.mat_kv.weight, '\n"
                   " 'module.blocks.4.ca.proj.weight, module.blocks.4.ffn.fc1.weight, module.blocks.4.ffn.fc2.weight, module.blocks.4.ca_norm.weight, module.blocks.5.sa.mat_qkv.weight, module.blocks.5.sa.proj.weight, '\n"
                   " 'module.blocks.5.ca.mat_q.weight, module.blocks.5.ca.mat_kv.weight, module.blocks.5.ca.proj.weight, module.blocks.5.ffn.fc1.weight, module.blocks.5.ffn.fc2.weight, module.blocks.5.ca_norm.weight, '\n"
                   " 'module.blocks.6.sa.mat_qkv.weight, module.blocks.6.sa.proj.weight, module.blocks.6.ca.mat_q.weight, module.blocks.6.ca.mat_kv.weight, module.blocks.6.ca.proj.weight, '\n"
                   " 'module.blocks.6.ffn.fc1.weight, module.blocks.6.ffn.fc2.weight, module.blocks.6.ca_norm.weight, module.blocks.7.sa.mat_qkv.weight, module.blocks.7.sa.proj.weight, module.blocks.7.ca.mat_q.weight, '\n"
                   " 'module.blocks.7.ca.mat_kv.weight, module.blocks.7.ca.proj.weight, module.blocks.7.ffn.fc1.weight, module.blocks.7.ffn.fc2.weight, module.blocks.7.ca_norm.weight, module.blocks.8.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.8.sa.proj.weight, module.blocks.8.ca.mat_q.weight, module.blocks.8.ca.mat_kv.weight, module.blocks.8.ca.proj.weight, module.blocks.8.ffn.fc1.weight, module.blocks.8.ffn.fc2.weight, '\n"
                   " 'module.blocks.8.ca_norm.weight, module.blocks.9.sa.mat_qkv.weight, module.blocks.9.sa.proj.weight, module.blocks.9.ca.mat_q.weight, module.blocks.9.ca.mat_kv.weight, '\n"
                   " 'module.blocks.9.ca.proj.weight, module.blocks.9.ffn.fc1.weight, module.blocks.9.ffn.fc2.weight, module.blocks.9.ca_norm.weight, module.blocks.10.sa.mat_qkv.weight, module.blocks.10.sa.proj.weight, '\n"
                   " 'module.blocks.10.ca.mat_q.weight, module.blocks.10.ca.mat_kv.weight, module.blocks.10.ca.proj.weight, module.blocks.10.ffn.fc1.weight, module.blocks.10.ffn.fc2.weight, '\n"
                   " 'module.blocks.10.ca_norm.weight, module.blocks.11.sa.mat_qkv.weight, module.blocks.11.sa.proj.weight, module.blocks.11.ca.mat_q.weight, module.blocks.11.ca.mat_kv.weight, '\n"
                   " 'module.blocks.11.ca.proj.weight, module.blocks.11.ffn.fc1.weight, module.blocks.11.ffn.fc2.weight, module.blocks.11.ca_norm.weight, module.blocks.12.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.12.sa.proj.weight, module.blocks.12.ca.mat_q.weight, module.blocks.12.ca.mat_kv.weight, module.blocks.12.ca.proj.weight, module.blocks.12.ffn.fc1.weight, '\n"
                   " 'module.blocks.12.ffn.fc2.weight, module.blocks.12.ca_norm.weight, module.blocks.13.sa.mat_qkv.weight, module.blocks.13.sa.proj.weight, module.blocks.13.ca.mat_q.weight, '\n"
                   " 'module.blocks.13.ca.mat_kv.weight, module.blocks.13.ca.proj.weight, module.blocks.13.ffn.fc1.weight, module.blocks.13.ffn.fc2.weight, module.blocks.13.ca_norm.weight, '\n"
                   " 'module.blocks.14.sa.mat_qkv.weight, module.blocks.14.sa.proj.weight, module.blocks.14.ca.mat_q.weight, module.blocks.14.ca.mat_kv.weight, module.blocks.14.ca.proj.weight, '\n"
                   " 'module.blocks.14.ffn.fc1.weight, module.blocks.14.ffn.fc2.weight, module.blocks.14.ca_norm.weight, module.blocks.15.sa.mat_qkv.weight, module.blocks.15.sa.proj.weight, '\n"
                   " 'module.blocks.15.ca.mat_q.weight, module.blocks.15.ca.mat_kv.weight, module.blocks.15.ca.proj.weight, module.blocks.15.ffn.fc1.weight, module.blocks.15.ffn.fc2.weight, '\n"
                   " 'module.blocks.15.ca_norm.weight, module.blocks.16.sa.mat_qkv.weight, module.blocks.16.sa.proj.weight, module.blocks.16.ca.mat_q.weight, module.blocks.16.ca.mat_kv.weight, '\n"
                   " 'module.blocks.16.ca.proj.weight, module.blocks.16.ffn.fc1.weight, module.blocks.16.ffn.fc2.weight, module.blocks.16.ca_norm.weight, module.blocks.17.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.17.sa.proj.weight, module.blocks.17.ca.mat_q.weight, module.blocks.17.ca.mat_kv.weight, module.blocks.17.ca.proj.weight, module.blocks.17.ffn.fc1.weight, '\n"
                   " 'module.blocks.17.ffn.fc2.weight, module.blocks.17.ca_norm.weight, module.blocks.18.sa.mat_qkv.weight, module.blocks.18.sa.proj.weight, module.blocks.18.ca.mat_q.weight, '\n"
                   " 'module.blocks.18.ca.mat_kv.weight, module.blocks.18.ca.proj.weight, module.blocks.18.ffn.fc1.weight, module.blocks.18.ffn.fc2.weight, module.blocks.18.ca_norm.weight, '\n"
                   " 'module.blocks.19.sa.mat_qkv.weight, module.blocks.19.sa.proj.weight, module.blocks.19.ca.mat_q.weight, module.blocks.19.ca.mat_kv.weight, module.blocks.19.ca.proj.weight, '\n"
                   " 'module.blocks.19.ffn.fc1.weight, module.blocks.19.ffn.fc2.weight, module.blocks.19.ca_norm.weight, module.blocks.20.sa.mat_qkv.weight, module.blocks.20.sa.proj.weight, '\n"
                   " 'module.blocks.20.ca.mat_q.weight, module.blocks.20.ca.mat_kv.weight, module.blocks.20.ca.proj.weight, module.blocks.20.ffn.fc1.weight, module.blocks.20.ffn.fc2.weight, '\n"
                   " 'module.blocks.20.ca_norm.weight, module.blocks.21.sa.mat_qkv.weight, module.blocks.21.sa.proj.weight, module.blocks.21.ca.mat_q.weight, module.blocks.21.ca.mat_kv.weight, '\n"
                   " 'module.blocks.21.ca.proj.weight, module.blocks.21.ffn.fc1.weight, module.blocks.21.ffn.fc2.weight, module.blocks.21.ca_norm.weight, module.blocks.22.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.22.sa.proj.weight, module.blocks.22.ca.mat_q.weight, module.blocks.22.ca.mat_kv.weight, module.blocks.22.ca.proj.weight, module.blocks.22.ffn.fc1.weight, '\n"
                   " 'module.blocks.22.ffn.fc2.weight, module.blocks.22.ca_norm.weight, module.blocks.23.sa.mat_qkv.weight, module.blocks.23.sa.proj.weight, module.blocks.23.ca.mat_q.weight, '\n"
                   " 'module.blocks.23.ca.mat_kv.weight, module.blocks.23.ca.proj.weight, module.blocks.23.ffn.fc1.weight, module.blocks.23.ffn.fc2.weight, module.blocks.23.ca_norm.weight, '\n"
                   " 'module.blocks.24.sa.mat_qkv.weight, module.blocks.24.sa.proj.weight, module.blocks.24.ca.mat_q.weight, module.blocks.24.ca.mat_kv.weight, module.blocks.24.ca.proj.weight, '\n"
                   " 'module.blocks.24.ffn.fc1.weight, module.blocks.24.ffn.fc2.weight, module.blocks.24.ca_norm.weight, module.blocks.25.sa.mat_qkv.weight, module.blocks.25.sa.proj.weight, '\n"
                   " 'module.blocks.25.ca.mat_q.weight, module.blocks.25.ca.mat_kv.weight, module.blocks.25.ca.proj.weight, module.blocks.25.ffn.fc1.weight, module.blocks.25.ffn.fc2.weight, '\n"
                   " 'module.blocks.25.ca_norm.weight, module.blocks.26.sa.mat_qkv.weight, module.blocks.26.sa.proj.weight, module.blocks.26.ca.mat_q.weight, module.blocks.26.ca.mat_kv.weight, '\n"
                   " 'module.blocks.26.ca.proj.weight, module.blocks.26.ffn.fc1.weight, module.blocks.26.ffn.fc2.weight, module.blocks.26.ca_norm.weight, module.blocks.27.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.27.sa.proj.weight, module.blocks.27.ca.mat_q.weight, module.blocks.27.ca.mat_kv.weight, module.blocks.27.ca.proj.weight, module.blocks.27.ffn.fc1.weight, '\n"
                   " 'module.blocks.27.ffn.fc2.weight, module.blocks.27.ca_norm.weight, module.blocks.28.sa.mat_qkv.weight, module.blocks.28.sa.proj.weight, module.blocks.28.ca.mat_q.weight, '\n"
                   " 'module.blocks.28.ca.mat_kv.weight, module.blocks.28.ca.proj.weight, module.blocks.28.ffn.fc1.weight, module.blocks.28.ffn.fc2.weight, module.blocks.28.ca_norm.weight, '\n"
                   " 'module.blocks.29.sa.mat_qkv.weight, module.blocks.29.sa.proj.weight, module.blocks.29.ca.mat_q.weight, module.blocks.29.ca.mat_kv.weight, module.blocks.29.ca.proj.weight, '\n"
                   " 'module.blocks.29.ffn.fc1.weight, module.blocks.29.ffn.fc2.weight, module.blocks.29.ca_norm.weight, module.blocks.30.sa.mat_qkv.weight, module.blocks.30.sa.proj.weight, '\n"
                   " 'module.blocks.30.ca.mat_q.weight, module.blocks.30.ca.mat_kv.weight, module.blocks.30.ca.proj.weight, module.blocks.30.ffn.fc1.weight, module.blocks.30.ffn.fc2.weight, '\n"
                   " 'module.blocks.30.ca_norm.weight, module.blocks.31.sa.mat_qkv.weight, module.blocks.31.sa.proj.weight, module.blocks.31.ca.mat_q.weight, module.blocks.31.ca.mat_kv.weight, '\n"
                   " 'module.blocks.31.ca.proj.weight, module.blocks.31.ffn.fc1.weight, module.blocks.31.ffn.fc2.weight, module.blocks.31.ca_norm.weight, module.blocks.32.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.32.sa.proj.weight, module.blocks.32.ca.mat_q.weight, module.blocks.32.ca.mat_kv.weight, module.blocks.32.ca.proj.weight, module.blocks.32.ffn.fc1.weight, '\n"
                   " 'module.blocks.32.ffn.fc2.weight, module.blocks.32.ca_norm.weight, module.blocks.33.sa.mat_qkv.weight, module.blocks.33.sa.proj.weight, module.blocks.33.ca.mat_q.weight, '\n"
                   " 'module.blocks.33.ca.mat_kv.weight, module.blocks.33.ca.proj.weight, module.blocks.33.ffn.fc1.weight, module.blocks.33.ffn.fc2.weight, module.blocks.33.ca_norm.weight, '\n"
                   " 'module.blocks.34.sa.mat_qkv.weight, module.blocks.34.sa.proj.weight, module.blocks.34.ca.mat_q.weight, module.blocks.34.ca.mat_kv.weight, module.blocks.34.ca.proj.weight, '\n"
                   " 'module.blocks.34.ffn.fc1.weight, module.blocks.34.ffn.fc2.weight, module.blocks.34.ca_norm.weight, module.blocks.35.sa.mat_qkv.weight, module.blocks.35.sa.proj.weight, '\n"
                   " 'module.blocks.35.ca.mat_q.weight, module.blocks.35.ca.mat_kv.weight, module.blocks.35.ca.proj.weight, module.blocks.35.ffn.fc1.weight, module.blocks.35.ffn.fc2.weight, '\n"
                   " 'module.blocks.35.ca_norm.weight, module.blocks.36.sa.mat_qkv.weight, module.blocks.36.sa.proj.weight, module.blocks.36.ca.mat_q.weight, module.blocks.36.ca.mat_kv.weight, '\n"
                   " 'module.blocks.36.ca.proj.weight, module.blocks.36.ffn.fc1.weight, module.blocks.36.ffn.fc2.weight, module.blocks.36.ca_norm.weight, module.blocks.37.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.37.sa.proj.weight, module.blocks.37.ca.mat_q.weight, module.blocks.37.ca.mat_kv.weight, module.blocks.37.ca.proj.weight, module.blocks.37.ffn.fc1.weight, '\n"
                   " 'module.blocks.37.ffn.fc2.weight, module.blocks.37.ca_norm.weight, module.blocks.38.sa.mat_qkv.weight, module.blocks.38.sa.proj.weight, module.blocks.38.ca.mat_q.weight, '\n"
                   " 'module.blocks.38.ca.mat_kv.weight, module.blocks.38.ca.proj.weight, module.blocks.38.ffn.fc1.weight, module.blocks.38.ffn.fc2.weight, module.blocks.38.ca_norm.weight, '\n"
                   " 'module.blocks.39.sa.mat_qkv.weight, module.blocks.39.sa.proj.weight, module.blocks.39.ca.mat_q.weight, module.blocks.39.ca.mat_kv.weight, module.blocks.39.ca.proj.weight, '\n"
                   " 'module.blocks.39.ffn.fc1.weight, module.blocks.39.ffn.fc2.weight, module.blocks.39.ca_norm.weight, module.blocks.40.sa.mat_qkv.weight, module.blocks.40.sa.proj.weight, '\n"
                   " 'module.blocks.40.ca.mat_q.weight, module.blocks.40.ca.mat_kv.weight, module.blocks.40.ca.proj.weight, module.blocks.40.ffn.fc1.weight, module.blocks.40.ffn.fc2.weight, '\n"
                   " 'module.blocks.40.ca_norm.weight, module.blocks.41.sa.mat_qkv.weight, module.blocks.41.sa.proj.weight, module.blocks.41.ca.mat_q.weight, module.blocks.41.ca.mat_kv.weight, '\n"
                   " 'module.blocks.41.ca.proj.weight, module.blocks.41.ffn.fc1.weight, module.blocks.41.ffn.fc2.weight, module.blocks.41.ca_norm.weight, module.blocks.42.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.42.sa.proj.weight, module.blocks.42.ca.mat_q.weight, module.blocks.42.ca.mat_kv.weight, module.blocks.42.ca.proj.weight, module.blocks.42.ffn.fc1.weight, '\n"
                   " 'module.blocks.42.ffn.fc2.weight, module.blocks.42.ca_norm.weight, module.blocks.43.sa.mat_qkv.weight, module.blocks.43.sa.proj.weight, module.blocks.43.ca.mat_q.weight, '\n"
                   " 'module.blocks.43.ca.mat_kv.weight, module.blocks.43.ca.proj.weight, module.blocks.43.ffn.fc1.weight, module.blocks.43.ffn.fc2.weight, module.blocks.43.ca_norm.weight, '\n"
                   " 'module.blocks.44.sa.mat_qkv.weight, module.blocks.44.sa.proj.weight, module.blocks.44.ca.mat_q.weight, module.blocks.44.ca.mat_kv.weight, module.blocks.44.ca.proj.weight, '\n"
                   " 'module.blocks.44.ffn.fc1.weight, module.blocks.44.ffn.fc2.weight, module.blocks.44.ca_norm.weight, module.blocks.45.sa.mat_qkv.weight, module.blocks.45.sa.proj.weight, '\n"
                   " 'module.blocks.45.ca.mat_q.weight, module.blocks.45.ca.mat_kv.weight, module.blocks.45.ca.proj.weight, module.blocks.45.ffn.fc1.weight, module.blocks.45.ffn.fc2.weight, '\n"
                   " 'module.blocks.45.ca_norm.weight, module.blocks.46.sa.mat_qkv.weight, module.blocks.46.sa.proj.weight, module.blocks.46.ca.mat_q.weight, module.blocks.46.ca.mat_kv.weight, '\n"
                   " 'module.blocks.46.ca.proj.weight, module.blocks.46.ffn.fc1.weight, module.blocks.46.ffn.fc2.weight, module.blocks.46.ca_norm.weight, module.blocks.47.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.47.sa.proj.weight, module.blocks.47.ca.mat_q.weight, module.blocks.47.ca.mat_kv.weight, module.blocks.47.ca.proj.weight, module.blocks.47.ffn.fc1.weight, '\n"
                   " 'module.blocks.47.ffn.fc2.weight, module.blocks.47.ca_norm.weight, module.blocks.48.sa.mat_qkv.weight, module.blocks.48.sa.proj.weight, module.blocks.48.ca.mat_q.weight, '\n"
                   " 'module.blocks.48.ca.mat_kv.weight, module.blocks.48.ca.proj.weight, module.blocks.48.ffn.fc1.weight, module.blocks.48.ffn.fc2.weight, module.blocks.48.ca_norm.weight, '\n"
                   " 'module.blocks.49.sa.mat_qkv.weight, module.blocks.49.sa.proj.weight, module.blocks.49.ca.mat_q.weight, module.blocks.49.ca.mat_kv.weight, module.blocks.49.ca.proj.weight, '\n"
                   " 'module.blocks.49.ffn.fc1.weight, module.blocks.49.ffn.fc2.weight, module.blocks.49.ca_norm.weight, module.blocks.50.sa.mat_qkv.weight, module.blocks.50.sa.proj.weight, '\n"
                   " 'module.blocks.50.ca.mat_q.weight, module.blocks.50.ca.mat_kv.weight, module.blocks.50.ca.proj.weight, module.blocks.50.ffn.fc1.weight, module.blocks.50.ffn.fc2.weight, '\n"
                   " 'module.blocks.50.ca_norm.weight, module.blocks.51.sa.mat_qkv.weight, module.blocks.51.sa.proj.weight, module.blocks.51.ca.mat_q.weight, module.blocks.51.ca.mat_kv.weight, '\n"
                   " 'module.blocks.51.ca.proj.weight, module.blocks.51.ffn.fc1.weight, module.blocks.51.ffn.fc2.weight, module.blocks.51.ca_norm.weight, module.blocks.52.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.52.sa.proj.weight, module.blocks.52.ca.mat_q.weight, module.blocks.52.ca.mat_kv.weight, module.blocks.52.ca.proj.weight, module.blocks.52.ffn.fc1.weight, '\n"
                   " 'module.blocks.52.ffn.fc2.weight, module.blocks.52.ca_norm.weight, module.blocks.53.sa.mat_qkv.weight, module.blocks.53.sa.proj.weight, module.blocks.53.ca.mat_q.weight, '\n"
                   " 'module.blocks.53.ca.mat_kv.weight, module.blocks.53.ca.proj.weight, module.blocks.53.ffn.fc1.weight, module.blocks.53.ffn.fc2.weight, module.blocks.53.ca_norm.weight, '\n"
                   " 'module.blocks.54.sa.mat_qkv.weight, module.blocks.54.sa.proj.weight, module.blocks.54.ca.mat_q.weight, module.blocks.54.ca.mat_kv.weight, module.blocks.54.ca.proj.weight, '\n"
                   " 'module.blocks.54.ffn.fc1.weight, module.blocks.54.ffn.fc2.weight, module.blocks.54.ca_norm.weight, module.blocks.55.sa.mat_qkv.weight, module.blocks.55.sa.proj.weight, '\n"
                   " 'module.blocks.55.ca.mat_q.weight, module.blocks.55.ca.mat_kv.weight, module.blocks.55.ca.proj.weight, module.blocks.55.ffn.fc1.weight, module.blocks.55.ffn.fc2.weight, '\n"
                   " 'module.blocks.55.ca_norm.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': '[no scale]',
          'params': "('module.cfg_uncond, module.pos_start, module.text_proj_for_sos.ca.mat_q, module.text_proj_for_sos.ca.v_bias, module.text_proj_for_sos.ca.proj.bias, module.text_proj_for_ca.0.bias, '\n"
                    " 'module.text_proj_for_ca.2.bias, module.lvl_embed.weight, module.word_embed.bias, module.shared_ada_lin.1.bias, module.head_nm.ada_lin.1.bias, module.head.bias, module.blocks.0.ada_gss, '\n"
                    " 'module.blocks.0.sa.scale_mul_1H11, module.blocks.0.sa.q_bias, module.blocks.0.sa.v_bias, module.blocks.0.sa.proj.bias, module.blocks.0.ca.v_bias, module.blocks.0.ca.mat_q.bias, '\n"
                    " 'module.blocks.0.ca.proj.bias, module.blocks.0.ffn.fc1.bias, module.blocks.0.ffn.fc2.bias, module.blocks.0.ca_norm.bias, module.blocks.1.ada_gss, module.blocks.1.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.1.sa.q_bias, module.blocks.1.sa.v_bias, module.blocks.1.sa.proj.bias, module.blocks.1.ca.v_bias, module.blocks.1.ca.mat_q.bias, module.blocks.1.ca.proj.bias, '\n"
                    " 'module.blocks.1.ffn.fc1.bias, module.blocks.1.ffn.fc2.bias, module.blocks.1.ca_norm.bias, module.blocks.2.ada_gss, module.blocks.2.sa.scale_mul_1H11, module.blocks.2.sa.q_bias, '\n"
                    " 'module.blocks.2.sa.v_bias, module.blocks.2.sa.proj.bias, module.blocks.2.ca.v_bias, module.blocks.2.ca.mat_q.bias, module.blocks.2.ca.proj.bias, module.blocks.2.ffn.fc1.bias, '\n"
                    " 'module.blocks.2.ffn.fc2.bias, module.blocks.2.ca_norm.bias, module.blocks.3.ada_gss, module.blocks.3.sa.scale_mul_1H11, module.blocks.3.sa.q_bias, module.blocks.3.sa.v_bias, '\n"
                    " 'module.blocks.3.sa.proj.bias, module.blocks.3.ca.v_bias, module.blocks.3.ca.mat_q.bias, module.blocks.3.ca.proj.bias, module.blocks.3.ffn.fc1.bias, module.blocks.3.ffn.fc2.bias, '\n"
                    " 'module.blocks.3.ca_norm.bias, module.blocks.4.ada_gss, module.blocks.4.sa.scale_mul_1H11, module.blocks.4.sa.q_bias, module.blocks.4.sa.v_bias, module.blocks.4.sa.proj.bias, '\n"
                    " 'module.blocks.4.ca.v_bias, module.blocks.4.ca.mat_q.bias, module.blocks.4.ca.proj.bias, module.blocks.4.ffn.fc1.bias, module.blocks.4.ffn.fc2.bias, module.blocks.4.ca_norm.bias, '\n"
                    " 'module.blocks.5.ada_gss, module.blocks.5.sa.scale_mul_1H11, module.blocks.5.sa.q_bias, module.blocks.5.sa.v_bias, module.blocks.5.sa.proj.bias, module.blocks.5.ca.v_bias, '\n"
                    " 'module.blocks.5.ca.mat_q.bias, module.blocks.5.ca.proj.bias, module.blocks.5.ffn.fc1.bias, module.blocks.5.ffn.fc2.bias, module.blocks.5.ca_norm.bias, module.blocks.6.ada_gss, '\n"
                    " 'module.blocks.6.sa.scale_mul_1H11, module.blocks.6.sa.q_bias, module.blocks.6.sa.v_bias, module.blocks.6.sa.proj.bias, module.blocks.6.ca.v_bias, module.blocks.6.ca.mat_q.bias, '\n"
                    " 'module.blocks.6.ca.proj.bias, module.blocks.6.ffn.fc1.bias, module.blocks.6.ffn.fc2.bias, module.blocks.6.ca_norm.bias, module.blocks.7.ada_gss, module.blocks.7.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.7.sa.q_bias, module.blocks.7.sa.v_bias, module.blocks.7.sa.proj.bias, module.blocks.7.ca.v_bias, module.blocks.7.ca.mat_q.bias, module.blocks.7.ca.proj.bias, '\n"
                    " 'module.blocks.7.ffn.fc1.bias, module.blocks.7.ffn.fc2.bias, module.blocks.7.ca_norm.bias, module.blocks.8.ada_gss, module.blocks.8.sa.scale_mul_1H11, module.blocks.8.sa.q_bias, '\n"
                    " 'module.blocks.8.sa.v_bias, module.blocks.8.sa.proj.bias, module.blocks.8.ca.v_bias, module.blocks.8.ca.mat_q.bias, module.blocks.8.ca.proj.bias, module.blocks.8.ffn.fc1.bias, '\n"
                    " 'module.blocks.8.ffn.fc2.bias, module.blocks.8.ca_norm.bias, module.blocks.9.ada_gss, module.blocks.9.sa.scale_mul_1H11, module.blocks.9.sa.q_bias, module.blocks.9.sa.v_bias, '\n"
                    " 'module.blocks.9.sa.proj.bias, module.blocks.9.ca.v_bias, module.blocks.9.ca.mat_q.bias, module.blocks.9.ca.proj.bias, module.blocks.9.ffn.fc1.bias, module.blocks.9.ffn.fc2.bias, '\n"
                    " 'module.blocks.9.ca_norm.bias, module.blocks.10.ada_gss, module.blocks.10.sa.scale_mul_1H11, module.blocks.10.sa.q_bias, module.blocks.10.sa.v_bias, module.blocks.10.sa.proj.bias, '\n"
                    " 'module.blocks.10.ca.v_bias, module.blocks.10.ca.mat_q.bias, module.blocks.10.ca.proj.bias, module.blocks.10.ffn.fc1.bias, module.blocks.10.ffn.fc2.bias, module.blocks.10.ca_norm.bias, '\n"
                    " 'module.blocks.11.ada_gss, module.blocks.11.sa.scale_mul_1H11, module.blocks.11.sa.q_bias, module.blocks.11.sa.v_bias, module.blocks.11.sa.proj.bias, module.blocks.11.ca.v_bias, '\n"
                    " 'module.blocks.11.ca.mat_q.bias, module.blocks.11.ca.proj.bias, module.blocks.11.ffn.fc1.bias, module.blocks.11.ffn.fc2.bias, module.blocks.11.ca_norm.bias, module.blocks.12.ada_gss, '\n"
                    " 'module.blocks.12.sa.scale_mul_1H11, module.blocks.12.sa.q_bias, module.blocks.12.sa.v_bias, module.blocks.12.sa.proj.bias, module.blocks.12.ca.v_bias, module.blocks.12.ca.mat_q.bias, '\n"
                    " 'module.blocks.12.ca.proj.bias, module.blocks.12.ffn.fc1.bias, module.blocks.12.ffn.fc2.bias, module.blocks.12.ca_norm.bias, module.blocks.13.ada_gss, module.blocks.13.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.13.sa.q_bias, module.blocks.13.sa.v_bias, module.blocks.13.sa.proj.bias, module.blocks.13.ca.v_bias, module.blocks.13.ca.mat_q.bias, module.blocks.13.ca.proj.bias, '\n"
                    " 'module.blocks.13.ffn.fc1.bias, module.blocks.13.ffn.fc2.bias, module.blocks.13.ca_norm.bias, module.blocks.14.ada_gss, module.blocks.14.sa.scale_mul_1H11, module.blocks.14.sa.q_bias, '\n"
                    " 'module.blocks.14.sa.v_bias, module.blocks.14.sa.proj.bias, module.blocks.14.ca.v_bias, module.blocks.14.ca.mat_q.bias, module.blocks.14.ca.proj.bias, module.blocks.14.ffn.fc1.bias, '\n"
                    " 'module.blocks.14.ffn.fc2.bias, module.blocks.14.ca_norm.bias, module.blocks.15.ada_gss, module.blocks.15.sa.scale_mul_1H11, module.blocks.15.sa.q_bias, module.blocks.15.sa.v_bias, '\n"
                    " 'module.blocks.15.sa.proj.bias, module.blocks.15.ca.v_bias, module.blocks.15.ca.mat_q.bias, module.blocks.15.ca.proj.bias, module.blocks.15.ffn.fc1.bias, module.blocks.15.ffn.fc2.bias, '\n"
                    " 'module.blocks.15.ca_norm.bias, module.blocks.16.ada_gss, module.blocks.16.sa.scale_mul_1H11, module.blocks.16.sa.q_bias, module.blocks.16.sa.v_bias, module.blocks.16.sa.proj.bias, '\n"
                    " 'module.blocks.16.ca.v_bias, module.blocks.16.ca.mat_q.bias, module.blocks.16.ca.proj.bias, module.blocks.16.ffn.fc1.bias, module.blocks.16.ffn.fc2.bias, module.blocks.16.ca_norm.bias, '\n"
                    " 'module.blocks.17.ada_gss, module.blocks.17.sa.scale_mul_1H11, module.blocks.17.sa.q_bias, module.blocks.17.sa.v_bias, module.blocks.17.sa.proj.bias, module.blocks.17.ca.v_bias, '\n"
                    " 'module.blocks.17.ca.mat_q.bias, module.blocks.17.ca.proj.bias, module.blocks.17.ffn.fc1.bias, module.blocks.17.ffn.fc2.bias, module.blocks.17.ca_norm.bias, module.blocks.18.ada_gss, '\n"
                    " 'module.blocks.18.sa.scale_mul_1H11, module.blocks.18.sa.q_bias, module.blocks.18.sa.v_bias, module.blocks.18.sa.proj.bias, module.blocks.18.ca.v_bias, module.blocks.18.ca.mat_q.bias, '\n"
                    " 'module.blocks.18.ca.proj.bias, module.blocks.18.ffn.fc1.bias, module.blocks.18.ffn.fc2.bias, module.blocks.18.ca_norm.bias, module.blocks.19.ada_gss, module.blocks.19.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.19.sa.q_bias, module.blocks.19.sa.v_bias, module.blocks.19.sa.proj.bias, module.blocks.19.ca.v_bias, module.blocks.19.ca.mat_q.bias, module.blocks.19.ca.proj.bias, '\n"
                    " 'module.blocks.19.ffn.fc1.bias, module.blocks.19.ffn.fc2.bias, module.blocks.19.ca_norm.bias, module.blocks.20.ada_gss, module.blocks.20.sa.scale_mul_1H11, module.blocks.20.sa.q_bias, '\n"
                    " 'module.blocks.20.sa.v_bias, module.blocks.20.sa.proj.bias, module.blocks.20.ca.v_bias, module.blocks.20.ca.mat_q.bias, module.blocks.20.ca.proj.bias, module.blocks.20.ffn.fc1.bias, '\n"
                    " 'module.blocks.20.ffn.fc2.bias, module.blocks.20.ca_norm.bias, module.blocks.21.ada_gss, module.blocks.21.sa.scale_mul_1H11, module.blocks.21.sa.q_bias, module.blocks.21.sa.v_bias, '\n"
                    " 'module.blocks.21.sa.proj.bias, module.blocks.21.ca.v_bias, module.blocks.21.ca.mat_q.bias, module.blocks.21.ca.proj.bias, module.blocks.21.ffn.fc1.bias, module.blocks.21.ffn.fc2.bias, '\n"
                    " 'module.blocks.21.ca_norm.bias, module.blocks.22.ada_gss, module.blocks.22.sa.scale_mul_1H11, module.blocks.22.sa.q_bias, module.blocks.22.sa.v_bias, module.blocks.22.sa.proj.bias, '\n"
                    " 'module.blocks.22.ca.v_bias, module.blocks.22.ca.mat_q.bias, module.blocks.22.ca.proj.bias, module.blocks.22.ffn.fc1.bias, module.blocks.22.ffn.fc2.bias, module.blocks.22.ca_norm.bias, '\n"
                    " 'module.blocks.23.ada_gss, module.blocks.23.sa.scale_mul_1H11, module.blocks.23.sa.q_bias, module.blocks.23.sa.v_bias, module.blocks.23.sa.proj.bias, module.blocks.23.ca.v_bias, '\n"
                    " 'module.blocks.23.ca.mat_q.bias, module.blocks.23.ca.proj.bias, module.blocks.23.ffn.fc1.bias, module.blocks.23.ffn.fc2.bias, module.blocks.23.ca_norm.bias, module.blocks.24.ada_gss, '\n"
                    " 'module.blocks.24.sa.scale_mul_1H11, module.blocks.24.sa.q_bias, module.blocks.24.sa.v_bias, module.blocks.24.sa.proj.bias, module.blocks.24.ca.v_bias, module.blocks.24.ca.mat_q.bias, '\n"
                    " 'module.blocks.24.ca.proj.bias, module.blocks.24.ffn.fc1.bias, module.blocks.24.ffn.fc2.bias, module.blocks.24.ca_norm.bias, module.blocks.25.ada_gss, module.blocks.25.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.25.sa.q_bias, module.blocks.25.sa.v_bias, module.blocks.25.sa.proj.bias, module.blocks.25.ca.v_bias, module.blocks.25.ca.mat_q.bias, module.blocks.25.ca.proj.bias, '\n"
                    " 'module.blocks.25.ffn.fc1.bias, module.blocks.25.ffn.fc2.bias, module.blocks.25.ca_norm.bias, module.blocks.26.ada_gss, module.blocks.26.sa.scale_mul_1H11, module.blocks.26.sa.q_bias, '\n"
                    " 'module.blocks.26.sa.v_bias, module.blocks.26.sa.proj.bias, module.blocks.26.ca.v_bias, module.blocks.26.ca.mat_q.bias, module.blocks.26.ca.proj.bias, module.blocks.26.ffn.fc1.bias, '\n"
                    " 'module.blocks.26.ffn.fc2.bias, module.blocks.26.ca_norm.bias, module.blocks.27.ada_gss, module.blocks.27.sa.scale_mul_1H11, module.blocks.27.sa.q_bias, module.blocks.27.sa.v_bias, '\n"
                    " 'module.blocks.27.sa.proj.bias, module.blocks.27.ca.v_bias, module.blocks.27.ca.mat_q.bias, module.blocks.27.ca.proj.bias, module.blocks.27.ffn.fc1.bias, module.blocks.27.ffn.fc2.bias, '\n"
                    " 'module.blocks.27.ca_norm.bias, module.blocks.28.ada_gss, module.blocks.28.sa.scale_mul_1H11, module.blocks.28.sa.q_bias, module.blocks.28.sa.v_bias, module.blocks.28.sa.proj.bias, '\n"
                    " 'module.blocks.28.ca.v_bias, module.blocks.28.ca.mat_q.bias, module.blocks.28.ca.proj.bias, module.blocks.28.ffn.fc1.bias, module.blocks.28.ffn.fc2.bias, module.blocks.28.ca_norm.bias, '\n"
                    " 'module.blocks.29.ada_gss, module.blocks.29.sa.scale_mul_1H11, module.blocks.29.sa.q_bias, module.blocks.29.sa.v_bias, module.blocks.29.sa.proj.bias, module.blocks.29.ca.v_bias, '\n"
                    " 'module.blocks.29.ca.mat_q.bias, module.blocks.29.ca.proj.bias, module.blocks.29.ffn.fc1.bias, module.blocks.29.ffn.fc2.bias, module.blocks.29.ca_norm.bias, module.blocks.30.ada_gss, '\n"
                    " 'module.blocks.30.sa.scale_mul_1H11, module.blocks.30.sa.q_bias, module.blocks.30.sa.v_bias, module.blocks.30.sa.proj.bias, module.blocks.30.ca.v_bias, module.blocks.30.ca.mat_q.bias, '\n"
                    " 'module.blocks.30.ca.proj.bias, module.blocks.30.ffn.fc1.bias, module.blocks.30.ffn.fc2.bias, module.blocks.30.ca_norm.bias, module.blocks.31.ada_gss, module.blocks.31.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.31.sa.q_bias, module.blocks.31.sa.v_bias, module.blocks.31.sa.proj.bias, module.blocks.31.ca.v_bias, module.blocks.31.ca.mat_q.bias, module.blocks.31.ca.proj.bias, '\n"
                    " 'module.blocks.31.ffn.fc1.bias, module.blocks.31.ffn.fc2.bias, module.blocks.31.ca_norm.bias, module.blocks.32.ada_gss, module.blocks.32.sa.scale_mul_1H11, module.blocks.32.sa.q_bias, '\n"
                    " 'module.blocks.32.sa.v_bias, module.blocks.32.sa.proj.bias, module.blocks.32.ca.v_bias, module.blocks.32.ca.mat_q.bias, module.blocks.32.ca.proj.bias, module.blocks.32.ffn.fc1.bias, '\n"
                    " 'module.blocks.32.ffn.fc2.bias, module.blocks.32.ca_norm.bias, module.blocks.33.ada_gss, module.blocks.33.sa.scale_mul_1H11, module.blocks.33.sa.q_bias, module.blocks.33.sa.v_bias, '\n"
                    " 'module.blocks.33.sa.proj.bias, module.blocks.33.ca.v_bias, module.blocks.33.ca.mat_q.bias, module.blocks.33.ca.proj.bias, module.blocks.33.ffn.fc1.bias, module.blocks.33.ffn.fc2.bias, '\n"
                    " 'module.blocks.33.ca_norm.bias, module.blocks.34.ada_gss, module.blocks.34.sa.scale_mul_1H11, module.blocks.34.sa.q_bias, module.blocks.34.sa.v_bias, module.blocks.34.sa.proj.bias, '\n"
                    " 'module.blocks.34.ca.v_bias, module.blocks.34.ca.mat_q.bias, module.blocks.34.ca.proj.bias, module.blocks.34.ffn.fc1.bias, module.blocks.34.ffn.fc2.bias, module.blocks.34.ca_norm.bias, '\n"
                    " 'module.blocks.35.ada_gss, module.blocks.35.sa.scale_mul_1H11, module.blocks.35.sa.q_bias, module.blocks.35.sa.v_bias, module.blocks.35.sa.proj.bias, module.blocks.35.ca.v_bias, '\n"
                    " 'module.blocks.35.ca.mat_q.bias, module.blocks.35.ca.proj.bias, module.blocks.35.ffn.fc1.bias, module.blocks.35.ffn.fc2.bias, module.blocks.35.ca_norm.bias, module.blocks.36.ada_gss, '\n"
                    " 'module.blocks.36.sa.scale_mul_1H11, module.blocks.36.sa.q_bias, module.blocks.36.sa.v_bias, module.blocks.36.sa.proj.bias, module.blocks.36.ca.v_bias, module.blocks.36.ca.mat_q.bias, '\n"
                    " 'module.blocks.36.ca.proj.bias, module.blocks.36.ffn.fc1.bias, module.blocks.36.ffn.fc2.bias, module.blocks.36.ca_norm.bias, module.blocks.37.ada_gss, module.blocks.37.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.37.sa.q_bias, module.blocks.37.sa.v_bias, module.blocks.37.sa.proj.bias, module.blocks.37.ca.v_bias, module.blocks.37.ca.mat_q.bias, module.blocks.37.ca.proj.bias, '\n"
                    " 'module.blocks.37.ffn.fc1.bias, module.blocks.37.ffn.fc2.bias, module.blocks.37.ca_norm.bias, module.blocks.38.ada_gss, module.blocks.38.sa.scale_mul_1H11, module.blocks.38.sa.q_bias, '\n"
                    " 'module.blocks.38.sa.v_bias, module.blocks.38.sa.proj.bias, module.blocks.38.ca.v_bias, module.blocks.38.ca.mat_q.bias, module.blocks.38.ca.proj.bias, module.blocks.38.ffn.fc1.bias, '\n"
                    " 'module.blocks.38.ffn.fc2.bias, module.blocks.38.ca_norm.bias, module.blocks.39.ada_gss, module.blocks.39.sa.scale_mul_1H11, module.blocks.39.sa.q_bias, module.blocks.39.sa.v_bias, '\n"
                    " 'module.blocks.39.sa.proj.bias, module.blocks.39.ca.v_bias, module.blocks.39.ca.mat_q.bias, module.blocks.39.ca.proj.bias, module.blocks.39.ffn.fc1.bias, module.blocks.39.ffn.fc2.bias, '\n"
                    " 'module.blocks.39.ca_norm.bias, module.blocks.40.ada_gss, module.blocks.40.sa.scale_mul_1H11, module.blocks.40.sa.q_bias, module.blocks.40.sa.v_bias, module.blocks.40.sa.proj.bias, '\n"
                    " 'module.blocks.40.ca.v_bias, module.blocks.40.ca.mat_q.bias, module.blocks.40.ca.proj.bias, module.blocks.40.ffn.fc1.bias, module.blocks.40.ffn.fc2.bias, module.blocks.40.ca_norm.bias, '\n"
                    " 'module.blocks.41.ada_gss, module.blocks.41.sa.scale_mul_1H11, module.blocks.41.sa.q_bias, module.blocks.41.sa.v_bias, module.blocks.41.sa.proj.bias, module.blocks.41.ca.v_bias, '\n"
                    " 'module.blocks.41.ca.mat_q.bias, module.blocks.41.ca.proj.bias, module.blocks.41.ffn.fc1.bias, module.blocks.41.ffn.fc2.bias, module.blocks.41.ca_norm.bias, module.blocks.42.ada_gss, '\n"
                    " 'module.blocks.42.sa.scale_mul_1H11, module.blocks.42.sa.q_bias, module.blocks.42.sa.v_bias, module.blocks.42.sa.proj.bias, module.blocks.42.ca.v_bias, module.blocks.42.ca.mat_q.bias, '\n"
                    " 'module.blocks.42.ca.proj.bias, module.blocks.42.ffn.fc1.bias, module.blocks.42.ffn.fc2.bias, module.blocks.42.ca_norm.bias, module.blocks.43.ada_gss, module.blocks.43.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.43.sa.q_bias, module.blocks.43.sa.v_bias, module.blocks.43.sa.proj.bias, module.blocks.43.ca.v_bias, module.blocks.43.ca.mat_q.bias, module.blocks.43.ca.proj.bias, '\n"
                    " 'module.blocks.43.ffn.fc1.bias, module.blocks.43.ffn.fc2.bias, module.blocks.43.ca_norm.bias, module.blocks.44.ada_gss, module.blocks.44.sa.scale_mul_1H11, module.blocks.44.sa.q_bias, '\n"
                    " 'module.blocks.44.sa.v_bias, module.blocks.44.sa.proj.bias, module.blocks.44.ca.v_bias, module.blocks.44.ca.mat_q.bias, module.blocks.44.ca.proj.bias, module.blocks.44.ffn.fc1.bias, '\n"
                    " 'module.blocks.44.ffn.fc2.bias, module.blocks.44.ca_norm.bias, module.blocks.45.ada_gss, module.blocks.45.sa.scale_mul_1H11, module.blocks.45.sa.q_bias, module.blocks.45.sa.v_bias, '\n"
                    " 'module.blocks.45.sa.proj.bias, module.blocks.45.ca.v_bias, module.blocks.45.ca.mat_q.bias, module.blocks.45.ca.proj.bias, module.blocks.45.ffn.fc1.bias, module.blocks.45.ffn.fc2.bias, '\n"
                    " 'module.blocks.45.ca_norm.bias, module.blocks.46.ada_gss, module.blocks.46.sa.scale_mul_1H11, module.blocks.46.sa.q_bias, module.blocks.46.sa.v_bias, module.blocks.46.sa.proj.bias, '\n"
                    " 'module.blocks.46.ca.v_bias, module.blocks.46.ca.mat_q.bias, module.blocks.46.ca.proj.bias, module.blocks.46.ffn.fc1.bias, module.blocks.46.ffn.fc2.bias, module.blocks.46.ca_norm.bias, '\n"
                    " 'module.blocks.47.ada_gss, module.blocks.47.sa.scale_mul_1H11, module.blocks.47.sa.q_bias, module.blocks.47.sa.v_bias, module.blocks.47.sa.proj.bias, module.blocks.47.ca.v_bias, '\n"
                    " 'module.blocks.47.ca.mat_q.bias, module.blocks.47.ca.proj.bias, module.blocks.47.ffn.fc1.bias, module.blocks.47.ffn.fc2.bias, module.blocks.47.ca_norm.bias, module.blocks.48.ada_gss, '\n"
                    " 'module.blocks.48.sa.scale_mul_1H11, module.blocks.48.sa.q_bias, module.blocks.48.sa.v_bias, module.blocks.48.sa.proj.bias, module.blocks.48.ca.v_bias, module.blocks.48.ca.mat_q.bias, '\n"
                    " 'module.blocks.48.ca.proj.bias, module.blocks.48.ffn.fc1.bias, module.blocks.48.ffn.fc2.bias, module.blocks.48.ca_norm.bias, module.blocks.49.ada_gss, module.blocks.49.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.49.sa.q_bias, module.blocks.49.sa.v_bias, module.blocks.49.sa.proj.bias, module.blocks.49.ca.v_bias, module.blocks.49.ca.mat_q.bias, module.blocks.49.ca.proj.bias, '\n"
                    " 'module.blocks.49.ffn.fc1.bias, module.blocks.49.ffn.fc2.bias, module.blocks.49.ca_norm.bias, module.blocks.50.ada_gss, module.blocks.50.sa.scale_mul_1H11, module.blocks.50.sa.q_bias, '\n"
                    " 'module.blocks.50.sa.v_bias, module.blocks.50.sa.proj.bias, module.blocks.50.ca.v_bias, module.blocks.50.ca.mat_q.bias, module.blocks.50.ca.proj.bias, module.blocks.50.ffn.fc1.bias, '\n"
                    " 'module.blocks.50.ffn.fc2.bias, module.blocks.50.ca_norm.bias, module.blocks.51.ada_gss, module.blocks.51.sa.scale_mul_1H11, module.blocks.51.sa.q_bias, module.blocks.51.sa.v_bias, '\n"
                    " 'module.blocks.51.sa.proj.bias, module.blocks.51.ca.v_bias, module.blocks.51.ca.mat_q.bias, module.blocks.51.ca.proj.bias, module.blocks.51.ffn.fc1.bias, module.blocks.51.ffn.fc2.bias, '\n"
                    " 'module.blocks.51.ca_norm.bias, module.blocks.52.ada_gss, module.blocks.52.sa.scale_mul_1H11, module.blocks.52.sa.q_bias, module.blocks.52.sa.v_bias, module.blocks.52.sa.proj.bias, '\n"
                    " 'module.blocks.52.ca.v_bias, module.blocks.52.ca.mat_q.bias, module.blocks.52.ca.proj.bias, module.blocks.52.ffn.fc1.bias, module.blocks.52.ffn.fc2.bias, module.blocks.52.ca_norm.bias, '\n"
                    " 'module.blocks.53.ada_gss, module.blocks.53.sa.scale_mul_1H11, module.blocks.53.sa.q_bias, module.blocks.53.sa.v_bias, module.blocks.53.sa.proj.bias, module.blocks.53.ca.v_bias, '\n"
                    " 'module.blocks.53.ca.mat_q.bias, module.blocks.53.ca.proj.bias, module.blocks.53.ffn.fc1.bias, module.blocks.53.ffn.fc2.bias, module.blocks.53.ca_norm.bias, module.blocks.54.ada_gss, '\n"
                    " 'module.blocks.54.sa.scale_mul_1H11, module.blocks.54.sa.q_bias, module.blocks.54.sa.v_bias, module.blocks.54.sa.proj.bias, module.blocks.54.ca.v_bias, module.blocks.54.ca.mat_q.bias, '\n"
                    " 'module.blocks.54.ca.proj.bias, module.blocks.54.ffn.fc1.bias, module.blocks.54.ffn.fc2.bias, module.blocks.54.ca_norm.bias, module.blocks.55.ada_gss, module.blocks.55.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.55.sa.q_bias, module.blocks.55.sa.v_bias, module.blocks.55.sa.proj.bias, module.blocks.55.ca.v_bias, module.blocks.55.ca.mat_q.bias, module.blocks.55.ca.proj.bias, '\n"
                    " 'module.blocks.55.ffn.fc1.bias, module.blocks.55.ffn.fc2.bias, module.blocks.55.ca_norm.bias')",
          'wd_sc': 0.0}}

[09-01 22:48:04] (nity/utils/lr_control.py, line 116)=> [get_param_groups][rank0] type(model).__name__='NullDDP' count=1085, numel=8573892016
[09-01 22:48:04] (nity/utils/lr_control.py, line 117)=> 
[09-01 22:48:04] (inity_circuit_breaker.py, line 343)=> [vgpt] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.97), fused=True), opt_kw={'lr': 9.375e-05, 'weight_decay': 0}

[09-01 22:48:05] (inity_circuit_breaker.py, line 348)=> Loading T5 from /home/gs285/VAR/my_model/weights/flan-t5-xl...







=======================================================   RESTART [09-01 22:56:05]   =======================================================
[09-01 22:56:05] (inity_circuit_breaker.py, line 588)=> ============================================================
[09-01 22:56:05] (inity_circuit_breaker.py, line 589)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[09-01 22:56:05] (inity_circuit_breaker.py, line 590)=> Circuit Breaker Alpha: 0.1
[09-01 22:56:05] (inity_circuit_breaker.py, line 591)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[09-01 22:56:05] (inity_circuit_breaker.py, line 592)=> Circuit Breaker Enabled: True
[09-01 22:56:05] (inity_circuit_breaker.py, line 593)=> Number of Examples: 1000
[09-01 22:56:05] (inity_circuit_breaker.py, line 594)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[09-01 22:56:05] (inity_circuit_breaker.py, line 595)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[09-01 22:56:05] (inity_circuit_breaker.py, line 596)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[09-01 22:56:05] (inity_circuit_breaker.py, line 597)=> Validation Ratio: 0.1
[09-01 22:56:05] (inity_circuit_breaker.py, line 598)=> Category: hate
[09-01 22:56:05] (inity_circuit_breaker.py, line 599)=> Batch Size: 4
[09-01 22:56:05] (inity_circuit_breaker.py, line 600)=> Workers: 4
[09-01 22:56:05] (inity_circuit_breaker.py, line 601)=> Device: cuda:0
[09-01 22:56:05] (inity_circuit_breaker.py, line 602)=> Model: infinity_8b
[09-01 22:56:05] (inity_circuit_breaker.py, line 603)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-01 22:56:05] (inity_circuit_breaker.py, line 604)=> Rush Resume: 
[09-01 22:56:05] (inity_circuit_breaker.py, line 605)=> Selective Layers: 0,1,2,3,4,5
[09-01 22:56:05] (inity_circuit_breaker.py, line 606)=> ============================================================
[09-01 22:56:05] (inity_circuit_breaker.py, line 388)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[09-01 22:56:05] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-01 22:56:05] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-01 22:56:05] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[09-01 22:56:05] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-01 22:56:05] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-01 22:56:06] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[09-01 22:56:06] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[09-01 22:56:06] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[09-01 22:56:06] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[09-01 22:56:06] (inity_circuit_breaker.py, line 406)=> args.batch_size=4, vbs=6
[09-01 22:56:06] (inity_circuit_breaker.py, line 429)=> len(dataloader): 100, len(dataset): 100
[09-01 22:56:06] (inity_circuit_breaker.py, line 431)=> total_samples: 200
[09-01 22:56:06] (inity_circuit_breaker.py, line 432)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[09-01 22:56:06] (inity_circuit_breaker.py, line 140)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[09-01 22:56:06] (inity_circuit_breaker.py, line 142)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[09-01 22:56:06] (inity_circuit_breaker.py, line 146)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-01 22:56:08] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[09-01 22:56:08] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[09-01 22:56:08] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[09-01 22:56:08] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[09-01 22:56:11] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[09-01 22:56:12] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[09-01 22:56:12] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[09-01 22:56:12] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[09-01 22:56:12] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[09-01 22:56:12] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

[09-01 22:56:23] (ity/models/init_param.py, line  15)=> [init_weights] Infinity with std=0.02
[09-01 22:56:23] (inity_circuit_breaker.py, line 246)=> [PT] GPT model = Infinity(
  drop_path_rate=0.2
  (norm0_cond): Identity()
  (text_norm): FastRMSNorm(C=2048, eps=1e-06, elementwise_affine=True)
  (text_proj_for_sos): TextAttentivePool(
    (ca): CrossAttention(
      Cq=3072, Ckv=2048, cos_attn=False
      (mat_kv): Linear(in_features=2048, out_features=6144, bias=False)
      (proj): Linear(in_features=3072, out_features=3072, bias=True)
      (proj_drop): Identity()
    )
  )
  (text_proj_for_ca): Sequential(
    (0): Linear(in_features=2048, out_features=3072, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=3072, out_features=3072, bias=True)
  )
  (lvl_embed): Embedding(15, 3072)
  (norm0_ve): Identity()
  (word_embed): Linear(in_features=56, out_features=3072, bias=True)
  (shared_ada_lin): Sequential(
    (0): SiLU()
    (1): SharedAdaLin(in_features=3072, out_features=18432, bias=True)
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=3072, out_features=6144, bias=True)
    )
  )
  (head): Linear(in_features=3072, out_features=112, bias=True)
  (blocks): ModuleList(
    (0): CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): Identity()
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
    (1-55): 55 x CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): DropPath(...)
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
  )
)


[09-01 22:56:23] (inity_circuit_breaker.py, line 248)=> [PT][#para] VAE=81.62, VAE.quant=0.00
[09-01 22:56:23] (inity_circuit_breaker.py, line 251)=> [PT][#para] GPT=8573.89


[09-01 22:56:23] (nity/utils/lr_control.py, line  71)=> [get_param_groups][lr decay] with_lr_scale=False, lr_scale=0.0
[09-01 22:56:23] (nity/utils/lr_control.py, line 111)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': '[no scale]',
         'params': "('module.text_norm.weight, module.text_proj_for_sos.ca.mat_kv.weight, module.text_proj_for_sos.ca.proj.weight, module.text_proj_for_ca.0.weight, module.text_proj_for_ca.2.weight, '\n"
                   " 'module.word_embed.weight, module.shared_ada_lin.1.weight, module.head_nm.ada_lin.1.weight, module.head.weight, module.blocks.0.sa.mat_qkv.weight, module.blocks.0.sa.proj.weight, '\n"
                   " 'module.blocks.0.ca.mat_q.weight, module.blocks.0.ca.mat_kv.weight, module.blocks.0.ca.proj.weight, module.blocks.0.ffn.fc1.weight, module.blocks.0.ffn.fc2.weight, module.blocks.0.ca_norm.weight, '\n"
                   " 'module.blocks.1.sa.mat_qkv.weight, module.blocks.1.sa.proj.weight, module.blocks.1.ca.mat_q.weight, module.blocks.1.ca.mat_kv.weight, module.blocks.1.ca.proj.weight, '\n"
                   " 'module.blocks.1.ffn.fc1.weight, module.blocks.1.ffn.fc2.weight, module.blocks.1.ca_norm.weight, module.blocks.2.sa.mat_qkv.weight, module.blocks.2.sa.proj.weight, module.blocks.2.ca.mat_q.weight, '\n"
                   " 'module.blocks.2.ca.mat_kv.weight, module.blocks.2.ca.proj.weight, module.blocks.2.ffn.fc1.weight, module.blocks.2.ffn.fc2.weight, module.blocks.2.ca_norm.weight, module.blocks.3.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.3.sa.proj.weight, module.blocks.3.ca.mat_q.weight, module.blocks.3.ca.mat_kv.weight, module.blocks.3.ca.proj.weight, module.blocks.3.ffn.fc1.weight, module.blocks.3.ffn.fc2.weight, '\n"
                   " 'module.blocks.3.ca_norm.weight, module.blocks.4.sa.mat_qkv.weight, module.blocks.4.sa.proj.weight, module.blocks.4.ca.mat_q.weight, module.blocks.4.ca.mat_kv.weight, '\n"
                   " 'module.blocks.4.ca.proj.weight, module.blocks.4.ffn.fc1.weight, module.blocks.4.ffn.fc2.weight, module.blocks.4.ca_norm.weight, module.blocks.5.sa.mat_qkv.weight, module.blocks.5.sa.proj.weight, '\n"
                   " 'module.blocks.5.ca.mat_q.weight, module.blocks.5.ca.mat_kv.weight, module.blocks.5.ca.proj.weight, module.blocks.5.ffn.fc1.weight, module.blocks.5.ffn.fc2.weight, module.blocks.5.ca_norm.weight, '\n"
                   " 'module.blocks.6.sa.mat_qkv.weight, module.blocks.6.sa.proj.weight, module.blocks.6.ca.mat_q.weight, module.blocks.6.ca.mat_kv.weight, module.blocks.6.ca.proj.weight, '\n"
                   " 'module.blocks.6.ffn.fc1.weight, module.blocks.6.ffn.fc2.weight, module.blocks.6.ca_norm.weight, module.blocks.7.sa.mat_qkv.weight, module.blocks.7.sa.proj.weight, module.blocks.7.ca.mat_q.weight, '\n"
                   " 'module.blocks.7.ca.mat_kv.weight, module.blocks.7.ca.proj.weight, module.blocks.7.ffn.fc1.weight, module.blocks.7.ffn.fc2.weight, module.blocks.7.ca_norm.weight, module.blocks.8.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.8.sa.proj.weight, module.blocks.8.ca.mat_q.weight, module.blocks.8.ca.mat_kv.weight, module.blocks.8.ca.proj.weight, module.blocks.8.ffn.fc1.weight, module.blocks.8.ffn.fc2.weight, '\n"
                   " 'module.blocks.8.ca_norm.weight, module.blocks.9.sa.mat_qkv.weight, module.blocks.9.sa.proj.weight, module.blocks.9.ca.mat_q.weight, module.blocks.9.ca.mat_kv.weight, '\n"
                   " 'module.blocks.9.ca.proj.weight, module.blocks.9.ffn.fc1.weight, module.blocks.9.ffn.fc2.weight, module.blocks.9.ca_norm.weight, module.blocks.10.sa.mat_qkv.weight, module.blocks.10.sa.proj.weight, '\n"
                   " 'module.blocks.10.ca.mat_q.weight, module.blocks.10.ca.mat_kv.weight, module.blocks.10.ca.proj.weight, module.blocks.10.ffn.fc1.weight, module.blocks.10.ffn.fc2.weight, '\n"
                   " 'module.blocks.10.ca_norm.weight, module.blocks.11.sa.mat_qkv.weight, module.blocks.11.sa.proj.weight, module.blocks.11.ca.mat_q.weight, module.blocks.11.ca.mat_kv.weight, '\n"
                   " 'module.blocks.11.ca.proj.weight, module.blocks.11.ffn.fc1.weight, module.blocks.11.ffn.fc2.weight, module.blocks.11.ca_norm.weight, module.blocks.12.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.12.sa.proj.weight, module.blocks.12.ca.mat_q.weight, module.blocks.12.ca.mat_kv.weight, module.blocks.12.ca.proj.weight, module.blocks.12.ffn.fc1.weight, '\n"
                   " 'module.blocks.12.ffn.fc2.weight, module.blocks.12.ca_norm.weight, module.blocks.13.sa.mat_qkv.weight, module.blocks.13.sa.proj.weight, module.blocks.13.ca.mat_q.weight, '\n"
                   " 'module.blocks.13.ca.mat_kv.weight, module.blocks.13.ca.proj.weight, module.blocks.13.ffn.fc1.weight, module.blocks.13.ffn.fc2.weight, module.blocks.13.ca_norm.weight, '\n"
                   " 'module.blocks.14.sa.mat_qkv.weight, module.blocks.14.sa.proj.weight, module.blocks.14.ca.mat_q.weight, module.blocks.14.ca.mat_kv.weight, module.blocks.14.ca.proj.weight, '\n"
                   " 'module.blocks.14.ffn.fc1.weight, module.blocks.14.ffn.fc2.weight, module.blocks.14.ca_norm.weight, module.blocks.15.sa.mat_qkv.weight, module.blocks.15.sa.proj.weight, '\n"
                   " 'module.blocks.15.ca.mat_q.weight, module.blocks.15.ca.mat_kv.weight, module.blocks.15.ca.proj.weight, module.blocks.15.ffn.fc1.weight, module.blocks.15.ffn.fc2.weight, '\n"
                   " 'module.blocks.15.ca_norm.weight, module.blocks.16.sa.mat_qkv.weight, module.blocks.16.sa.proj.weight, module.blocks.16.ca.mat_q.weight, module.blocks.16.ca.mat_kv.weight, '\n"
                   " 'module.blocks.16.ca.proj.weight, module.blocks.16.ffn.fc1.weight, module.blocks.16.ffn.fc2.weight, module.blocks.16.ca_norm.weight, module.blocks.17.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.17.sa.proj.weight, module.blocks.17.ca.mat_q.weight, module.blocks.17.ca.mat_kv.weight, module.blocks.17.ca.proj.weight, module.blocks.17.ffn.fc1.weight, '\n"
                   " 'module.blocks.17.ffn.fc2.weight, module.blocks.17.ca_norm.weight, module.blocks.18.sa.mat_qkv.weight, module.blocks.18.sa.proj.weight, module.blocks.18.ca.mat_q.weight, '\n"
                   " 'module.blocks.18.ca.mat_kv.weight, module.blocks.18.ca.proj.weight, module.blocks.18.ffn.fc1.weight, module.blocks.18.ffn.fc2.weight, module.blocks.18.ca_norm.weight, '\n"
                   " 'module.blocks.19.sa.mat_qkv.weight, module.blocks.19.sa.proj.weight, module.blocks.19.ca.mat_q.weight, module.blocks.19.ca.mat_kv.weight, module.blocks.19.ca.proj.weight, '\n"
                   " 'module.blocks.19.ffn.fc1.weight, module.blocks.19.ffn.fc2.weight, module.blocks.19.ca_norm.weight, module.blocks.20.sa.mat_qkv.weight, module.blocks.20.sa.proj.weight, '\n"
                   " 'module.blocks.20.ca.mat_q.weight, module.blocks.20.ca.mat_kv.weight, module.blocks.20.ca.proj.weight, module.blocks.20.ffn.fc1.weight, module.blocks.20.ffn.fc2.weight, '\n"
                   " 'module.blocks.20.ca_norm.weight, module.blocks.21.sa.mat_qkv.weight, module.blocks.21.sa.proj.weight, module.blocks.21.ca.mat_q.weight, module.blocks.21.ca.mat_kv.weight, '\n"
                   " 'module.blocks.21.ca.proj.weight, module.blocks.21.ffn.fc1.weight, module.blocks.21.ffn.fc2.weight, module.blocks.21.ca_norm.weight, module.blocks.22.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.22.sa.proj.weight, module.blocks.22.ca.mat_q.weight, module.blocks.22.ca.mat_kv.weight, module.blocks.22.ca.proj.weight, module.blocks.22.ffn.fc1.weight, '\n"
                   " 'module.blocks.22.ffn.fc2.weight, module.blocks.22.ca_norm.weight, module.blocks.23.sa.mat_qkv.weight, module.blocks.23.sa.proj.weight, module.blocks.23.ca.mat_q.weight, '\n"
                   " 'module.blocks.23.ca.mat_kv.weight, module.blocks.23.ca.proj.weight, module.blocks.23.ffn.fc1.weight, module.blocks.23.ffn.fc2.weight, module.blocks.23.ca_norm.weight, '\n"
                   " 'module.blocks.24.sa.mat_qkv.weight, module.blocks.24.sa.proj.weight, module.blocks.24.ca.mat_q.weight, module.blocks.24.ca.mat_kv.weight, module.blocks.24.ca.proj.weight, '\n"
                   " 'module.blocks.24.ffn.fc1.weight, module.blocks.24.ffn.fc2.weight, module.blocks.24.ca_norm.weight, module.blocks.25.sa.mat_qkv.weight, module.blocks.25.sa.proj.weight, '\n"
                   " 'module.blocks.25.ca.mat_q.weight, module.blocks.25.ca.mat_kv.weight, module.blocks.25.ca.proj.weight, module.blocks.25.ffn.fc1.weight, module.blocks.25.ffn.fc2.weight, '\n"
                   " 'module.blocks.25.ca_norm.weight, module.blocks.26.sa.mat_qkv.weight, module.blocks.26.sa.proj.weight, module.blocks.26.ca.mat_q.weight, module.blocks.26.ca.mat_kv.weight, '\n"
                   " 'module.blocks.26.ca.proj.weight, module.blocks.26.ffn.fc1.weight, module.blocks.26.ffn.fc2.weight, module.blocks.26.ca_norm.weight, module.blocks.27.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.27.sa.proj.weight, module.blocks.27.ca.mat_q.weight, module.blocks.27.ca.mat_kv.weight, module.blocks.27.ca.proj.weight, module.blocks.27.ffn.fc1.weight, '\n"
                   " 'module.blocks.27.ffn.fc2.weight, module.blocks.27.ca_norm.weight, module.blocks.28.sa.mat_qkv.weight, module.blocks.28.sa.proj.weight, module.blocks.28.ca.mat_q.weight, '\n"
                   " 'module.blocks.28.ca.mat_kv.weight, module.blocks.28.ca.proj.weight, module.blocks.28.ffn.fc1.weight, module.blocks.28.ffn.fc2.weight, module.blocks.28.ca_norm.weight, '\n"
                   " 'module.blocks.29.sa.mat_qkv.weight, module.blocks.29.sa.proj.weight, module.blocks.29.ca.mat_q.weight, module.blocks.29.ca.mat_kv.weight, module.blocks.29.ca.proj.weight, '\n"
                   " 'module.blocks.29.ffn.fc1.weight, module.blocks.29.ffn.fc2.weight, module.blocks.29.ca_norm.weight, module.blocks.30.sa.mat_qkv.weight, module.blocks.30.sa.proj.weight, '\n"
                   " 'module.blocks.30.ca.mat_q.weight, module.blocks.30.ca.mat_kv.weight, module.blocks.30.ca.proj.weight, module.blocks.30.ffn.fc1.weight, module.blocks.30.ffn.fc2.weight, '\n"
                   " 'module.blocks.30.ca_norm.weight, module.blocks.31.sa.mat_qkv.weight, module.blocks.31.sa.proj.weight, module.blocks.31.ca.mat_q.weight, module.blocks.31.ca.mat_kv.weight, '\n"
                   " 'module.blocks.31.ca.proj.weight, module.blocks.31.ffn.fc1.weight, module.blocks.31.ffn.fc2.weight, module.blocks.31.ca_norm.weight, module.blocks.32.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.32.sa.proj.weight, module.blocks.32.ca.mat_q.weight, module.blocks.32.ca.mat_kv.weight, module.blocks.32.ca.proj.weight, module.blocks.32.ffn.fc1.weight, '\n"
                   " 'module.blocks.32.ffn.fc2.weight, module.blocks.32.ca_norm.weight, module.blocks.33.sa.mat_qkv.weight, module.blocks.33.sa.proj.weight, module.blocks.33.ca.mat_q.weight, '\n"
                   " 'module.blocks.33.ca.mat_kv.weight, module.blocks.33.ca.proj.weight, module.blocks.33.ffn.fc1.weight, module.blocks.33.ffn.fc2.weight, module.blocks.33.ca_norm.weight, '\n"
                   " 'module.blocks.34.sa.mat_qkv.weight, module.blocks.34.sa.proj.weight, module.blocks.34.ca.mat_q.weight, module.blocks.34.ca.mat_kv.weight, module.blocks.34.ca.proj.weight, '\n"
                   " 'module.blocks.34.ffn.fc1.weight, module.blocks.34.ffn.fc2.weight, module.blocks.34.ca_norm.weight, module.blocks.35.sa.mat_qkv.weight, module.blocks.35.sa.proj.weight, '\n"
                   " 'module.blocks.35.ca.mat_q.weight, module.blocks.35.ca.mat_kv.weight, module.blocks.35.ca.proj.weight, module.blocks.35.ffn.fc1.weight, module.blocks.35.ffn.fc2.weight, '\n"
                   " 'module.blocks.35.ca_norm.weight, module.blocks.36.sa.mat_qkv.weight, module.blocks.36.sa.proj.weight, module.blocks.36.ca.mat_q.weight, module.blocks.36.ca.mat_kv.weight, '\n"
                   " 'module.blocks.36.ca.proj.weight, module.blocks.36.ffn.fc1.weight, module.blocks.36.ffn.fc2.weight, module.blocks.36.ca_norm.weight, module.blocks.37.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.37.sa.proj.weight, module.blocks.37.ca.mat_q.weight, module.blocks.37.ca.mat_kv.weight, module.blocks.37.ca.proj.weight, module.blocks.37.ffn.fc1.weight, '\n"
                   " 'module.blocks.37.ffn.fc2.weight, module.blocks.37.ca_norm.weight, module.blocks.38.sa.mat_qkv.weight, module.blocks.38.sa.proj.weight, module.blocks.38.ca.mat_q.weight, '\n"
                   " 'module.blocks.38.ca.mat_kv.weight, module.blocks.38.ca.proj.weight, module.blocks.38.ffn.fc1.weight, module.blocks.38.ffn.fc2.weight, module.blocks.38.ca_norm.weight, '\n"
                   " 'module.blocks.39.sa.mat_qkv.weight, module.blocks.39.sa.proj.weight, module.blocks.39.ca.mat_q.weight, module.blocks.39.ca.mat_kv.weight, module.blocks.39.ca.proj.weight, '\n"
                   " 'module.blocks.39.ffn.fc1.weight, module.blocks.39.ffn.fc2.weight, module.blocks.39.ca_norm.weight, module.blocks.40.sa.mat_qkv.weight, module.blocks.40.sa.proj.weight, '\n"
                   " 'module.blocks.40.ca.mat_q.weight, module.blocks.40.ca.mat_kv.weight, module.blocks.40.ca.proj.weight, module.blocks.40.ffn.fc1.weight, module.blocks.40.ffn.fc2.weight, '\n"
                   " 'module.blocks.40.ca_norm.weight, module.blocks.41.sa.mat_qkv.weight, module.blocks.41.sa.proj.weight, module.blocks.41.ca.mat_q.weight, module.blocks.41.ca.mat_kv.weight, '\n"
                   " 'module.blocks.41.ca.proj.weight, module.blocks.41.ffn.fc1.weight, module.blocks.41.ffn.fc2.weight, module.blocks.41.ca_norm.weight, module.blocks.42.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.42.sa.proj.weight, module.blocks.42.ca.mat_q.weight, module.blocks.42.ca.mat_kv.weight, module.blocks.42.ca.proj.weight, module.blocks.42.ffn.fc1.weight, '\n"
                   " 'module.blocks.42.ffn.fc2.weight, module.blocks.42.ca_norm.weight, module.blocks.43.sa.mat_qkv.weight, module.blocks.43.sa.proj.weight, module.blocks.43.ca.mat_q.weight, '\n"
                   " 'module.blocks.43.ca.mat_kv.weight, module.blocks.43.ca.proj.weight, module.blocks.43.ffn.fc1.weight, module.blocks.43.ffn.fc2.weight, module.blocks.43.ca_norm.weight, '\n"
                   " 'module.blocks.44.sa.mat_qkv.weight, module.blocks.44.sa.proj.weight, module.blocks.44.ca.mat_q.weight, module.blocks.44.ca.mat_kv.weight, module.blocks.44.ca.proj.weight, '\n"
                   " 'module.blocks.44.ffn.fc1.weight, module.blocks.44.ffn.fc2.weight, module.blocks.44.ca_norm.weight, module.blocks.45.sa.mat_qkv.weight, module.blocks.45.sa.proj.weight, '\n"
                   " 'module.blocks.45.ca.mat_q.weight, module.blocks.45.ca.mat_kv.weight, module.blocks.45.ca.proj.weight, module.blocks.45.ffn.fc1.weight, module.blocks.45.ffn.fc2.weight, '\n"
                   " 'module.blocks.45.ca_norm.weight, module.blocks.46.sa.mat_qkv.weight, module.blocks.46.sa.proj.weight, module.blocks.46.ca.mat_q.weight, module.blocks.46.ca.mat_kv.weight, '\n"
                   " 'module.blocks.46.ca.proj.weight, module.blocks.46.ffn.fc1.weight, module.blocks.46.ffn.fc2.weight, module.blocks.46.ca_norm.weight, module.blocks.47.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.47.sa.proj.weight, module.blocks.47.ca.mat_q.weight, module.blocks.47.ca.mat_kv.weight, module.blocks.47.ca.proj.weight, module.blocks.47.ffn.fc1.weight, '\n"
                   " 'module.blocks.47.ffn.fc2.weight, module.blocks.47.ca_norm.weight, module.blocks.48.sa.mat_qkv.weight, module.blocks.48.sa.proj.weight, module.blocks.48.ca.mat_q.weight, '\n"
                   " 'module.blocks.48.ca.mat_kv.weight, module.blocks.48.ca.proj.weight, module.blocks.48.ffn.fc1.weight, module.blocks.48.ffn.fc2.weight, module.blocks.48.ca_norm.weight, '\n"
                   " 'module.blocks.49.sa.mat_qkv.weight, module.blocks.49.sa.proj.weight, module.blocks.49.ca.mat_q.weight, module.blocks.49.ca.mat_kv.weight, module.blocks.49.ca.proj.weight, '\n"
                   " 'module.blocks.49.ffn.fc1.weight, module.blocks.49.ffn.fc2.weight, module.blocks.49.ca_norm.weight, module.blocks.50.sa.mat_qkv.weight, module.blocks.50.sa.proj.weight, '\n"
                   " 'module.blocks.50.ca.mat_q.weight, module.blocks.50.ca.mat_kv.weight, module.blocks.50.ca.proj.weight, module.blocks.50.ffn.fc1.weight, module.blocks.50.ffn.fc2.weight, '\n"
                   " 'module.blocks.50.ca_norm.weight, module.blocks.51.sa.mat_qkv.weight, module.blocks.51.sa.proj.weight, module.blocks.51.ca.mat_q.weight, module.blocks.51.ca.mat_kv.weight, '\n"
                   " 'module.blocks.51.ca.proj.weight, module.blocks.51.ffn.fc1.weight, module.blocks.51.ffn.fc2.weight, module.blocks.51.ca_norm.weight, module.blocks.52.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.52.sa.proj.weight, module.blocks.52.ca.mat_q.weight, module.blocks.52.ca.mat_kv.weight, module.blocks.52.ca.proj.weight, module.blocks.52.ffn.fc1.weight, '\n"
                   " 'module.blocks.52.ffn.fc2.weight, module.blocks.52.ca_norm.weight, module.blocks.53.sa.mat_qkv.weight, module.blocks.53.sa.proj.weight, module.blocks.53.ca.mat_q.weight, '\n"
                   " 'module.blocks.53.ca.mat_kv.weight, module.blocks.53.ca.proj.weight, module.blocks.53.ffn.fc1.weight, module.blocks.53.ffn.fc2.weight, module.blocks.53.ca_norm.weight, '\n"
                   " 'module.blocks.54.sa.mat_qkv.weight, module.blocks.54.sa.proj.weight, module.blocks.54.ca.mat_q.weight, module.blocks.54.ca.mat_kv.weight, module.blocks.54.ca.proj.weight, '\n"
                   " 'module.blocks.54.ffn.fc1.weight, module.blocks.54.ffn.fc2.weight, module.blocks.54.ca_norm.weight, module.blocks.55.sa.mat_qkv.weight, module.blocks.55.sa.proj.weight, '\n"
                   " 'module.blocks.55.ca.mat_q.weight, module.blocks.55.ca.mat_kv.weight, module.blocks.55.ca.proj.weight, module.blocks.55.ffn.fc1.weight, module.blocks.55.ffn.fc2.weight, '\n"
                   " 'module.blocks.55.ca_norm.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': '[no scale]',
          'params': "('module.cfg_uncond, module.pos_start, module.text_proj_for_sos.ca.mat_q, module.text_proj_for_sos.ca.v_bias, module.text_proj_for_sos.ca.proj.bias, module.text_proj_for_ca.0.bias, '\n"
                    " 'module.text_proj_for_ca.2.bias, module.lvl_embed.weight, module.word_embed.bias, module.shared_ada_lin.1.bias, module.head_nm.ada_lin.1.bias, module.head.bias, module.blocks.0.ada_gss, '\n"
                    " 'module.blocks.0.sa.scale_mul_1H11, module.blocks.0.sa.q_bias, module.blocks.0.sa.v_bias, module.blocks.0.sa.proj.bias, module.blocks.0.ca.v_bias, module.blocks.0.ca.mat_q.bias, '\n"
                    " 'module.blocks.0.ca.proj.bias, module.blocks.0.ffn.fc1.bias, module.blocks.0.ffn.fc2.bias, module.blocks.0.ca_norm.bias, module.blocks.1.ada_gss, module.blocks.1.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.1.sa.q_bias, module.blocks.1.sa.v_bias, module.blocks.1.sa.proj.bias, module.blocks.1.ca.v_bias, module.blocks.1.ca.mat_q.bias, module.blocks.1.ca.proj.bias, '\n"
                    " 'module.blocks.1.ffn.fc1.bias, module.blocks.1.ffn.fc2.bias, module.blocks.1.ca_norm.bias, module.blocks.2.ada_gss, module.blocks.2.sa.scale_mul_1H11, module.blocks.2.sa.q_bias, '\n"
                    " 'module.blocks.2.sa.v_bias, module.blocks.2.sa.proj.bias, module.blocks.2.ca.v_bias, module.blocks.2.ca.mat_q.bias, module.blocks.2.ca.proj.bias, module.blocks.2.ffn.fc1.bias, '\n"
                    " 'module.blocks.2.ffn.fc2.bias, module.blocks.2.ca_norm.bias, module.blocks.3.ada_gss, module.blocks.3.sa.scale_mul_1H11, module.blocks.3.sa.q_bias, module.blocks.3.sa.v_bias, '\n"
                    " 'module.blocks.3.sa.proj.bias, module.blocks.3.ca.v_bias, module.blocks.3.ca.mat_q.bias, module.blocks.3.ca.proj.bias, module.blocks.3.ffn.fc1.bias, module.blocks.3.ffn.fc2.bias, '\n"
                    " 'module.blocks.3.ca_norm.bias, module.blocks.4.ada_gss, module.blocks.4.sa.scale_mul_1H11, module.blocks.4.sa.q_bias, module.blocks.4.sa.v_bias, module.blocks.4.sa.proj.bias, '\n"
                    " 'module.blocks.4.ca.v_bias, module.blocks.4.ca.mat_q.bias, module.blocks.4.ca.proj.bias, module.blocks.4.ffn.fc1.bias, module.blocks.4.ffn.fc2.bias, module.blocks.4.ca_norm.bias, '\n"
                    " 'module.blocks.5.ada_gss, module.blocks.5.sa.scale_mul_1H11, module.blocks.5.sa.q_bias, module.blocks.5.sa.v_bias, module.blocks.5.sa.proj.bias, module.blocks.5.ca.v_bias, '\n"
                    " 'module.blocks.5.ca.mat_q.bias, module.blocks.5.ca.proj.bias, module.blocks.5.ffn.fc1.bias, module.blocks.5.ffn.fc2.bias, module.blocks.5.ca_norm.bias, module.blocks.6.ada_gss, '\n"
                    " 'module.blocks.6.sa.scale_mul_1H11, module.blocks.6.sa.q_bias, module.blocks.6.sa.v_bias, module.blocks.6.sa.proj.bias, module.blocks.6.ca.v_bias, module.blocks.6.ca.mat_q.bias, '\n"
                    " 'module.blocks.6.ca.proj.bias, module.blocks.6.ffn.fc1.bias, module.blocks.6.ffn.fc2.bias, module.blocks.6.ca_norm.bias, module.blocks.7.ada_gss, module.blocks.7.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.7.sa.q_bias, module.blocks.7.sa.v_bias, module.blocks.7.sa.proj.bias, module.blocks.7.ca.v_bias, module.blocks.7.ca.mat_q.bias, module.blocks.7.ca.proj.bias, '\n"
                    " 'module.blocks.7.ffn.fc1.bias, module.blocks.7.ffn.fc2.bias, module.blocks.7.ca_norm.bias, module.blocks.8.ada_gss, module.blocks.8.sa.scale_mul_1H11, module.blocks.8.sa.q_bias, '\n"
                    " 'module.blocks.8.sa.v_bias, module.blocks.8.sa.proj.bias, module.blocks.8.ca.v_bias, module.blocks.8.ca.mat_q.bias, module.blocks.8.ca.proj.bias, module.blocks.8.ffn.fc1.bias, '\n"
                    " 'module.blocks.8.ffn.fc2.bias, module.blocks.8.ca_norm.bias, module.blocks.9.ada_gss, module.blocks.9.sa.scale_mul_1H11, module.blocks.9.sa.q_bias, module.blocks.9.sa.v_bias, '\n"
                    " 'module.blocks.9.sa.proj.bias, module.blocks.9.ca.v_bias, module.blocks.9.ca.mat_q.bias, module.blocks.9.ca.proj.bias, module.blocks.9.ffn.fc1.bias, module.blocks.9.ffn.fc2.bias, '\n"
                    " 'module.blocks.9.ca_norm.bias, module.blocks.10.ada_gss, module.blocks.10.sa.scale_mul_1H11, module.blocks.10.sa.q_bias, module.blocks.10.sa.v_bias, module.blocks.10.sa.proj.bias, '\n"
                    " 'module.blocks.10.ca.v_bias, module.blocks.10.ca.mat_q.bias, module.blocks.10.ca.proj.bias, module.blocks.10.ffn.fc1.bias, module.blocks.10.ffn.fc2.bias, module.blocks.10.ca_norm.bias, '\n"
                    " 'module.blocks.11.ada_gss, module.blocks.11.sa.scale_mul_1H11, module.blocks.11.sa.q_bias, module.blocks.11.sa.v_bias, module.blocks.11.sa.proj.bias, module.blocks.11.ca.v_bias, '\n"
                    " 'module.blocks.11.ca.mat_q.bias, module.blocks.11.ca.proj.bias, module.blocks.11.ffn.fc1.bias, module.blocks.11.ffn.fc2.bias, module.blocks.11.ca_norm.bias, module.blocks.12.ada_gss, '\n"
                    " 'module.blocks.12.sa.scale_mul_1H11, module.blocks.12.sa.q_bias, module.blocks.12.sa.v_bias, module.blocks.12.sa.proj.bias, module.blocks.12.ca.v_bias, module.blocks.12.ca.mat_q.bias, '\n"
                    " 'module.blocks.12.ca.proj.bias, module.blocks.12.ffn.fc1.bias, module.blocks.12.ffn.fc2.bias, module.blocks.12.ca_norm.bias, module.blocks.13.ada_gss, module.blocks.13.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.13.sa.q_bias, module.blocks.13.sa.v_bias, module.blocks.13.sa.proj.bias, module.blocks.13.ca.v_bias, module.blocks.13.ca.mat_q.bias, module.blocks.13.ca.proj.bias, '\n"
                    " 'module.blocks.13.ffn.fc1.bias, module.blocks.13.ffn.fc2.bias, module.blocks.13.ca_norm.bias, module.blocks.14.ada_gss, module.blocks.14.sa.scale_mul_1H11, module.blocks.14.sa.q_bias, '\n"
                    " 'module.blocks.14.sa.v_bias, module.blocks.14.sa.proj.bias, module.blocks.14.ca.v_bias, module.blocks.14.ca.mat_q.bias, module.blocks.14.ca.proj.bias, module.blocks.14.ffn.fc1.bias, '\n"
                    " 'module.blocks.14.ffn.fc2.bias, module.blocks.14.ca_norm.bias, module.blocks.15.ada_gss, module.blocks.15.sa.scale_mul_1H11, module.blocks.15.sa.q_bias, module.blocks.15.sa.v_bias, '\n"
                    " 'module.blocks.15.sa.proj.bias, module.blocks.15.ca.v_bias, module.blocks.15.ca.mat_q.bias, module.blocks.15.ca.proj.bias, module.blocks.15.ffn.fc1.bias, module.blocks.15.ffn.fc2.bias, '\n"
                    " 'module.blocks.15.ca_norm.bias, module.blocks.16.ada_gss, module.blocks.16.sa.scale_mul_1H11, module.blocks.16.sa.q_bias, module.blocks.16.sa.v_bias, module.blocks.16.sa.proj.bias, '\n"
                    " 'module.blocks.16.ca.v_bias, module.blocks.16.ca.mat_q.bias, module.blocks.16.ca.proj.bias, module.blocks.16.ffn.fc1.bias, module.blocks.16.ffn.fc2.bias, module.blocks.16.ca_norm.bias, '\n"
                    " 'module.blocks.17.ada_gss, module.blocks.17.sa.scale_mul_1H11, module.blocks.17.sa.q_bias, module.blocks.17.sa.v_bias, module.blocks.17.sa.proj.bias, module.blocks.17.ca.v_bias, '\n"
                    " 'module.blocks.17.ca.mat_q.bias, module.blocks.17.ca.proj.bias, module.blocks.17.ffn.fc1.bias, module.blocks.17.ffn.fc2.bias, module.blocks.17.ca_norm.bias, module.blocks.18.ada_gss, '\n"
                    " 'module.blocks.18.sa.scale_mul_1H11, module.blocks.18.sa.q_bias, module.blocks.18.sa.v_bias, module.blocks.18.sa.proj.bias, module.blocks.18.ca.v_bias, module.blocks.18.ca.mat_q.bias, '\n"
                    " 'module.blocks.18.ca.proj.bias, module.blocks.18.ffn.fc1.bias, module.blocks.18.ffn.fc2.bias, module.blocks.18.ca_norm.bias, module.blocks.19.ada_gss, module.blocks.19.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.19.sa.q_bias, module.blocks.19.sa.v_bias, module.blocks.19.sa.proj.bias, module.blocks.19.ca.v_bias, module.blocks.19.ca.mat_q.bias, module.blocks.19.ca.proj.bias, '\n"
                    " 'module.blocks.19.ffn.fc1.bias, module.blocks.19.ffn.fc2.bias, module.blocks.19.ca_norm.bias, module.blocks.20.ada_gss, module.blocks.20.sa.scale_mul_1H11, module.blocks.20.sa.q_bias, '\n"
                    " 'module.blocks.20.sa.v_bias, module.blocks.20.sa.proj.bias, module.blocks.20.ca.v_bias, module.blocks.20.ca.mat_q.bias, module.blocks.20.ca.proj.bias, module.blocks.20.ffn.fc1.bias, '\n"
                    " 'module.blocks.20.ffn.fc2.bias, module.blocks.20.ca_norm.bias, module.blocks.21.ada_gss, module.blocks.21.sa.scale_mul_1H11, module.blocks.21.sa.q_bias, module.blocks.21.sa.v_bias, '\n"
                    " 'module.blocks.21.sa.proj.bias, module.blocks.21.ca.v_bias, module.blocks.21.ca.mat_q.bias, module.blocks.21.ca.proj.bias, module.blocks.21.ffn.fc1.bias, module.blocks.21.ffn.fc2.bias, '\n"
                    " 'module.blocks.21.ca_norm.bias, module.blocks.22.ada_gss, module.blocks.22.sa.scale_mul_1H11, module.blocks.22.sa.q_bias, module.blocks.22.sa.v_bias, module.blocks.22.sa.proj.bias, '\n"
                    " 'module.blocks.22.ca.v_bias, module.blocks.22.ca.mat_q.bias, module.blocks.22.ca.proj.bias, module.blocks.22.ffn.fc1.bias, module.blocks.22.ffn.fc2.bias, module.blocks.22.ca_norm.bias, '\n"
                    " 'module.blocks.23.ada_gss, module.blocks.23.sa.scale_mul_1H11, module.blocks.23.sa.q_bias, module.blocks.23.sa.v_bias, module.blocks.23.sa.proj.bias, module.blocks.23.ca.v_bias, '\n"
                    " 'module.blocks.23.ca.mat_q.bias, module.blocks.23.ca.proj.bias, module.blocks.23.ffn.fc1.bias, module.blocks.23.ffn.fc2.bias, module.blocks.23.ca_norm.bias, module.blocks.24.ada_gss, '\n"
                    " 'module.blocks.24.sa.scale_mul_1H11, module.blocks.24.sa.q_bias, module.blocks.24.sa.v_bias, module.blocks.24.sa.proj.bias, module.blocks.24.ca.v_bias, module.blocks.24.ca.mat_q.bias, '\n"
                    " 'module.blocks.24.ca.proj.bias, module.blocks.24.ffn.fc1.bias, module.blocks.24.ffn.fc2.bias, module.blocks.24.ca_norm.bias, module.blocks.25.ada_gss, module.blocks.25.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.25.sa.q_bias, module.blocks.25.sa.v_bias, module.blocks.25.sa.proj.bias, module.blocks.25.ca.v_bias, module.blocks.25.ca.mat_q.bias, module.blocks.25.ca.proj.bias, '\n"
                    " 'module.blocks.25.ffn.fc1.bias, module.blocks.25.ffn.fc2.bias, module.blocks.25.ca_norm.bias, module.blocks.26.ada_gss, module.blocks.26.sa.scale_mul_1H11, module.blocks.26.sa.q_bias, '\n"
                    " 'module.blocks.26.sa.v_bias, module.blocks.26.sa.proj.bias, module.blocks.26.ca.v_bias, module.blocks.26.ca.mat_q.bias, module.blocks.26.ca.proj.bias, module.blocks.26.ffn.fc1.bias, '\n"
                    " 'module.blocks.26.ffn.fc2.bias, module.blocks.26.ca_norm.bias, module.blocks.27.ada_gss, module.blocks.27.sa.scale_mul_1H11, module.blocks.27.sa.q_bias, module.blocks.27.sa.v_bias, '\n"
                    " 'module.blocks.27.sa.proj.bias, module.blocks.27.ca.v_bias, module.blocks.27.ca.mat_q.bias, module.blocks.27.ca.proj.bias, module.blocks.27.ffn.fc1.bias, module.blocks.27.ffn.fc2.bias, '\n"
                    " 'module.blocks.27.ca_norm.bias, module.blocks.28.ada_gss, module.blocks.28.sa.scale_mul_1H11, module.blocks.28.sa.q_bias, module.blocks.28.sa.v_bias, module.blocks.28.sa.proj.bias, '\n"
                    " 'module.blocks.28.ca.v_bias, module.blocks.28.ca.mat_q.bias, module.blocks.28.ca.proj.bias, module.blocks.28.ffn.fc1.bias, module.blocks.28.ffn.fc2.bias, module.blocks.28.ca_norm.bias, '\n"
                    " 'module.blocks.29.ada_gss, module.blocks.29.sa.scale_mul_1H11, module.blocks.29.sa.q_bias, module.blocks.29.sa.v_bias, module.blocks.29.sa.proj.bias, module.blocks.29.ca.v_bias, '\n"
                    " 'module.blocks.29.ca.mat_q.bias, module.blocks.29.ca.proj.bias, module.blocks.29.ffn.fc1.bias, module.blocks.29.ffn.fc2.bias, module.blocks.29.ca_norm.bias, module.blocks.30.ada_gss, '\n"
                    " 'module.blocks.30.sa.scale_mul_1H11, module.blocks.30.sa.q_bias, module.blocks.30.sa.v_bias, module.blocks.30.sa.proj.bias, module.blocks.30.ca.v_bias, module.blocks.30.ca.mat_q.bias, '\n"
                    " 'module.blocks.30.ca.proj.bias, module.blocks.30.ffn.fc1.bias, module.blocks.30.ffn.fc2.bias, module.blocks.30.ca_norm.bias, module.blocks.31.ada_gss, module.blocks.31.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.31.sa.q_bias, module.blocks.31.sa.v_bias, module.blocks.31.sa.proj.bias, module.blocks.31.ca.v_bias, module.blocks.31.ca.mat_q.bias, module.blocks.31.ca.proj.bias, '\n"
                    " 'module.blocks.31.ffn.fc1.bias, module.blocks.31.ffn.fc2.bias, module.blocks.31.ca_norm.bias, module.blocks.32.ada_gss, module.blocks.32.sa.scale_mul_1H11, module.blocks.32.sa.q_bias, '\n"
                    " 'module.blocks.32.sa.v_bias, module.blocks.32.sa.proj.bias, module.blocks.32.ca.v_bias, module.blocks.32.ca.mat_q.bias, module.blocks.32.ca.proj.bias, module.blocks.32.ffn.fc1.bias, '\n"
                    " 'module.blocks.32.ffn.fc2.bias, module.blocks.32.ca_norm.bias, module.blocks.33.ada_gss, module.blocks.33.sa.scale_mul_1H11, module.blocks.33.sa.q_bias, module.blocks.33.sa.v_bias, '\n"
                    " 'module.blocks.33.sa.proj.bias, module.blocks.33.ca.v_bias, module.blocks.33.ca.mat_q.bias, module.blocks.33.ca.proj.bias, module.blocks.33.ffn.fc1.bias, module.blocks.33.ffn.fc2.bias, '\n"
                    " 'module.blocks.33.ca_norm.bias, module.blocks.34.ada_gss, module.blocks.34.sa.scale_mul_1H11, module.blocks.34.sa.q_bias, module.blocks.34.sa.v_bias, module.blocks.34.sa.proj.bias, '\n"
                    " 'module.blocks.34.ca.v_bias, module.blocks.34.ca.mat_q.bias, module.blocks.34.ca.proj.bias, module.blocks.34.ffn.fc1.bias, module.blocks.34.ffn.fc2.bias, module.blocks.34.ca_norm.bias, '\n"
                    " 'module.blocks.35.ada_gss, module.blocks.35.sa.scale_mul_1H11, module.blocks.35.sa.q_bias, module.blocks.35.sa.v_bias, module.blocks.35.sa.proj.bias, module.blocks.35.ca.v_bias, '\n"
                    " 'module.blocks.35.ca.mat_q.bias, module.blocks.35.ca.proj.bias, module.blocks.35.ffn.fc1.bias, module.blocks.35.ffn.fc2.bias, module.blocks.35.ca_norm.bias, module.blocks.36.ada_gss, '\n"
                    " 'module.blocks.36.sa.scale_mul_1H11, module.blocks.36.sa.q_bias, module.blocks.36.sa.v_bias, module.blocks.36.sa.proj.bias, module.blocks.36.ca.v_bias, module.blocks.36.ca.mat_q.bias, '\n"
                    " 'module.blocks.36.ca.proj.bias, module.blocks.36.ffn.fc1.bias, module.blocks.36.ffn.fc2.bias, module.blocks.36.ca_norm.bias, module.blocks.37.ada_gss, module.blocks.37.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.37.sa.q_bias, module.blocks.37.sa.v_bias, module.blocks.37.sa.proj.bias, module.blocks.37.ca.v_bias, module.blocks.37.ca.mat_q.bias, module.blocks.37.ca.proj.bias, '\n"
                    " 'module.blocks.37.ffn.fc1.bias, module.blocks.37.ffn.fc2.bias, module.blocks.37.ca_norm.bias, module.blocks.38.ada_gss, module.blocks.38.sa.scale_mul_1H11, module.blocks.38.sa.q_bias, '\n"
                    " 'module.blocks.38.sa.v_bias, module.blocks.38.sa.proj.bias, module.blocks.38.ca.v_bias, module.blocks.38.ca.mat_q.bias, module.blocks.38.ca.proj.bias, module.blocks.38.ffn.fc1.bias, '\n"
                    " 'module.blocks.38.ffn.fc2.bias, module.blocks.38.ca_norm.bias, module.blocks.39.ada_gss, module.blocks.39.sa.scale_mul_1H11, module.blocks.39.sa.q_bias, module.blocks.39.sa.v_bias, '\n"
                    " 'module.blocks.39.sa.proj.bias, module.blocks.39.ca.v_bias, module.blocks.39.ca.mat_q.bias, module.blocks.39.ca.proj.bias, module.blocks.39.ffn.fc1.bias, module.blocks.39.ffn.fc2.bias, '\n"
                    " 'module.blocks.39.ca_norm.bias, module.blocks.40.ada_gss, module.blocks.40.sa.scale_mul_1H11, module.blocks.40.sa.q_bias, module.blocks.40.sa.v_bias, module.blocks.40.sa.proj.bias, '\n"
                    " 'module.blocks.40.ca.v_bias, module.blocks.40.ca.mat_q.bias, module.blocks.40.ca.proj.bias, module.blocks.40.ffn.fc1.bias, module.blocks.40.ffn.fc2.bias, module.blocks.40.ca_norm.bias, '\n"
                    " 'module.blocks.41.ada_gss, module.blocks.41.sa.scale_mul_1H11, module.blocks.41.sa.q_bias, module.blocks.41.sa.v_bias, module.blocks.41.sa.proj.bias, module.blocks.41.ca.v_bias, '\n"
                    " 'module.blocks.41.ca.mat_q.bias, module.blocks.41.ca.proj.bias, module.blocks.41.ffn.fc1.bias, module.blocks.41.ffn.fc2.bias, module.blocks.41.ca_norm.bias, module.blocks.42.ada_gss, '\n"
                    " 'module.blocks.42.sa.scale_mul_1H11, module.blocks.42.sa.q_bias, module.blocks.42.sa.v_bias, module.blocks.42.sa.proj.bias, module.blocks.42.ca.v_bias, module.blocks.42.ca.mat_q.bias, '\n"
                    " 'module.blocks.42.ca.proj.bias, module.blocks.42.ffn.fc1.bias, module.blocks.42.ffn.fc2.bias, module.blocks.42.ca_norm.bias, module.blocks.43.ada_gss, module.blocks.43.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.43.sa.q_bias, module.blocks.43.sa.v_bias, module.blocks.43.sa.proj.bias, module.blocks.43.ca.v_bias, module.blocks.43.ca.mat_q.bias, module.blocks.43.ca.proj.bias, '\n"
                    " 'module.blocks.43.ffn.fc1.bias, module.blocks.43.ffn.fc2.bias, module.blocks.43.ca_norm.bias, module.blocks.44.ada_gss, module.blocks.44.sa.scale_mul_1H11, module.blocks.44.sa.q_bias, '\n"
                    " 'module.blocks.44.sa.v_bias, module.blocks.44.sa.proj.bias, module.blocks.44.ca.v_bias, module.blocks.44.ca.mat_q.bias, module.blocks.44.ca.proj.bias, module.blocks.44.ffn.fc1.bias, '\n"
                    " 'module.blocks.44.ffn.fc2.bias, module.blocks.44.ca_norm.bias, module.blocks.45.ada_gss, module.blocks.45.sa.scale_mul_1H11, module.blocks.45.sa.q_bias, module.blocks.45.sa.v_bias, '\n"
                    " 'module.blocks.45.sa.proj.bias, module.blocks.45.ca.v_bias, module.blocks.45.ca.mat_q.bias, module.blocks.45.ca.proj.bias, module.blocks.45.ffn.fc1.bias, module.blocks.45.ffn.fc2.bias, '\n"
                    " 'module.blocks.45.ca_norm.bias, module.blocks.46.ada_gss, module.blocks.46.sa.scale_mul_1H11, module.blocks.46.sa.q_bias, module.blocks.46.sa.v_bias, module.blocks.46.sa.proj.bias, '\n"
                    " 'module.blocks.46.ca.v_bias, module.blocks.46.ca.mat_q.bias, module.blocks.46.ca.proj.bias, module.blocks.46.ffn.fc1.bias, module.blocks.46.ffn.fc2.bias, module.blocks.46.ca_norm.bias, '\n"
                    " 'module.blocks.47.ada_gss, module.blocks.47.sa.scale_mul_1H11, module.blocks.47.sa.q_bias, module.blocks.47.sa.v_bias, module.blocks.47.sa.proj.bias, module.blocks.47.ca.v_bias, '\n"
                    " 'module.blocks.47.ca.mat_q.bias, module.blocks.47.ca.proj.bias, module.blocks.47.ffn.fc1.bias, module.blocks.47.ffn.fc2.bias, module.blocks.47.ca_norm.bias, module.blocks.48.ada_gss, '\n"
                    " 'module.blocks.48.sa.scale_mul_1H11, module.blocks.48.sa.q_bias, module.blocks.48.sa.v_bias, module.blocks.48.sa.proj.bias, module.blocks.48.ca.v_bias, module.blocks.48.ca.mat_q.bias, '\n"
                    " 'module.blocks.48.ca.proj.bias, module.blocks.48.ffn.fc1.bias, module.blocks.48.ffn.fc2.bias, module.blocks.48.ca_norm.bias, module.blocks.49.ada_gss, module.blocks.49.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.49.sa.q_bias, module.blocks.49.sa.v_bias, module.blocks.49.sa.proj.bias, module.blocks.49.ca.v_bias, module.blocks.49.ca.mat_q.bias, module.blocks.49.ca.proj.bias, '\n"
                    " 'module.blocks.49.ffn.fc1.bias, module.blocks.49.ffn.fc2.bias, module.blocks.49.ca_norm.bias, module.blocks.50.ada_gss, module.blocks.50.sa.scale_mul_1H11, module.blocks.50.sa.q_bias, '\n"
                    " 'module.blocks.50.sa.v_bias, module.blocks.50.sa.proj.bias, module.blocks.50.ca.v_bias, module.blocks.50.ca.mat_q.bias, module.blocks.50.ca.proj.bias, module.blocks.50.ffn.fc1.bias, '\n"
                    " 'module.blocks.50.ffn.fc2.bias, module.blocks.50.ca_norm.bias, module.blocks.51.ada_gss, module.blocks.51.sa.scale_mul_1H11, module.blocks.51.sa.q_bias, module.blocks.51.sa.v_bias, '\n"
                    " 'module.blocks.51.sa.proj.bias, module.blocks.51.ca.v_bias, module.blocks.51.ca.mat_q.bias, module.blocks.51.ca.proj.bias, module.blocks.51.ffn.fc1.bias, module.blocks.51.ffn.fc2.bias, '\n"
                    " 'module.blocks.51.ca_norm.bias, module.blocks.52.ada_gss, module.blocks.52.sa.scale_mul_1H11, module.blocks.52.sa.q_bias, module.blocks.52.sa.v_bias, module.blocks.52.sa.proj.bias, '\n"
                    " 'module.blocks.52.ca.v_bias, module.blocks.52.ca.mat_q.bias, module.blocks.52.ca.proj.bias, module.blocks.52.ffn.fc1.bias, module.blocks.52.ffn.fc2.bias, module.blocks.52.ca_norm.bias, '\n"
                    " 'module.blocks.53.ada_gss, module.blocks.53.sa.scale_mul_1H11, module.blocks.53.sa.q_bias, module.blocks.53.sa.v_bias, module.blocks.53.sa.proj.bias, module.blocks.53.ca.v_bias, '\n"
                    " 'module.blocks.53.ca.mat_q.bias, module.blocks.53.ca.proj.bias, module.blocks.53.ffn.fc1.bias, module.blocks.53.ffn.fc2.bias, module.blocks.53.ca_norm.bias, module.blocks.54.ada_gss, '\n"
                    " 'module.blocks.54.sa.scale_mul_1H11, module.blocks.54.sa.q_bias, module.blocks.54.sa.v_bias, module.blocks.54.sa.proj.bias, module.blocks.54.ca.v_bias, module.blocks.54.ca.mat_q.bias, '\n"
                    " 'module.blocks.54.ca.proj.bias, module.blocks.54.ffn.fc1.bias, module.blocks.54.ffn.fc2.bias, module.blocks.54.ca_norm.bias, module.blocks.55.ada_gss, module.blocks.55.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.55.sa.q_bias, module.blocks.55.sa.v_bias, module.blocks.55.sa.proj.bias, module.blocks.55.ca.v_bias, module.blocks.55.ca.mat_q.bias, module.blocks.55.ca.proj.bias, '\n"
                    " 'module.blocks.55.ffn.fc1.bias, module.blocks.55.ffn.fc2.bias, module.blocks.55.ca_norm.bias')",
          'wd_sc': 0.0}}

[09-01 22:56:23] (nity/utils/lr_control.py, line 116)=> [get_param_groups][rank0] type(model).__name__='NullDDP' count=1085, numel=8573892016
[09-01 22:56:23] (nity/utils/lr_control.py, line 117)=> 
[09-01 22:56:23] (inity_circuit_breaker.py, line 343)=> [vgpt] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.97), fused=True), opt_kw={'lr': 9.375e-05, 'weight_decay': 0}

[09-01 22:56:23] (inity_circuit_breaker.py, line 348)=> Loading T5 from /home/gs285/VAR/my_model/weights/flan-t5-xl...







=======================================================   RESTART [09-01 23:01:49]   =======================================================
[09-01 23:01:49] (inity_circuit_breaker.py, line 577)=> ============================================================
[09-01 23:01:49] (inity_circuit_breaker.py, line 578)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[09-01 23:01:49] (inity_circuit_breaker.py, line 579)=> Circuit Breaker Alpha: 0.1
[09-01 23:01:49] (inity_circuit_breaker.py, line 580)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[09-01 23:01:49] (inity_circuit_breaker.py, line 581)=> Circuit Breaker Enabled: True
[09-01 23:01:49] (inity_circuit_breaker.py, line 582)=> Number of Examples: 1000
[09-01 23:01:49] (inity_circuit_breaker.py, line 583)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[09-01 23:01:49] (inity_circuit_breaker.py, line 584)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[09-01 23:01:49] (inity_circuit_breaker.py, line 585)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[09-01 23:01:49] (inity_circuit_breaker.py, line 586)=> Validation Ratio: 0.1
[09-01 23:01:49] (inity_circuit_breaker.py, line 587)=> Category: hate
[09-01 23:01:49] (inity_circuit_breaker.py, line 588)=> Batch Size: 4
[09-01 23:01:49] (inity_circuit_breaker.py, line 589)=> Workers: 4
[09-01 23:01:49] (inity_circuit_breaker.py, line 590)=> Device: cuda:0
[09-01 23:01:49] (inity_circuit_breaker.py, line 591)=> Model: infinity_8b
[09-01 23:01:49] (inity_circuit_breaker.py, line 592)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-01 23:01:49] (inity_circuit_breaker.py, line 593)=> Rush Resume: 
[09-01 23:01:49] (inity_circuit_breaker.py, line 594)=> Selective Layers: 0,1,2,3,4,5
[09-01 23:01:49] (inity_circuit_breaker.py, line 595)=> ============================================================
[09-01 23:01:49] (inity_circuit_breaker.py, line 377)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[09-01 23:01:49] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-01 23:01:49] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-01 23:01:49] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[09-01 23:01:49] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-01 23:01:49] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-01 23:01:50] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[09-01 23:01:50] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[09-01 23:01:50] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[09-01 23:01:50] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[09-01 23:01:50] (inity_circuit_breaker.py, line 395)=> args.batch_size=4, vbs=6
[09-01 23:01:50] (inity_circuit_breaker.py, line 418)=> len(dataloader): 100, len(dataset): 100
[09-01 23:01:50] (inity_circuit_breaker.py, line 420)=> total_samples: 200
[09-01 23:01:50] (inity_circuit_breaker.py, line 421)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[09-01 23:01:50] (inity_circuit_breaker.py, line 140)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[09-01 23:01:50] (inity_circuit_breaker.py, line 142)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[09-01 23:01:50] (inity_circuit_breaker.py, line 146)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-01 23:01:52] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[09-01 23:01:52] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[09-01 23:01:52] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[09-01 23:01:52] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[09-01 23:01:54] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[09-01 23:01:56] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[09-01 23:01:56] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[09-01 23:01:56] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[09-01 23:01:56] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[09-01 23:01:56] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

[09-01 23:02:07] (ity/models/init_param.py, line  15)=> [init_weights] Infinity with std=0.02
[09-01 23:02:07] (inity_circuit_breaker.py, line 246)=> [PT] GPT model = Infinity(
  drop_path_rate=0.2
  (norm0_cond): Identity()
  (text_norm): FastRMSNorm(C=2048, eps=1e-06, elementwise_affine=True)
  (text_proj_for_sos): TextAttentivePool(
    (ca): CrossAttention(
      Cq=3072, Ckv=2048, cos_attn=False
      (mat_kv): Linear(in_features=2048, out_features=6144, bias=False)
      (proj): Linear(in_features=3072, out_features=3072, bias=True)
      (proj_drop): Identity()
    )
  )
  (text_proj_for_ca): Sequential(
    (0): Linear(in_features=2048, out_features=3072, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=3072, out_features=3072, bias=True)
  )
  (lvl_embed): Embedding(15, 3072)
  (norm0_ve): Identity()
  (word_embed): Linear(in_features=56, out_features=3072, bias=True)
  (shared_ada_lin): Sequential(
    (0): SiLU()
    (1): SharedAdaLin(in_features=3072, out_features=18432, bias=True)
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=3072, out_features=6144, bias=True)
    )
  )
  (head): Linear(in_features=3072, out_features=112, bias=True)
  (blocks): ModuleList(
    (0): CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): Identity()
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
    (1-55): 55 x CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): DropPath(...)
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
  )
)


[09-01 23:02:07] (inity_circuit_breaker.py, line 248)=> [PT][#para] VAE=81.62, VAE.quant=0.00
[09-01 23:02:07] (inity_circuit_breaker.py, line 251)=> [PT][#para] GPT=8573.89


[09-01 23:02:07] (nity/utils/lr_control.py, line  71)=> [get_param_groups][lr decay] with_lr_scale=False, lr_scale=0.0
[09-01 23:02:07] (nity/utils/lr_control.py, line 111)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': '[no scale]',
         'params': "('module.text_norm.weight, module.text_proj_for_sos.ca.mat_kv.weight, module.text_proj_for_sos.ca.proj.weight, module.text_proj_for_ca.0.weight, module.text_proj_for_ca.2.weight, '\n"
                   " 'module.word_embed.weight, module.shared_ada_lin.1.weight, module.head_nm.ada_lin.1.weight, module.head.weight, module.blocks.0.sa.mat_qkv.weight, module.blocks.0.sa.proj.weight, '\n"
                   " 'module.blocks.0.ca.mat_q.weight, module.blocks.0.ca.mat_kv.weight, module.blocks.0.ca.proj.weight, module.blocks.0.ffn.fc1.weight, module.blocks.0.ffn.fc2.weight, module.blocks.0.ca_norm.weight, '\n"
                   " 'module.blocks.1.sa.mat_qkv.weight, module.blocks.1.sa.proj.weight, module.blocks.1.ca.mat_q.weight, module.blocks.1.ca.mat_kv.weight, module.blocks.1.ca.proj.weight, '\n"
                   " 'module.blocks.1.ffn.fc1.weight, module.blocks.1.ffn.fc2.weight, module.blocks.1.ca_norm.weight, module.blocks.2.sa.mat_qkv.weight, module.blocks.2.sa.proj.weight, module.blocks.2.ca.mat_q.weight, '\n"
                   " 'module.blocks.2.ca.mat_kv.weight, module.blocks.2.ca.proj.weight, module.blocks.2.ffn.fc1.weight, module.blocks.2.ffn.fc2.weight, module.blocks.2.ca_norm.weight, module.blocks.3.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.3.sa.proj.weight, module.blocks.3.ca.mat_q.weight, module.blocks.3.ca.mat_kv.weight, module.blocks.3.ca.proj.weight, module.blocks.3.ffn.fc1.weight, module.blocks.3.ffn.fc2.weight, '\n"
                   " 'module.blocks.3.ca_norm.weight, module.blocks.4.sa.mat_qkv.weight, module.blocks.4.sa.proj.weight, module.blocks.4.ca.mat_q.weight, module.blocks.4.ca.mat_kv.weight, '\n"
                   " 'module.blocks.4.ca.proj.weight, module.blocks.4.ffn.fc1.weight, module.blocks.4.ffn.fc2.weight, module.blocks.4.ca_norm.weight, module.blocks.5.sa.mat_qkv.weight, module.blocks.5.sa.proj.weight, '\n"
                   " 'module.blocks.5.ca.mat_q.weight, module.blocks.5.ca.mat_kv.weight, module.blocks.5.ca.proj.weight, module.blocks.5.ffn.fc1.weight, module.blocks.5.ffn.fc2.weight, module.blocks.5.ca_norm.weight, '\n"
                   " 'module.blocks.6.sa.mat_qkv.weight, module.blocks.6.sa.proj.weight, module.blocks.6.ca.mat_q.weight, module.blocks.6.ca.mat_kv.weight, module.blocks.6.ca.proj.weight, '\n"
                   " 'module.blocks.6.ffn.fc1.weight, module.blocks.6.ffn.fc2.weight, module.blocks.6.ca_norm.weight, module.blocks.7.sa.mat_qkv.weight, module.blocks.7.sa.proj.weight, module.blocks.7.ca.mat_q.weight, '\n"
                   " 'module.blocks.7.ca.mat_kv.weight, module.blocks.7.ca.proj.weight, module.blocks.7.ffn.fc1.weight, module.blocks.7.ffn.fc2.weight, module.blocks.7.ca_norm.weight, module.blocks.8.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.8.sa.proj.weight, module.blocks.8.ca.mat_q.weight, module.blocks.8.ca.mat_kv.weight, module.blocks.8.ca.proj.weight, module.blocks.8.ffn.fc1.weight, module.blocks.8.ffn.fc2.weight, '\n"
                   " 'module.blocks.8.ca_norm.weight, module.blocks.9.sa.mat_qkv.weight, module.blocks.9.sa.proj.weight, module.blocks.9.ca.mat_q.weight, module.blocks.9.ca.mat_kv.weight, '\n"
                   " 'module.blocks.9.ca.proj.weight, module.blocks.9.ffn.fc1.weight, module.blocks.9.ffn.fc2.weight, module.blocks.9.ca_norm.weight, module.blocks.10.sa.mat_qkv.weight, module.blocks.10.sa.proj.weight, '\n"
                   " 'module.blocks.10.ca.mat_q.weight, module.blocks.10.ca.mat_kv.weight, module.blocks.10.ca.proj.weight, module.blocks.10.ffn.fc1.weight, module.blocks.10.ffn.fc2.weight, '\n"
                   " 'module.blocks.10.ca_norm.weight, module.blocks.11.sa.mat_qkv.weight, module.blocks.11.sa.proj.weight, module.blocks.11.ca.mat_q.weight, module.blocks.11.ca.mat_kv.weight, '\n"
                   " 'module.blocks.11.ca.proj.weight, module.blocks.11.ffn.fc1.weight, module.blocks.11.ffn.fc2.weight, module.blocks.11.ca_norm.weight, module.blocks.12.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.12.sa.proj.weight, module.blocks.12.ca.mat_q.weight, module.blocks.12.ca.mat_kv.weight, module.blocks.12.ca.proj.weight, module.blocks.12.ffn.fc1.weight, '\n"
                   " 'module.blocks.12.ffn.fc2.weight, module.blocks.12.ca_norm.weight, module.blocks.13.sa.mat_qkv.weight, module.blocks.13.sa.proj.weight, module.blocks.13.ca.mat_q.weight, '\n"
                   " 'module.blocks.13.ca.mat_kv.weight, module.blocks.13.ca.proj.weight, module.blocks.13.ffn.fc1.weight, module.blocks.13.ffn.fc2.weight, module.blocks.13.ca_norm.weight, '\n"
                   " 'module.blocks.14.sa.mat_qkv.weight, module.blocks.14.sa.proj.weight, module.blocks.14.ca.mat_q.weight, module.blocks.14.ca.mat_kv.weight, module.blocks.14.ca.proj.weight, '\n"
                   " 'module.blocks.14.ffn.fc1.weight, module.blocks.14.ffn.fc2.weight, module.blocks.14.ca_norm.weight, module.blocks.15.sa.mat_qkv.weight, module.blocks.15.sa.proj.weight, '\n"
                   " 'module.blocks.15.ca.mat_q.weight, module.blocks.15.ca.mat_kv.weight, module.blocks.15.ca.proj.weight, module.blocks.15.ffn.fc1.weight, module.blocks.15.ffn.fc2.weight, '\n"
                   " 'module.blocks.15.ca_norm.weight, module.blocks.16.sa.mat_qkv.weight, module.blocks.16.sa.proj.weight, module.blocks.16.ca.mat_q.weight, module.blocks.16.ca.mat_kv.weight, '\n"
                   " 'module.blocks.16.ca.proj.weight, module.blocks.16.ffn.fc1.weight, module.blocks.16.ffn.fc2.weight, module.blocks.16.ca_norm.weight, module.blocks.17.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.17.sa.proj.weight, module.blocks.17.ca.mat_q.weight, module.blocks.17.ca.mat_kv.weight, module.blocks.17.ca.proj.weight, module.blocks.17.ffn.fc1.weight, '\n"
                   " 'module.blocks.17.ffn.fc2.weight, module.blocks.17.ca_norm.weight, module.blocks.18.sa.mat_qkv.weight, module.blocks.18.sa.proj.weight, module.blocks.18.ca.mat_q.weight, '\n"
                   " 'module.blocks.18.ca.mat_kv.weight, module.blocks.18.ca.proj.weight, module.blocks.18.ffn.fc1.weight, module.blocks.18.ffn.fc2.weight, module.blocks.18.ca_norm.weight, '\n"
                   " 'module.blocks.19.sa.mat_qkv.weight, module.blocks.19.sa.proj.weight, module.blocks.19.ca.mat_q.weight, module.blocks.19.ca.mat_kv.weight, module.blocks.19.ca.proj.weight, '\n"
                   " 'module.blocks.19.ffn.fc1.weight, module.blocks.19.ffn.fc2.weight, module.blocks.19.ca_norm.weight, module.blocks.20.sa.mat_qkv.weight, module.blocks.20.sa.proj.weight, '\n"
                   " 'module.blocks.20.ca.mat_q.weight, module.blocks.20.ca.mat_kv.weight, module.blocks.20.ca.proj.weight, module.blocks.20.ffn.fc1.weight, module.blocks.20.ffn.fc2.weight, '\n"
                   " 'module.blocks.20.ca_norm.weight, module.blocks.21.sa.mat_qkv.weight, module.blocks.21.sa.proj.weight, module.blocks.21.ca.mat_q.weight, module.blocks.21.ca.mat_kv.weight, '\n"
                   " 'module.blocks.21.ca.proj.weight, module.blocks.21.ffn.fc1.weight, module.blocks.21.ffn.fc2.weight, module.blocks.21.ca_norm.weight, module.blocks.22.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.22.sa.proj.weight, module.blocks.22.ca.mat_q.weight, module.blocks.22.ca.mat_kv.weight, module.blocks.22.ca.proj.weight, module.blocks.22.ffn.fc1.weight, '\n"
                   " 'module.blocks.22.ffn.fc2.weight, module.blocks.22.ca_norm.weight, module.blocks.23.sa.mat_qkv.weight, module.blocks.23.sa.proj.weight, module.blocks.23.ca.mat_q.weight, '\n"
                   " 'module.blocks.23.ca.mat_kv.weight, module.blocks.23.ca.proj.weight, module.blocks.23.ffn.fc1.weight, module.blocks.23.ffn.fc2.weight, module.blocks.23.ca_norm.weight, '\n"
                   " 'module.blocks.24.sa.mat_qkv.weight, module.blocks.24.sa.proj.weight, module.blocks.24.ca.mat_q.weight, module.blocks.24.ca.mat_kv.weight, module.blocks.24.ca.proj.weight, '\n"
                   " 'module.blocks.24.ffn.fc1.weight, module.blocks.24.ffn.fc2.weight, module.blocks.24.ca_norm.weight, module.blocks.25.sa.mat_qkv.weight, module.blocks.25.sa.proj.weight, '\n"
                   " 'module.blocks.25.ca.mat_q.weight, module.blocks.25.ca.mat_kv.weight, module.blocks.25.ca.proj.weight, module.blocks.25.ffn.fc1.weight, module.blocks.25.ffn.fc2.weight, '\n"
                   " 'module.blocks.25.ca_norm.weight, module.blocks.26.sa.mat_qkv.weight, module.blocks.26.sa.proj.weight, module.blocks.26.ca.mat_q.weight, module.blocks.26.ca.mat_kv.weight, '\n"
                   " 'module.blocks.26.ca.proj.weight, module.blocks.26.ffn.fc1.weight, module.blocks.26.ffn.fc2.weight, module.blocks.26.ca_norm.weight, module.blocks.27.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.27.sa.proj.weight, module.blocks.27.ca.mat_q.weight, module.blocks.27.ca.mat_kv.weight, module.blocks.27.ca.proj.weight, module.blocks.27.ffn.fc1.weight, '\n"
                   " 'module.blocks.27.ffn.fc2.weight, module.blocks.27.ca_norm.weight, module.blocks.28.sa.mat_qkv.weight, module.blocks.28.sa.proj.weight, module.blocks.28.ca.mat_q.weight, '\n"
                   " 'module.blocks.28.ca.mat_kv.weight, module.blocks.28.ca.proj.weight, module.blocks.28.ffn.fc1.weight, module.blocks.28.ffn.fc2.weight, module.blocks.28.ca_norm.weight, '\n"
                   " 'module.blocks.29.sa.mat_qkv.weight, module.blocks.29.sa.proj.weight, module.blocks.29.ca.mat_q.weight, module.blocks.29.ca.mat_kv.weight, module.blocks.29.ca.proj.weight, '\n"
                   " 'module.blocks.29.ffn.fc1.weight, module.blocks.29.ffn.fc2.weight, module.blocks.29.ca_norm.weight, module.blocks.30.sa.mat_qkv.weight, module.blocks.30.sa.proj.weight, '\n"
                   " 'module.blocks.30.ca.mat_q.weight, module.blocks.30.ca.mat_kv.weight, module.blocks.30.ca.proj.weight, module.blocks.30.ffn.fc1.weight, module.blocks.30.ffn.fc2.weight, '\n"
                   " 'module.blocks.30.ca_norm.weight, module.blocks.31.sa.mat_qkv.weight, module.blocks.31.sa.proj.weight, module.blocks.31.ca.mat_q.weight, module.blocks.31.ca.mat_kv.weight, '\n"
                   " 'module.blocks.31.ca.proj.weight, module.blocks.31.ffn.fc1.weight, module.blocks.31.ffn.fc2.weight, module.blocks.31.ca_norm.weight, module.blocks.32.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.32.sa.proj.weight, module.blocks.32.ca.mat_q.weight, module.blocks.32.ca.mat_kv.weight, module.blocks.32.ca.proj.weight, module.blocks.32.ffn.fc1.weight, '\n"
                   " 'module.blocks.32.ffn.fc2.weight, module.blocks.32.ca_norm.weight, module.blocks.33.sa.mat_qkv.weight, module.blocks.33.sa.proj.weight, module.blocks.33.ca.mat_q.weight, '\n"
                   " 'module.blocks.33.ca.mat_kv.weight, module.blocks.33.ca.proj.weight, module.blocks.33.ffn.fc1.weight, module.blocks.33.ffn.fc2.weight, module.blocks.33.ca_norm.weight, '\n"
                   " 'module.blocks.34.sa.mat_qkv.weight, module.blocks.34.sa.proj.weight, module.blocks.34.ca.mat_q.weight, module.blocks.34.ca.mat_kv.weight, module.blocks.34.ca.proj.weight, '\n"
                   " 'module.blocks.34.ffn.fc1.weight, module.blocks.34.ffn.fc2.weight, module.blocks.34.ca_norm.weight, module.blocks.35.sa.mat_qkv.weight, module.blocks.35.sa.proj.weight, '\n"
                   " 'module.blocks.35.ca.mat_q.weight, module.blocks.35.ca.mat_kv.weight, module.blocks.35.ca.proj.weight, module.blocks.35.ffn.fc1.weight, module.blocks.35.ffn.fc2.weight, '\n"
                   " 'module.blocks.35.ca_norm.weight, module.blocks.36.sa.mat_qkv.weight, module.blocks.36.sa.proj.weight, module.blocks.36.ca.mat_q.weight, module.blocks.36.ca.mat_kv.weight, '\n"
                   " 'module.blocks.36.ca.proj.weight, module.blocks.36.ffn.fc1.weight, module.blocks.36.ffn.fc2.weight, module.blocks.36.ca_norm.weight, module.blocks.37.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.37.sa.proj.weight, module.blocks.37.ca.mat_q.weight, module.blocks.37.ca.mat_kv.weight, module.blocks.37.ca.proj.weight, module.blocks.37.ffn.fc1.weight, '\n"
                   " 'module.blocks.37.ffn.fc2.weight, module.blocks.37.ca_norm.weight, module.blocks.38.sa.mat_qkv.weight, module.blocks.38.sa.proj.weight, module.blocks.38.ca.mat_q.weight, '\n"
                   " 'module.blocks.38.ca.mat_kv.weight, module.blocks.38.ca.proj.weight, module.blocks.38.ffn.fc1.weight, module.blocks.38.ffn.fc2.weight, module.blocks.38.ca_norm.weight, '\n"
                   " 'module.blocks.39.sa.mat_qkv.weight, module.blocks.39.sa.proj.weight, module.blocks.39.ca.mat_q.weight, module.blocks.39.ca.mat_kv.weight, module.blocks.39.ca.proj.weight, '\n"
                   " 'module.blocks.39.ffn.fc1.weight, module.blocks.39.ffn.fc2.weight, module.blocks.39.ca_norm.weight, module.blocks.40.sa.mat_qkv.weight, module.blocks.40.sa.proj.weight, '\n"
                   " 'module.blocks.40.ca.mat_q.weight, module.blocks.40.ca.mat_kv.weight, module.blocks.40.ca.proj.weight, module.blocks.40.ffn.fc1.weight, module.blocks.40.ffn.fc2.weight, '\n"
                   " 'module.blocks.40.ca_norm.weight, module.blocks.41.sa.mat_qkv.weight, module.blocks.41.sa.proj.weight, module.blocks.41.ca.mat_q.weight, module.blocks.41.ca.mat_kv.weight, '\n"
                   " 'module.blocks.41.ca.proj.weight, module.blocks.41.ffn.fc1.weight, module.blocks.41.ffn.fc2.weight, module.blocks.41.ca_norm.weight, module.blocks.42.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.42.sa.proj.weight, module.blocks.42.ca.mat_q.weight, module.blocks.42.ca.mat_kv.weight, module.blocks.42.ca.proj.weight, module.blocks.42.ffn.fc1.weight, '\n"
                   " 'module.blocks.42.ffn.fc2.weight, module.blocks.42.ca_norm.weight, module.blocks.43.sa.mat_qkv.weight, module.blocks.43.sa.proj.weight, module.blocks.43.ca.mat_q.weight, '\n"
                   " 'module.blocks.43.ca.mat_kv.weight, module.blocks.43.ca.proj.weight, module.blocks.43.ffn.fc1.weight, module.blocks.43.ffn.fc2.weight, module.blocks.43.ca_norm.weight, '\n"
                   " 'module.blocks.44.sa.mat_qkv.weight, module.blocks.44.sa.proj.weight, module.blocks.44.ca.mat_q.weight, module.blocks.44.ca.mat_kv.weight, module.blocks.44.ca.proj.weight, '\n"
                   " 'module.blocks.44.ffn.fc1.weight, module.blocks.44.ffn.fc2.weight, module.blocks.44.ca_norm.weight, module.blocks.45.sa.mat_qkv.weight, module.blocks.45.sa.proj.weight, '\n"
                   " 'module.blocks.45.ca.mat_q.weight, module.blocks.45.ca.mat_kv.weight, module.blocks.45.ca.proj.weight, module.blocks.45.ffn.fc1.weight, module.blocks.45.ffn.fc2.weight, '\n"
                   " 'module.blocks.45.ca_norm.weight, module.blocks.46.sa.mat_qkv.weight, module.blocks.46.sa.proj.weight, module.blocks.46.ca.mat_q.weight, module.blocks.46.ca.mat_kv.weight, '\n"
                   " 'module.blocks.46.ca.proj.weight, module.blocks.46.ffn.fc1.weight, module.blocks.46.ffn.fc2.weight, module.blocks.46.ca_norm.weight, module.blocks.47.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.47.sa.proj.weight, module.blocks.47.ca.mat_q.weight, module.blocks.47.ca.mat_kv.weight, module.blocks.47.ca.proj.weight, module.blocks.47.ffn.fc1.weight, '\n"
                   " 'module.blocks.47.ffn.fc2.weight, module.blocks.47.ca_norm.weight, module.blocks.48.sa.mat_qkv.weight, module.blocks.48.sa.proj.weight, module.blocks.48.ca.mat_q.weight, '\n"
                   " 'module.blocks.48.ca.mat_kv.weight, module.blocks.48.ca.proj.weight, module.blocks.48.ffn.fc1.weight, module.blocks.48.ffn.fc2.weight, module.blocks.48.ca_norm.weight, '\n"
                   " 'module.blocks.49.sa.mat_qkv.weight, module.blocks.49.sa.proj.weight, module.blocks.49.ca.mat_q.weight, module.blocks.49.ca.mat_kv.weight, module.blocks.49.ca.proj.weight, '\n"
                   " 'module.blocks.49.ffn.fc1.weight, module.blocks.49.ffn.fc2.weight, module.blocks.49.ca_norm.weight, module.blocks.50.sa.mat_qkv.weight, module.blocks.50.sa.proj.weight, '\n"
                   " 'module.blocks.50.ca.mat_q.weight, module.blocks.50.ca.mat_kv.weight, module.blocks.50.ca.proj.weight, module.blocks.50.ffn.fc1.weight, module.blocks.50.ffn.fc2.weight, '\n"
                   " 'module.blocks.50.ca_norm.weight, module.blocks.51.sa.mat_qkv.weight, module.blocks.51.sa.proj.weight, module.blocks.51.ca.mat_q.weight, module.blocks.51.ca.mat_kv.weight, '\n"
                   " 'module.blocks.51.ca.proj.weight, module.blocks.51.ffn.fc1.weight, module.blocks.51.ffn.fc2.weight, module.blocks.51.ca_norm.weight, module.blocks.52.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.52.sa.proj.weight, module.blocks.52.ca.mat_q.weight, module.blocks.52.ca.mat_kv.weight, module.blocks.52.ca.proj.weight, module.blocks.52.ffn.fc1.weight, '\n"
                   " 'module.blocks.52.ffn.fc2.weight, module.blocks.52.ca_norm.weight, module.blocks.53.sa.mat_qkv.weight, module.blocks.53.sa.proj.weight, module.blocks.53.ca.mat_q.weight, '\n"
                   " 'module.blocks.53.ca.mat_kv.weight, module.blocks.53.ca.proj.weight, module.blocks.53.ffn.fc1.weight, module.blocks.53.ffn.fc2.weight, module.blocks.53.ca_norm.weight, '\n"
                   " 'module.blocks.54.sa.mat_qkv.weight, module.blocks.54.sa.proj.weight, module.blocks.54.ca.mat_q.weight, module.blocks.54.ca.mat_kv.weight, module.blocks.54.ca.proj.weight, '\n"
                   " 'module.blocks.54.ffn.fc1.weight, module.blocks.54.ffn.fc2.weight, module.blocks.54.ca_norm.weight, module.blocks.55.sa.mat_qkv.weight, module.blocks.55.sa.proj.weight, '\n"
                   " 'module.blocks.55.ca.mat_q.weight, module.blocks.55.ca.mat_kv.weight, module.blocks.55.ca.proj.weight, module.blocks.55.ffn.fc1.weight, module.blocks.55.ffn.fc2.weight, '\n"
                   " 'module.blocks.55.ca_norm.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': '[no scale]',
          'params': "('module.cfg_uncond, module.pos_start, module.text_proj_for_sos.ca.mat_q, module.text_proj_for_sos.ca.v_bias, module.text_proj_for_sos.ca.proj.bias, module.text_proj_for_ca.0.bias, '\n"
                    " 'module.text_proj_for_ca.2.bias, module.lvl_embed.weight, module.word_embed.bias, module.shared_ada_lin.1.bias, module.head_nm.ada_lin.1.bias, module.head.bias, module.blocks.0.ada_gss, '\n"
                    " 'module.blocks.0.sa.scale_mul_1H11, module.blocks.0.sa.q_bias, module.blocks.0.sa.v_bias, module.blocks.0.sa.proj.bias, module.blocks.0.ca.v_bias, module.blocks.0.ca.mat_q.bias, '\n"
                    " 'module.blocks.0.ca.proj.bias, module.blocks.0.ffn.fc1.bias, module.blocks.0.ffn.fc2.bias, module.blocks.0.ca_norm.bias, module.blocks.1.ada_gss, module.blocks.1.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.1.sa.q_bias, module.blocks.1.sa.v_bias, module.blocks.1.sa.proj.bias, module.blocks.1.ca.v_bias, module.blocks.1.ca.mat_q.bias, module.blocks.1.ca.proj.bias, '\n"
                    " 'module.blocks.1.ffn.fc1.bias, module.blocks.1.ffn.fc2.bias, module.blocks.1.ca_norm.bias, module.blocks.2.ada_gss, module.blocks.2.sa.scale_mul_1H11, module.blocks.2.sa.q_bias, '\n"
                    " 'module.blocks.2.sa.v_bias, module.blocks.2.sa.proj.bias, module.blocks.2.ca.v_bias, module.blocks.2.ca.mat_q.bias, module.blocks.2.ca.proj.bias, module.blocks.2.ffn.fc1.bias, '\n"
                    " 'module.blocks.2.ffn.fc2.bias, module.blocks.2.ca_norm.bias, module.blocks.3.ada_gss, module.blocks.3.sa.scale_mul_1H11, module.blocks.3.sa.q_bias, module.blocks.3.sa.v_bias, '\n"
                    " 'module.blocks.3.sa.proj.bias, module.blocks.3.ca.v_bias, module.blocks.3.ca.mat_q.bias, module.blocks.3.ca.proj.bias, module.blocks.3.ffn.fc1.bias, module.blocks.3.ffn.fc2.bias, '\n"
                    " 'module.blocks.3.ca_norm.bias, module.blocks.4.ada_gss, module.blocks.4.sa.scale_mul_1H11, module.blocks.4.sa.q_bias, module.blocks.4.sa.v_bias, module.blocks.4.sa.proj.bias, '\n"
                    " 'module.blocks.4.ca.v_bias, module.blocks.4.ca.mat_q.bias, module.blocks.4.ca.proj.bias, module.blocks.4.ffn.fc1.bias, module.blocks.4.ffn.fc2.bias, module.blocks.4.ca_norm.bias, '\n"
                    " 'module.blocks.5.ada_gss, module.blocks.5.sa.scale_mul_1H11, module.blocks.5.sa.q_bias, module.blocks.5.sa.v_bias, module.blocks.5.sa.proj.bias, module.blocks.5.ca.v_bias, '\n"
                    " 'module.blocks.5.ca.mat_q.bias, module.blocks.5.ca.proj.bias, module.blocks.5.ffn.fc1.bias, module.blocks.5.ffn.fc2.bias, module.blocks.5.ca_norm.bias, module.blocks.6.ada_gss, '\n"
                    " 'module.blocks.6.sa.scale_mul_1H11, module.blocks.6.sa.q_bias, module.blocks.6.sa.v_bias, module.blocks.6.sa.proj.bias, module.blocks.6.ca.v_bias, module.blocks.6.ca.mat_q.bias, '\n"
                    " 'module.blocks.6.ca.proj.bias, module.blocks.6.ffn.fc1.bias, module.blocks.6.ffn.fc2.bias, module.blocks.6.ca_norm.bias, module.blocks.7.ada_gss, module.blocks.7.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.7.sa.q_bias, module.blocks.7.sa.v_bias, module.blocks.7.sa.proj.bias, module.blocks.7.ca.v_bias, module.blocks.7.ca.mat_q.bias, module.blocks.7.ca.proj.bias, '\n"
                    " 'module.blocks.7.ffn.fc1.bias, module.blocks.7.ffn.fc2.bias, module.blocks.7.ca_norm.bias, module.blocks.8.ada_gss, module.blocks.8.sa.scale_mul_1H11, module.blocks.8.sa.q_bias, '\n"
                    " 'module.blocks.8.sa.v_bias, module.blocks.8.sa.proj.bias, module.blocks.8.ca.v_bias, module.blocks.8.ca.mat_q.bias, module.blocks.8.ca.proj.bias, module.blocks.8.ffn.fc1.bias, '\n"
                    " 'module.blocks.8.ffn.fc2.bias, module.blocks.8.ca_norm.bias, module.blocks.9.ada_gss, module.blocks.9.sa.scale_mul_1H11, module.blocks.9.sa.q_bias, module.blocks.9.sa.v_bias, '\n"
                    " 'module.blocks.9.sa.proj.bias, module.blocks.9.ca.v_bias, module.blocks.9.ca.mat_q.bias, module.blocks.9.ca.proj.bias, module.blocks.9.ffn.fc1.bias, module.blocks.9.ffn.fc2.bias, '\n"
                    " 'module.blocks.9.ca_norm.bias, module.blocks.10.ada_gss, module.blocks.10.sa.scale_mul_1H11, module.blocks.10.sa.q_bias, module.blocks.10.sa.v_bias, module.blocks.10.sa.proj.bias, '\n"
                    " 'module.blocks.10.ca.v_bias, module.blocks.10.ca.mat_q.bias, module.blocks.10.ca.proj.bias, module.blocks.10.ffn.fc1.bias, module.blocks.10.ffn.fc2.bias, module.blocks.10.ca_norm.bias, '\n"
                    " 'module.blocks.11.ada_gss, module.blocks.11.sa.scale_mul_1H11, module.blocks.11.sa.q_bias, module.blocks.11.sa.v_bias, module.blocks.11.sa.proj.bias, module.blocks.11.ca.v_bias, '\n"
                    " 'module.blocks.11.ca.mat_q.bias, module.blocks.11.ca.proj.bias, module.blocks.11.ffn.fc1.bias, module.blocks.11.ffn.fc2.bias, module.blocks.11.ca_norm.bias, module.blocks.12.ada_gss, '\n"
                    " 'module.blocks.12.sa.scale_mul_1H11, module.blocks.12.sa.q_bias, module.blocks.12.sa.v_bias, module.blocks.12.sa.proj.bias, module.blocks.12.ca.v_bias, module.blocks.12.ca.mat_q.bias, '\n"
                    " 'module.blocks.12.ca.proj.bias, module.blocks.12.ffn.fc1.bias, module.blocks.12.ffn.fc2.bias, module.blocks.12.ca_norm.bias, module.blocks.13.ada_gss, module.blocks.13.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.13.sa.q_bias, module.blocks.13.sa.v_bias, module.blocks.13.sa.proj.bias, module.blocks.13.ca.v_bias, module.blocks.13.ca.mat_q.bias, module.blocks.13.ca.proj.bias, '\n"
                    " 'module.blocks.13.ffn.fc1.bias, module.blocks.13.ffn.fc2.bias, module.blocks.13.ca_norm.bias, module.blocks.14.ada_gss, module.blocks.14.sa.scale_mul_1H11, module.blocks.14.sa.q_bias, '\n"
                    " 'module.blocks.14.sa.v_bias, module.blocks.14.sa.proj.bias, module.blocks.14.ca.v_bias, module.blocks.14.ca.mat_q.bias, module.blocks.14.ca.proj.bias, module.blocks.14.ffn.fc1.bias, '\n"
                    " 'module.blocks.14.ffn.fc2.bias, module.blocks.14.ca_norm.bias, module.blocks.15.ada_gss, module.blocks.15.sa.scale_mul_1H11, module.blocks.15.sa.q_bias, module.blocks.15.sa.v_bias, '\n"
                    " 'module.blocks.15.sa.proj.bias, module.blocks.15.ca.v_bias, module.blocks.15.ca.mat_q.bias, module.blocks.15.ca.proj.bias, module.blocks.15.ffn.fc1.bias, module.blocks.15.ffn.fc2.bias, '\n"
                    " 'module.blocks.15.ca_norm.bias, module.blocks.16.ada_gss, module.blocks.16.sa.scale_mul_1H11, module.blocks.16.sa.q_bias, module.blocks.16.sa.v_bias, module.blocks.16.sa.proj.bias, '\n"
                    " 'module.blocks.16.ca.v_bias, module.blocks.16.ca.mat_q.bias, module.blocks.16.ca.proj.bias, module.blocks.16.ffn.fc1.bias, module.blocks.16.ffn.fc2.bias, module.blocks.16.ca_norm.bias, '\n"
                    " 'module.blocks.17.ada_gss, module.blocks.17.sa.scale_mul_1H11, module.blocks.17.sa.q_bias, module.blocks.17.sa.v_bias, module.blocks.17.sa.proj.bias, module.blocks.17.ca.v_bias, '\n"
                    " 'module.blocks.17.ca.mat_q.bias, module.blocks.17.ca.proj.bias, module.blocks.17.ffn.fc1.bias, module.blocks.17.ffn.fc2.bias, module.blocks.17.ca_norm.bias, module.blocks.18.ada_gss, '\n"
                    " 'module.blocks.18.sa.scale_mul_1H11, module.blocks.18.sa.q_bias, module.blocks.18.sa.v_bias, module.blocks.18.sa.proj.bias, module.blocks.18.ca.v_bias, module.blocks.18.ca.mat_q.bias, '\n"
                    " 'module.blocks.18.ca.proj.bias, module.blocks.18.ffn.fc1.bias, module.blocks.18.ffn.fc2.bias, module.blocks.18.ca_norm.bias, module.blocks.19.ada_gss, module.blocks.19.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.19.sa.q_bias, module.blocks.19.sa.v_bias, module.blocks.19.sa.proj.bias, module.blocks.19.ca.v_bias, module.blocks.19.ca.mat_q.bias, module.blocks.19.ca.proj.bias, '\n"
                    " 'module.blocks.19.ffn.fc1.bias, module.blocks.19.ffn.fc2.bias, module.blocks.19.ca_norm.bias, module.blocks.20.ada_gss, module.blocks.20.sa.scale_mul_1H11, module.blocks.20.sa.q_bias, '\n"
                    " 'module.blocks.20.sa.v_bias, module.blocks.20.sa.proj.bias, module.blocks.20.ca.v_bias, module.blocks.20.ca.mat_q.bias, module.blocks.20.ca.proj.bias, module.blocks.20.ffn.fc1.bias, '\n"
                    " 'module.blocks.20.ffn.fc2.bias, module.blocks.20.ca_norm.bias, module.blocks.21.ada_gss, module.blocks.21.sa.scale_mul_1H11, module.blocks.21.sa.q_bias, module.blocks.21.sa.v_bias, '\n"
                    " 'module.blocks.21.sa.proj.bias, module.blocks.21.ca.v_bias, module.blocks.21.ca.mat_q.bias, module.blocks.21.ca.proj.bias, module.blocks.21.ffn.fc1.bias, module.blocks.21.ffn.fc2.bias, '\n"
                    " 'module.blocks.21.ca_norm.bias, module.blocks.22.ada_gss, module.blocks.22.sa.scale_mul_1H11, module.blocks.22.sa.q_bias, module.blocks.22.sa.v_bias, module.blocks.22.sa.proj.bias, '\n"
                    " 'module.blocks.22.ca.v_bias, module.blocks.22.ca.mat_q.bias, module.blocks.22.ca.proj.bias, module.blocks.22.ffn.fc1.bias, module.blocks.22.ffn.fc2.bias, module.blocks.22.ca_norm.bias, '\n"
                    " 'module.blocks.23.ada_gss, module.blocks.23.sa.scale_mul_1H11, module.blocks.23.sa.q_bias, module.blocks.23.sa.v_bias, module.blocks.23.sa.proj.bias, module.blocks.23.ca.v_bias, '\n"
                    " 'module.blocks.23.ca.mat_q.bias, module.blocks.23.ca.proj.bias, module.blocks.23.ffn.fc1.bias, module.blocks.23.ffn.fc2.bias, module.blocks.23.ca_norm.bias, module.blocks.24.ada_gss, '\n"
                    " 'module.blocks.24.sa.scale_mul_1H11, module.blocks.24.sa.q_bias, module.blocks.24.sa.v_bias, module.blocks.24.sa.proj.bias, module.blocks.24.ca.v_bias, module.blocks.24.ca.mat_q.bias, '\n"
                    " 'module.blocks.24.ca.proj.bias, module.blocks.24.ffn.fc1.bias, module.blocks.24.ffn.fc2.bias, module.blocks.24.ca_norm.bias, module.blocks.25.ada_gss, module.blocks.25.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.25.sa.q_bias, module.blocks.25.sa.v_bias, module.blocks.25.sa.proj.bias, module.blocks.25.ca.v_bias, module.blocks.25.ca.mat_q.bias, module.blocks.25.ca.proj.bias, '\n"
                    " 'module.blocks.25.ffn.fc1.bias, module.blocks.25.ffn.fc2.bias, module.blocks.25.ca_norm.bias, module.blocks.26.ada_gss, module.blocks.26.sa.scale_mul_1H11, module.blocks.26.sa.q_bias, '\n"
                    " 'module.blocks.26.sa.v_bias, module.blocks.26.sa.proj.bias, module.blocks.26.ca.v_bias, module.blocks.26.ca.mat_q.bias, module.blocks.26.ca.proj.bias, module.blocks.26.ffn.fc1.bias, '\n"
                    " 'module.blocks.26.ffn.fc2.bias, module.blocks.26.ca_norm.bias, module.blocks.27.ada_gss, module.blocks.27.sa.scale_mul_1H11, module.blocks.27.sa.q_bias, module.blocks.27.sa.v_bias, '\n"
                    " 'module.blocks.27.sa.proj.bias, module.blocks.27.ca.v_bias, module.blocks.27.ca.mat_q.bias, module.blocks.27.ca.proj.bias, module.blocks.27.ffn.fc1.bias, module.blocks.27.ffn.fc2.bias, '\n"
                    " 'module.blocks.27.ca_norm.bias, module.blocks.28.ada_gss, module.blocks.28.sa.scale_mul_1H11, module.blocks.28.sa.q_bias, module.blocks.28.sa.v_bias, module.blocks.28.sa.proj.bias, '\n"
                    " 'module.blocks.28.ca.v_bias, module.blocks.28.ca.mat_q.bias, module.blocks.28.ca.proj.bias, module.blocks.28.ffn.fc1.bias, module.blocks.28.ffn.fc2.bias, module.blocks.28.ca_norm.bias, '\n"
                    " 'module.blocks.29.ada_gss, module.blocks.29.sa.scale_mul_1H11, module.blocks.29.sa.q_bias, module.blocks.29.sa.v_bias, module.blocks.29.sa.proj.bias, module.blocks.29.ca.v_bias, '\n"
                    " 'module.blocks.29.ca.mat_q.bias, module.blocks.29.ca.proj.bias, module.blocks.29.ffn.fc1.bias, module.blocks.29.ffn.fc2.bias, module.blocks.29.ca_norm.bias, module.blocks.30.ada_gss, '\n"
                    " 'module.blocks.30.sa.scale_mul_1H11, module.blocks.30.sa.q_bias, module.blocks.30.sa.v_bias, module.blocks.30.sa.proj.bias, module.blocks.30.ca.v_bias, module.blocks.30.ca.mat_q.bias, '\n"
                    " 'module.blocks.30.ca.proj.bias, module.blocks.30.ffn.fc1.bias, module.blocks.30.ffn.fc2.bias, module.blocks.30.ca_norm.bias, module.blocks.31.ada_gss, module.blocks.31.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.31.sa.q_bias, module.blocks.31.sa.v_bias, module.blocks.31.sa.proj.bias, module.blocks.31.ca.v_bias, module.blocks.31.ca.mat_q.bias, module.blocks.31.ca.proj.bias, '\n"
                    " 'module.blocks.31.ffn.fc1.bias, module.blocks.31.ffn.fc2.bias, module.blocks.31.ca_norm.bias, module.blocks.32.ada_gss, module.blocks.32.sa.scale_mul_1H11, module.blocks.32.sa.q_bias, '\n"
                    " 'module.blocks.32.sa.v_bias, module.blocks.32.sa.proj.bias, module.blocks.32.ca.v_bias, module.blocks.32.ca.mat_q.bias, module.blocks.32.ca.proj.bias, module.blocks.32.ffn.fc1.bias, '\n"
                    " 'module.blocks.32.ffn.fc2.bias, module.blocks.32.ca_norm.bias, module.blocks.33.ada_gss, module.blocks.33.sa.scale_mul_1H11, module.blocks.33.sa.q_bias, module.blocks.33.sa.v_bias, '\n"
                    " 'module.blocks.33.sa.proj.bias, module.blocks.33.ca.v_bias, module.blocks.33.ca.mat_q.bias, module.blocks.33.ca.proj.bias, module.blocks.33.ffn.fc1.bias, module.blocks.33.ffn.fc2.bias, '\n"
                    " 'module.blocks.33.ca_norm.bias, module.blocks.34.ada_gss, module.blocks.34.sa.scale_mul_1H11, module.blocks.34.sa.q_bias, module.blocks.34.sa.v_bias, module.blocks.34.sa.proj.bias, '\n"
                    " 'module.blocks.34.ca.v_bias, module.blocks.34.ca.mat_q.bias, module.blocks.34.ca.proj.bias, module.blocks.34.ffn.fc1.bias, module.blocks.34.ffn.fc2.bias, module.blocks.34.ca_norm.bias, '\n"
                    " 'module.blocks.35.ada_gss, module.blocks.35.sa.scale_mul_1H11, module.blocks.35.sa.q_bias, module.blocks.35.sa.v_bias, module.blocks.35.sa.proj.bias, module.blocks.35.ca.v_bias, '\n"
                    " 'module.blocks.35.ca.mat_q.bias, module.blocks.35.ca.proj.bias, module.blocks.35.ffn.fc1.bias, module.blocks.35.ffn.fc2.bias, module.blocks.35.ca_norm.bias, module.blocks.36.ada_gss, '\n"
                    " 'module.blocks.36.sa.scale_mul_1H11, module.blocks.36.sa.q_bias, module.blocks.36.sa.v_bias, module.blocks.36.sa.proj.bias, module.blocks.36.ca.v_bias, module.blocks.36.ca.mat_q.bias, '\n"
                    " 'module.blocks.36.ca.proj.bias, module.blocks.36.ffn.fc1.bias, module.blocks.36.ffn.fc2.bias, module.blocks.36.ca_norm.bias, module.blocks.37.ada_gss, module.blocks.37.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.37.sa.q_bias, module.blocks.37.sa.v_bias, module.blocks.37.sa.proj.bias, module.blocks.37.ca.v_bias, module.blocks.37.ca.mat_q.bias, module.blocks.37.ca.proj.bias, '\n"
                    " 'module.blocks.37.ffn.fc1.bias, module.blocks.37.ffn.fc2.bias, module.blocks.37.ca_norm.bias, module.blocks.38.ada_gss, module.blocks.38.sa.scale_mul_1H11, module.blocks.38.sa.q_bias, '\n"
                    " 'module.blocks.38.sa.v_bias, module.blocks.38.sa.proj.bias, module.blocks.38.ca.v_bias, module.blocks.38.ca.mat_q.bias, module.blocks.38.ca.proj.bias, module.blocks.38.ffn.fc1.bias, '\n"
                    " 'module.blocks.38.ffn.fc2.bias, module.blocks.38.ca_norm.bias, module.blocks.39.ada_gss, module.blocks.39.sa.scale_mul_1H11, module.blocks.39.sa.q_bias, module.blocks.39.sa.v_bias, '\n"
                    " 'module.blocks.39.sa.proj.bias, module.blocks.39.ca.v_bias, module.blocks.39.ca.mat_q.bias, module.blocks.39.ca.proj.bias, module.blocks.39.ffn.fc1.bias, module.blocks.39.ffn.fc2.bias, '\n"
                    " 'module.blocks.39.ca_norm.bias, module.blocks.40.ada_gss, module.blocks.40.sa.scale_mul_1H11, module.blocks.40.sa.q_bias, module.blocks.40.sa.v_bias, module.blocks.40.sa.proj.bias, '\n"
                    " 'module.blocks.40.ca.v_bias, module.blocks.40.ca.mat_q.bias, module.blocks.40.ca.proj.bias, module.blocks.40.ffn.fc1.bias, module.blocks.40.ffn.fc2.bias, module.blocks.40.ca_norm.bias, '\n"
                    " 'module.blocks.41.ada_gss, module.blocks.41.sa.scale_mul_1H11, module.blocks.41.sa.q_bias, module.blocks.41.sa.v_bias, module.blocks.41.sa.proj.bias, module.blocks.41.ca.v_bias, '\n"
                    " 'module.blocks.41.ca.mat_q.bias, module.blocks.41.ca.proj.bias, module.blocks.41.ffn.fc1.bias, module.blocks.41.ffn.fc2.bias, module.blocks.41.ca_norm.bias, module.blocks.42.ada_gss, '\n"
                    " 'module.blocks.42.sa.scale_mul_1H11, module.blocks.42.sa.q_bias, module.blocks.42.sa.v_bias, module.blocks.42.sa.proj.bias, module.blocks.42.ca.v_bias, module.blocks.42.ca.mat_q.bias, '\n"
                    " 'module.blocks.42.ca.proj.bias, module.blocks.42.ffn.fc1.bias, module.blocks.42.ffn.fc2.bias, module.blocks.42.ca_norm.bias, module.blocks.43.ada_gss, module.blocks.43.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.43.sa.q_bias, module.blocks.43.sa.v_bias, module.blocks.43.sa.proj.bias, module.blocks.43.ca.v_bias, module.blocks.43.ca.mat_q.bias, module.blocks.43.ca.proj.bias, '\n"
                    " 'module.blocks.43.ffn.fc1.bias, module.blocks.43.ffn.fc2.bias, module.blocks.43.ca_norm.bias, module.blocks.44.ada_gss, module.blocks.44.sa.scale_mul_1H11, module.blocks.44.sa.q_bias, '\n"
                    " 'module.blocks.44.sa.v_bias, module.blocks.44.sa.proj.bias, module.blocks.44.ca.v_bias, module.blocks.44.ca.mat_q.bias, module.blocks.44.ca.proj.bias, module.blocks.44.ffn.fc1.bias, '\n"
                    " 'module.blocks.44.ffn.fc2.bias, module.blocks.44.ca_norm.bias, module.blocks.45.ada_gss, module.blocks.45.sa.scale_mul_1H11, module.blocks.45.sa.q_bias, module.blocks.45.sa.v_bias, '\n"
                    " 'module.blocks.45.sa.proj.bias, module.blocks.45.ca.v_bias, module.blocks.45.ca.mat_q.bias, module.blocks.45.ca.proj.bias, module.blocks.45.ffn.fc1.bias, module.blocks.45.ffn.fc2.bias, '\n"
                    " 'module.blocks.45.ca_norm.bias, module.blocks.46.ada_gss, module.blocks.46.sa.scale_mul_1H11, module.blocks.46.sa.q_bias, module.blocks.46.sa.v_bias, module.blocks.46.sa.proj.bias, '\n"
                    " 'module.blocks.46.ca.v_bias, module.blocks.46.ca.mat_q.bias, module.blocks.46.ca.proj.bias, module.blocks.46.ffn.fc1.bias, module.blocks.46.ffn.fc2.bias, module.blocks.46.ca_norm.bias, '\n"
                    " 'module.blocks.47.ada_gss, module.blocks.47.sa.scale_mul_1H11, module.blocks.47.sa.q_bias, module.blocks.47.sa.v_bias, module.blocks.47.sa.proj.bias, module.blocks.47.ca.v_bias, '\n"
                    " 'module.blocks.47.ca.mat_q.bias, module.blocks.47.ca.proj.bias, module.blocks.47.ffn.fc1.bias, module.blocks.47.ffn.fc2.bias, module.blocks.47.ca_norm.bias, module.blocks.48.ada_gss, '\n"
                    " 'module.blocks.48.sa.scale_mul_1H11, module.blocks.48.sa.q_bias, module.blocks.48.sa.v_bias, module.blocks.48.sa.proj.bias, module.blocks.48.ca.v_bias, module.blocks.48.ca.mat_q.bias, '\n"
                    " 'module.blocks.48.ca.proj.bias, module.blocks.48.ffn.fc1.bias, module.blocks.48.ffn.fc2.bias, module.blocks.48.ca_norm.bias, module.blocks.49.ada_gss, module.blocks.49.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.49.sa.q_bias, module.blocks.49.sa.v_bias, module.blocks.49.sa.proj.bias, module.blocks.49.ca.v_bias, module.blocks.49.ca.mat_q.bias, module.blocks.49.ca.proj.bias, '\n"
                    " 'module.blocks.49.ffn.fc1.bias, module.blocks.49.ffn.fc2.bias, module.blocks.49.ca_norm.bias, module.blocks.50.ada_gss, module.blocks.50.sa.scale_mul_1H11, module.blocks.50.sa.q_bias, '\n"
                    " 'module.blocks.50.sa.v_bias, module.blocks.50.sa.proj.bias, module.blocks.50.ca.v_bias, module.blocks.50.ca.mat_q.bias, module.blocks.50.ca.proj.bias, module.blocks.50.ffn.fc1.bias, '\n"
                    " 'module.blocks.50.ffn.fc2.bias, module.blocks.50.ca_norm.bias, module.blocks.51.ada_gss, module.blocks.51.sa.scale_mul_1H11, module.blocks.51.sa.q_bias, module.blocks.51.sa.v_bias, '\n"
                    " 'module.blocks.51.sa.proj.bias, module.blocks.51.ca.v_bias, module.blocks.51.ca.mat_q.bias, module.blocks.51.ca.proj.bias, module.blocks.51.ffn.fc1.bias, module.blocks.51.ffn.fc2.bias, '\n"
                    " 'module.blocks.51.ca_norm.bias, module.blocks.52.ada_gss, module.blocks.52.sa.scale_mul_1H11, module.blocks.52.sa.q_bias, module.blocks.52.sa.v_bias, module.blocks.52.sa.proj.bias, '\n"
                    " 'module.blocks.52.ca.v_bias, module.blocks.52.ca.mat_q.bias, module.blocks.52.ca.proj.bias, module.blocks.52.ffn.fc1.bias, module.blocks.52.ffn.fc2.bias, module.blocks.52.ca_norm.bias, '\n"
                    " 'module.blocks.53.ada_gss, module.blocks.53.sa.scale_mul_1H11, module.blocks.53.sa.q_bias, module.blocks.53.sa.v_bias, module.blocks.53.sa.proj.bias, module.blocks.53.ca.v_bias, '\n"
                    " 'module.blocks.53.ca.mat_q.bias, module.blocks.53.ca.proj.bias, module.blocks.53.ffn.fc1.bias, module.blocks.53.ffn.fc2.bias, module.blocks.53.ca_norm.bias, module.blocks.54.ada_gss, '\n"
                    " 'module.blocks.54.sa.scale_mul_1H11, module.blocks.54.sa.q_bias, module.blocks.54.sa.v_bias, module.blocks.54.sa.proj.bias, module.blocks.54.ca.v_bias, module.blocks.54.ca.mat_q.bias, '\n"
                    " 'module.blocks.54.ca.proj.bias, module.blocks.54.ffn.fc1.bias, module.blocks.54.ffn.fc2.bias, module.blocks.54.ca_norm.bias, module.blocks.55.ada_gss, module.blocks.55.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.55.sa.q_bias, module.blocks.55.sa.v_bias, module.blocks.55.sa.proj.bias, module.blocks.55.ca.v_bias, module.blocks.55.ca.mat_q.bias, module.blocks.55.ca.proj.bias, '\n"
                    " 'module.blocks.55.ffn.fc1.bias, module.blocks.55.ffn.fc2.bias, module.blocks.55.ca_norm.bias')",
          'wd_sc': 0.0}}

[09-01 23:02:07] (nity/utils/lr_control.py, line 116)=> [get_param_groups][rank0] type(model).__name__='NullDDP' count=1085, numel=8573892016
[09-01 23:02:07] (nity/utils/lr_control.py, line 117)=> 
[09-01 23:02:07] (inity_circuit_breaker.py, line 343)=> [vgpt] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.97), fused=True), opt_kw={'lr': 9.375e-05, 'weight_decay': 0}

[09-01 23:02:07] (inity_circuit_breaker.py, line 348)=> Loading T5 from /home/gs285/VAR/my_model/weights/flan-t5-xl...







=======================================================   RESTART [09-01 23:02:38]   =======================================================
[09-01 23:02:38] (inity_circuit_breaker.py, line 577)=> ============================================================
[09-01 23:02:38] (inity_circuit_breaker.py, line 578)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[09-01 23:02:38] (inity_circuit_breaker.py, line 579)=> Circuit Breaker Alpha: 0.1
[09-01 23:02:38] (inity_circuit_breaker.py, line 580)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[09-01 23:02:38] (inity_circuit_breaker.py, line 581)=> Circuit Breaker Enabled: True
[09-01 23:02:38] (inity_circuit_breaker.py, line 582)=> Number of Examples: 1000
[09-01 23:02:38] (inity_circuit_breaker.py, line 583)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[09-01 23:02:38] (inity_circuit_breaker.py, line 584)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[09-01 23:02:38] (inity_circuit_breaker.py, line 585)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[09-01 23:02:38] (inity_circuit_breaker.py, line 586)=> Validation Ratio: 0.1
[09-01 23:02:38] (inity_circuit_breaker.py, line 587)=> Category: hate
[09-01 23:02:38] (inity_circuit_breaker.py, line 588)=> Batch Size: 4
[09-01 23:02:38] (inity_circuit_breaker.py, line 589)=> Workers: 4
[09-01 23:02:38] (inity_circuit_breaker.py, line 590)=> Device: cuda:0
[09-01 23:02:38] (inity_circuit_breaker.py, line 591)=> Model: infinity_8b
[09-01 23:02:38] (inity_circuit_breaker.py, line 592)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-01 23:02:38] (inity_circuit_breaker.py, line 593)=> Rush Resume: 
[09-01 23:02:38] (inity_circuit_breaker.py, line 594)=> Selective Layers: 0,1,2,3,4,5
[09-01 23:02:38] (inity_circuit_breaker.py, line 595)=> ============================================================
[09-01 23:02:38] (inity_circuit_breaker.py, line 377)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[09-01 23:02:38] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-01 23:02:38] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-01 23:02:38] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[09-01 23:02:38] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-01 23:02:38] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-01 23:02:38] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[09-01 23:02:38] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[09-01 23:02:38] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[09-01 23:02:38] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[09-01 23:02:38] (inity_circuit_breaker.py, line 395)=> args.batch_size=4, vbs=6
[09-01 23:02:38] (inity_circuit_breaker.py, line 418)=> len(dataloader): 100, len(dataset): 100
[09-01 23:02:38] (inity_circuit_breaker.py, line 420)=> total_samples: 200
[09-01 23:02:38] (inity_circuit_breaker.py, line 421)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[09-01 23:02:38] (inity_circuit_breaker.py, line 140)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[09-01 23:02:38] (inity_circuit_breaker.py, line 142)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[09-01 23:02:38] (inity_circuit_breaker.py, line 146)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-01 23:02:41] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[09-01 23:02:41] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[09-01 23:02:41] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[09-01 23:02:41] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[09-01 23:02:43] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[09-01 23:02:45] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[09-01 23:02:45] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[09-01 23:02:45] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[09-01 23:02:45] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[09-01 23:02:45] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

[09-01 23:02:55] (ity/models/init_param.py, line  15)=> [init_weights] Infinity with std=0.02
[09-01 23:02:56] (inity_circuit_breaker.py, line 246)=> [PT] GPT model = Infinity(
  drop_path_rate=0.2
  (norm0_cond): Identity()
  (text_norm): FastRMSNorm(C=2048, eps=1e-06, elementwise_affine=True)
  (text_proj_for_sos): TextAttentivePool(
    (ca): CrossAttention(
      Cq=3072, Ckv=2048, cos_attn=False
      (mat_kv): Linear(in_features=2048, out_features=6144, bias=False)
      (proj): Linear(in_features=3072, out_features=3072, bias=True)
      (proj_drop): Identity()
    )
  )
  (text_proj_for_ca): Sequential(
    (0): Linear(in_features=2048, out_features=3072, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=3072, out_features=3072, bias=True)
  )
  (lvl_embed): Embedding(15, 3072)
  (norm0_ve): Identity()
  (word_embed): Linear(in_features=56, out_features=3072, bias=True)
  (shared_ada_lin): Sequential(
    (0): SiLU()
    (1): SharedAdaLin(in_features=3072, out_features=18432, bias=True)
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=3072, out_features=6144, bias=True)
    )
  )
  (head): Linear(in_features=3072, out_features=112, bias=True)
  (blocks): ModuleList(
    (0): CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): Identity()
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
    (1-55): 55 x CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): DropPath(...)
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
  )
)


[09-01 23:02:56] (inity_circuit_breaker.py, line 248)=> [PT][#para] VAE=81.62, VAE.quant=0.00
[09-01 23:02:56] (inity_circuit_breaker.py, line 251)=> [PT][#para] GPT=8573.89


[09-01 23:02:56] (nity/utils/lr_control.py, line  71)=> [get_param_groups][lr decay] with_lr_scale=False, lr_scale=0.0
[09-01 23:02:56] (nity/utils/lr_control.py, line 111)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': '[no scale]',
         'params': "('module.text_norm.weight, module.text_proj_for_sos.ca.mat_kv.weight, module.text_proj_for_sos.ca.proj.weight, module.text_proj_for_ca.0.weight, module.text_proj_for_ca.2.weight, '\n"
                   " 'module.word_embed.weight, module.shared_ada_lin.1.weight, module.head_nm.ada_lin.1.weight, module.head.weight, module.blocks.0.sa.mat_qkv.weight, module.blocks.0.sa.proj.weight, '\n"
                   " 'module.blocks.0.ca.mat_q.weight, module.blocks.0.ca.mat_kv.weight, module.blocks.0.ca.proj.weight, module.blocks.0.ffn.fc1.weight, module.blocks.0.ffn.fc2.weight, module.blocks.0.ca_norm.weight, '\n"
                   " 'module.blocks.1.sa.mat_qkv.weight, module.blocks.1.sa.proj.weight, module.blocks.1.ca.mat_q.weight, module.blocks.1.ca.mat_kv.weight, module.blocks.1.ca.proj.weight, '\n"
                   " 'module.blocks.1.ffn.fc1.weight, module.blocks.1.ffn.fc2.weight, module.blocks.1.ca_norm.weight, module.blocks.2.sa.mat_qkv.weight, module.blocks.2.sa.proj.weight, module.blocks.2.ca.mat_q.weight, '\n"
                   " 'module.blocks.2.ca.mat_kv.weight, module.blocks.2.ca.proj.weight, module.blocks.2.ffn.fc1.weight, module.blocks.2.ffn.fc2.weight, module.blocks.2.ca_norm.weight, module.blocks.3.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.3.sa.proj.weight, module.blocks.3.ca.mat_q.weight, module.blocks.3.ca.mat_kv.weight, module.blocks.3.ca.proj.weight, module.blocks.3.ffn.fc1.weight, module.blocks.3.ffn.fc2.weight, '\n"
                   " 'module.blocks.3.ca_norm.weight, module.blocks.4.sa.mat_qkv.weight, module.blocks.4.sa.proj.weight, module.blocks.4.ca.mat_q.weight, module.blocks.4.ca.mat_kv.weight, '\n"
                   " 'module.blocks.4.ca.proj.weight, module.blocks.4.ffn.fc1.weight, module.blocks.4.ffn.fc2.weight, module.blocks.4.ca_norm.weight, module.blocks.5.sa.mat_qkv.weight, module.blocks.5.sa.proj.weight, '\n"
                   " 'module.blocks.5.ca.mat_q.weight, module.blocks.5.ca.mat_kv.weight, module.blocks.5.ca.proj.weight, module.blocks.5.ffn.fc1.weight, module.blocks.5.ffn.fc2.weight, module.blocks.5.ca_norm.weight, '\n"
                   " 'module.blocks.6.sa.mat_qkv.weight, module.blocks.6.sa.proj.weight, module.blocks.6.ca.mat_q.weight, module.blocks.6.ca.mat_kv.weight, module.blocks.6.ca.proj.weight, '\n"
                   " 'module.blocks.6.ffn.fc1.weight, module.blocks.6.ffn.fc2.weight, module.blocks.6.ca_norm.weight, module.blocks.7.sa.mat_qkv.weight, module.blocks.7.sa.proj.weight, module.blocks.7.ca.mat_q.weight, '\n"
                   " 'module.blocks.7.ca.mat_kv.weight, module.blocks.7.ca.proj.weight, module.blocks.7.ffn.fc1.weight, module.blocks.7.ffn.fc2.weight, module.blocks.7.ca_norm.weight, module.blocks.8.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.8.sa.proj.weight, module.blocks.8.ca.mat_q.weight, module.blocks.8.ca.mat_kv.weight, module.blocks.8.ca.proj.weight, module.blocks.8.ffn.fc1.weight, module.blocks.8.ffn.fc2.weight, '\n"
                   " 'module.blocks.8.ca_norm.weight, module.blocks.9.sa.mat_qkv.weight, module.blocks.9.sa.proj.weight, module.blocks.9.ca.mat_q.weight, module.blocks.9.ca.mat_kv.weight, '\n"
                   " 'module.blocks.9.ca.proj.weight, module.blocks.9.ffn.fc1.weight, module.blocks.9.ffn.fc2.weight, module.blocks.9.ca_norm.weight, module.blocks.10.sa.mat_qkv.weight, module.blocks.10.sa.proj.weight, '\n"
                   " 'module.blocks.10.ca.mat_q.weight, module.blocks.10.ca.mat_kv.weight, module.blocks.10.ca.proj.weight, module.blocks.10.ffn.fc1.weight, module.blocks.10.ffn.fc2.weight, '\n"
                   " 'module.blocks.10.ca_norm.weight, module.blocks.11.sa.mat_qkv.weight, module.blocks.11.sa.proj.weight, module.blocks.11.ca.mat_q.weight, module.blocks.11.ca.mat_kv.weight, '\n"
                   " 'module.blocks.11.ca.proj.weight, module.blocks.11.ffn.fc1.weight, module.blocks.11.ffn.fc2.weight, module.blocks.11.ca_norm.weight, module.blocks.12.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.12.sa.proj.weight, module.blocks.12.ca.mat_q.weight, module.blocks.12.ca.mat_kv.weight, module.blocks.12.ca.proj.weight, module.blocks.12.ffn.fc1.weight, '\n"
                   " 'module.blocks.12.ffn.fc2.weight, module.blocks.12.ca_norm.weight, module.blocks.13.sa.mat_qkv.weight, module.blocks.13.sa.proj.weight, module.blocks.13.ca.mat_q.weight, '\n"
                   " 'module.blocks.13.ca.mat_kv.weight, module.blocks.13.ca.proj.weight, module.blocks.13.ffn.fc1.weight, module.blocks.13.ffn.fc2.weight, module.blocks.13.ca_norm.weight, '\n"
                   " 'module.blocks.14.sa.mat_qkv.weight, module.blocks.14.sa.proj.weight, module.blocks.14.ca.mat_q.weight, module.blocks.14.ca.mat_kv.weight, module.blocks.14.ca.proj.weight, '\n"
                   " 'module.blocks.14.ffn.fc1.weight, module.blocks.14.ffn.fc2.weight, module.blocks.14.ca_norm.weight, module.blocks.15.sa.mat_qkv.weight, module.blocks.15.sa.proj.weight, '\n"
                   " 'module.blocks.15.ca.mat_q.weight, module.blocks.15.ca.mat_kv.weight, module.blocks.15.ca.proj.weight, module.blocks.15.ffn.fc1.weight, module.blocks.15.ffn.fc2.weight, '\n"
                   " 'module.blocks.15.ca_norm.weight, module.blocks.16.sa.mat_qkv.weight, module.blocks.16.sa.proj.weight, module.blocks.16.ca.mat_q.weight, module.blocks.16.ca.mat_kv.weight, '\n"
                   " 'module.blocks.16.ca.proj.weight, module.blocks.16.ffn.fc1.weight, module.blocks.16.ffn.fc2.weight, module.blocks.16.ca_norm.weight, module.blocks.17.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.17.sa.proj.weight, module.blocks.17.ca.mat_q.weight, module.blocks.17.ca.mat_kv.weight, module.blocks.17.ca.proj.weight, module.blocks.17.ffn.fc1.weight, '\n"
                   " 'module.blocks.17.ffn.fc2.weight, module.blocks.17.ca_norm.weight, module.blocks.18.sa.mat_qkv.weight, module.blocks.18.sa.proj.weight, module.blocks.18.ca.mat_q.weight, '\n"
                   " 'module.blocks.18.ca.mat_kv.weight, module.blocks.18.ca.proj.weight, module.blocks.18.ffn.fc1.weight, module.blocks.18.ffn.fc2.weight, module.blocks.18.ca_norm.weight, '\n"
                   " 'module.blocks.19.sa.mat_qkv.weight, module.blocks.19.sa.proj.weight, module.blocks.19.ca.mat_q.weight, module.blocks.19.ca.mat_kv.weight, module.blocks.19.ca.proj.weight, '\n"
                   " 'module.blocks.19.ffn.fc1.weight, module.blocks.19.ffn.fc2.weight, module.blocks.19.ca_norm.weight, module.blocks.20.sa.mat_qkv.weight, module.blocks.20.sa.proj.weight, '\n"
                   " 'module.blocks.20.ca.mat_q.weight, module.blocks.20.ca.mat_kv.weight, module.blocks.20.ca.proj.weight, module.blocks.20.ffn.fc1.weight, module.blocks.20.ffn.fc2.weight, '\n"
                   " 'module.blocks.20.ca_norm.weight, module.blocks.21.sa.mat_qkv.weight, module.blocks.21.sa.proj.weight, module.blocks.21.ca.mat_q.weight, module.blocks.21.ca.mat_kv.weight, '\n"
                   " 'module.blocks.21.ca.proj.weight, module.blocks.21.ffn.fc1.weight, module.blocks.21.ffn.fc2.weight, module.blocks.21.ca_norm.weight, module.blocks.22.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.22.sa.proj.weight, module.blocks.22.ca.mat_q.weight, module.blocks.22.ca.mat_kv.weight, module.blocks.22.ca.proj.weight, module.blocks.22.ffn.fc1.weight, '\n"
                   " 'module.blocks.22.ffn.fc2.weight, module.blocks.22.ca_norm.weight, module.blocks.23.sa.mat_qkv.weight, module.blocks.23.sa.proj.weight, module.blocks.23.ca.mat_q.weight, '\n"
                   " 'module.blocks.23.ca.mat_kv.weight, module.blocks.23.ca.proj.weight, module.blocks.23.ffn.fc1.weight, module.blocks.23.ffn.fc2.weight, module.blocks.23.ca_norm.weight, '\n"
                   " 'module.blocks.24.sa.mat_qkv.weight, module.blocks.24.sa.proj.weight, module.blocks.24.ca.mat_q.weight, module.blocks.24.ca.mat_kv.weight, module.blocks.24.ca.proj.weight, '\n"
                   " 'module.blocks.24.ffn.fc1.weight, module.blocks.24.ffn.fc2.weight, module.blocks.24.ca_norm.weight, module.blocks.25.sa.mat_qkv.weight, module.blocks.25.sa.proj.weight, '\n"
                   " 'module.blocks.25.ca.mat_q.weight, module.blocks.25.ca.mat_kv.weight, module.blocks.25.ca.proj.weight, module.blocks.25.ffn.fc1.weight, module.blocks.25.ffn.fc2.weight, '\n"
                   " 'module.blocks.25.ca_norm.weight, module.blocks.26.sa.mat_qkv.weight, module.blocks.26.sa.proj.weight, module.blocks.26.ca.mat_q.weight, module.blocks.26.ca.mat_kv.weight, '\n"
                   " 'module.blocks.26.ca.proj.weight, module.blocks.26.ffn.fc1.weight, module.blocks.26.ffn.fc2.weight, module.blocks.26.ca_norm.weight, module.blocks.27.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.27.sa.proj.weight, module.blocks.27.ca.mat_q.weight, module.blocks.27.ca.mat_kv.weight, module.blocks.27.ca.proj.weight, module.blocks.27.ffn.fc1.weight, '\n"
                   " 'module.blocks.27.ffn.fc2.weight, module.blocks.27.ca_norm.weight, module.blocks.28.sa.mat_qkv.weight, module.blocks.28.sa.proj.weight, module.blocks.28.ca.mat_q.weight, '\n"
                   " 'module.blocks.28.ca.mat_kv.weight, module.blocks.28.ca.proj.weight, module.blocks.28.ffn.fc1.weight, module.blocks.28.ffn.fc2.weight, module.blocks.28.ca_norm.weight, '\n"
                   " 'module.blocks.29.sa.mat_qkv.weight, module.blocks.29.sa.proj.weight, module.blocks.29.ca.mat_q.weight, module.blocks.29.ca.mat_kv.weight, module.blocks.29.ca.proj.weight, '\n"
                   " 'module.blocks.29.ffn.fc1.weight, module.blocks.29.ffn.fc2.weight, module.blocks.29.ca_norm.weight, module.blocks.30.sa.mat_qkv.weight, module.blocks.30.sa.proj.weight, '\n"
                   " 'module.blocks.30.ca.mat_q.weight, module.blocks.30.ca.mat_kv.weight, module.blocks.30.ca.proj.weight, module.blocks.30.ffn.fc1.weight, module.blocks.30.ffn.fc2.weight, '\n"
                   " 'module.blocks.30.ca_norm.weight, module.blocks.31.sa.mat_qkv.weight, module.blocks.31.sa.proj.weight, module.blocks.31.ca.mat_q.weight, module.blocks.31.ca.mat_kv.weight, '\n"
                   " 'module.blocks.31.ca.proj.weight, module.blocks.31.ffn.fc1.weight, module.blocks.31.ffn.fc2.weight, module.blocks.31.ca_norm.weight, module.blocks.32.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.32.sa.proj.weight, module.blocks.32.ca.mat_q.weight, module.blocks.32.ca.mat_kv.weight, module.blocks.32.ca.proj.weight, module.blocks.32.ffn.fc1.weight, '\n"
                   " 'module.blocks.32.ffn.fc2.weight, module.blocks.32.ca_norm.weight, module.blocks.33.sa.mat_qkv.weight, module.blocks.33.sa.proj.weight, module.blocks.33.ca.mat_q.weight, '\n"
                   " 'module.blocks.33.ca.mat_kv.weight, module.blocks.33.ca.proj.weight, module.blocks.33.ffn.fc1.weight, module.blocks.33.ffn.fc2.weight, module.blocks.33.ca_norm.weight, '\n"
                   " 'module.blocks.34.sa.mat_qkv.weight, module.blocks.34.sa.proj.weight, module.blocks.34.ca.mat_q.weight, module.blocks.34.ca.mat_kv.weight, module.blocks.34.ca.proj.weight, '\n"
                   " 'module.blocks.34.ffn.fc1.weight, module.blocks.34.ffn.fc2.weight, module.blocks.34.ca_norm.weight, module.blocks.35.sa.mat_qkv.weight, module.blocks.35.sa.proj.weight, '\n"
                   " 'module.blocks.35.ca.mat_q.weight, module.blocks.35.ca.mat_kv.weight, module.blocks.35.ca.proj.weight, module.blocks.35.ffn.fc1.weight, module.blocks.35.ffn.fc2.weight, '\n"
                   " 'module.blocks.35.ca_norm.weight, module.blocks.36.sa.mat_qkv.weight, module.blocks.36.sa.proj.weight, module.blocks.36.ca.mat_q.weight, module.blocks.36.ca.mat_kv.weight, '\n"
                   " 'module.blocks.36.ca.proj.weight, module.blocks.36.ffn.fc1.weight, module.blocks.36.ffn.fc2.weight, module.blocks.36.ca_norm.weight, module.blocks.37.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.37.sa.proj.weight, module.blocks.37.ca.mat_q.weight, module.blocks.37.ca.mat_kv.weight, module.blocks.37.ca.proj.weight, module.blocks.37.ffn.fc1.weight, '\n"
                   " 'module.blocks.37.ffn.fc2.weight, module.blocks.37.ca_norm.weight, module.blocks.38.sa.mat_qkv.weight, module.blocks.38.sa.proj.weight, module.blocks.38.ca.mat_q.weight, '\n"
                   " 'module.blocks.38.ca.mat_kv.weight, module.blocks.38.ca.proj.weight, module.blocks.38.ffn.fc1.weight, module.blocks.38.ffn.fc2.weight, module.blocks.38.ca_norm.weight, '\n"
                   " 'module.blocks.39.sa.mat_qkv.weight, module.blocks.39.sa.proj.weight, module.blocks.39.ca.mat_q.weight, module.blocks.39.ca.mat_kv.weight, module.blocks.39.ca.proj.weight, '\n"
                   " 'module.blocks.39.ffn.fc1.weight, module.blocks.39.ffn.fc2.weight, module.blocks.39.ca_norm.weight, module.blocks.40.sa.mat_qkv.weight, module.blocks.40.sa.proj.weight, '\n"
                   " 'module.blocks.40.ca.mat_q.weight, module.blocks.40.ca.mat_kv.weight, module.blocks.40.ca.proj.weight, module.blocks.40.ffn.fc1.weight, module.blocks.40.ffn.fc2.weight, '\n"
                   " 'module.blocks.40.ca_norm.weight, module.blocks.41.sa.mat_qkv.weight, module.blocks.41.sa.proj.weight, module.blocks.41.ca.mat_q.weight, module.blocks.41.ca.mat_kv.weight, '\n"
                   " 'module.blocks.41.ca.proj.weight, module.blocks.41.ffn.fc1.weight, module.blocks.41.ffn.fc2.weight, module.blocks.41.ca_norm.weight, module.blocks.42.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.42.sa.proj.weight, module.blocks.42.ca.mat_q.weight, module.blocks.42.ca.mat_kv.weight, module.blocks.42.ca.proj.weight, module.blocks.42.ffn.fc1.weight, '\n"
                   " 'module.blocks.42.ffn.fc2.weight, module.blocks.42.ca_norm.weight, module.blocks.43.sa.mat_qkv.weight, module.blocks.43.sa.proj.weight, module.blocks.43.ca.mat_q.weight, '\n"
                   " 'module.blocks.43.ca.mat_kv.weight, module.blocks.43.ca.proj.weight, module.blocks.43.ffn.fc1.weight, module.blocks.43.ffn.fc2.weight, module.blocks.43.ca_norm.weight, '\n"
                   " 'module.blocks.44.sa.mat_qkv.weight, module.blocks.44.sa.proj.weight, module.blocks.44.ca.mat_q.weight, module.blocks.44.ca.mat_kv.weight, module.blocks.44.ca.proj.weight, '\n"
                   " 'module.blocks.44.ffn.fc1.weight, module.blocks.44.ffn.fc2.weight, module.blocks.44.ca_norm.weight, module.blocks.45.sa.mat_qkv.weight, module.blocks.45.sa.proj.weight, '\n"
                   " 'module.blocks.45.ca.mat_q.weight, module.blocks.45.ca.mat_kv.weight, module.blocks.45.ca.proj.weight, module.blocks.45.ffn.fc1.weight, module.blocks.45.ffn.fc2.weight, '\n"
                   " 'module.blocks.45.ca_norm.weight, module.blocks.46.sa.mat_qkv.weight, module.blocks.46.sa.proj.weight, module.blocks.46.ca.mat_q.weight, module.blocks.46.ca.mat_kv.weight, '\n"
                   " 'module.blocks.46.ca.proj.weight, module.blocks.46.ffn.fc1.weight, module.blocks.46.ffn.fc2.weight, module.blocks.46.ca_norm.weight, module.blocks.47.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.47.sa.proj.weight, module.blocks.47.ca.mat_q.weight, module.blocks.47.ca.mat_kv.weight, module.blocks.47.ca.proj.weight, module.blocks.47.ffn.fc1.weight, '\n"
                   " 'module.blocks.47.ffn.fc2.weight, module.blocks.47.ca_norm.weight, module.blocks.48.sa.mat_qkv.weight, module.blocks.48.sa.proj.weight, module.blocks.48.ca.mat_q.weight, '\n"
                   " 'module.blocks.48.ca.mat_kv.weight, module.blocks.48.ca.proj.weight, module.blocks.48.ffn.fc1.weight, module.blocks.48.ffn.fc2.weight, module.blocks.48.ca_norm.weight, '\n"
                   " 'module.blocks.49.sa.mat_qkv.weight, module.blocks.49.sa.proj.weight, module.blocks.49.ca.mat_q.weight, module.blocks.49.ca.mat_kv.weight, module.blocks.49.ca.proj.weight, '\n"
                   " 'module.blocks.49.ffn.fc1.weight, module.blocks.49.ffn.fc2.weight, module.blocks.49.ca_norm.weight, module.blocks.50.sa.mat_qkv.weight, module.blocks.50.sa.proj.weight, '\n"
                   " 'module.blocks.50.ca.mat_q.weight, module.blocks.50.ca.mat_kv.weight, module.blocks.50.ca.proj.weight, module.blocks.50.ffn.fc1.weight, module.blocks.50.ffn.fc2.weight, '\n"
                   " 'module.blocks.50.ca_norm.weight, module.blocks.51.sa.mat_qkv.weight, module.blocks.51.sa.proj.weight, module.blocks.51.ca.mat_q.weight, module.blocks.51.ca.mat_kv.weight, '\n"
                   " 'module.blocks.51.ca.proj.weight, module.blocks.51.ffn.fc1.weight, module.blocks.51.ffn.fc2.weight, module.blocks.51.ca_norm.weight, module.blocks.52.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.52.sa.proj.weight, module.blocks.52.ca.mat_q.weight, module.blocks.52.ca.mat_kv.weight, module.blocks.52.ca.proj.weight, module.blocks.52.ffn.fc1.weight, '\n"
                   " 'module.blocks.52.ffn.fc2.weight, module.blocks.52.ca_norm.weight, module.blocks.53.sa.mat_qkv.weight, module.blocks.53.sa.proj.weight, module.blocks.53.ca.mat_q.weight, '\n"
                   " 'module.blocks.53.ca.mat_kv.weight, module.blocks.53.ca.proj.weight, module.blocks.53.ffn.fc1.weight, module.blocks.53.ffn.fc2.weight, module.blocks.53.ca_norm.weight, '\n"
                   " 'module.blocks.54.sa.mat_qkv.weight, module.blocks.54.sa.proj.weight, module.blocks.54.ca.mat_q.weight, module.blocks.54.ca.mat_kv.weight, module.blocks.54.ca.proj.weight, '\n"
                   " 'module.blocks.54.ffn.fc1.weight, module.blocks.54.ffn.fc2.weight, module.blocks.54.ca_norm.weight, module.blocks.55.sa.mat_qkv.weight, module.blocks.55.sa.proj.weight, '\n"
                   " 'module.blocks.55.ca.mat_q.weight, module.blocks.55.ca.mat_kv.weight, module.blocks.55.ca.proj.weight, module.blocks.55.ffn.fc1.weight, module.blocks.55.ffn.fc2.weight, '\n"
                   " 'module.blocks.55.ca_norm.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': '[no scale]',
          'params': "('module.cfg_uncond, module.pos_start, module.text_proj_for_sos.ca.mat_q, module.text_proj_for_sos.ca.v_bias, module.text_proj_for_sos.ca.proj.bias, module.text_proj_for_ca.0.bias, '\n"
                    " 'module.text_proj_for_ca.2.bias, module.lvl_embed.weight, module.word_embed.bias, module.shared_ada_lin.1.bias, module.head_nm.ada_lin.1.bias, module.head.bias, module.blocks.0.ada_gss, '\n"
                    " 'module.blocks.0.sa.scale_mul_1H11, module.blocks.0.sa.q_bias, module.blocks.0.sa.v_bias, module.blocks.0.sa.proj.bias, module.blocks.0.ca.v_bias, module.blocks.0.ca.mat_q.bias, '\n"
                    " 'module.blocks.0.ca.proj.bias, module.blocks.0.ffn.fc1.bias, module.blocks.0.ffn.fc2.bias, module.blocks.0.ca_norm.bias, module.blocks.1.ada_gss, module.blocks.1.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.1.sa.q_bias, module.blocks.1.sa.v_bias, module.blocks.1.sa.proj.bias, module.blocks.1.ca.v_bias, module.blocks.1.ca.mat_q.bias, module.blocks.1.ca.proj.bias, '\n"
                    " 'module.blocks.1.ffn.fc1.bias, module.blocks.1.ffn.fc2.bias, module.blocks.1.ca_norm.bias, module.blocks.2.ada_gss, module.blocks.2.sa.scale_mul_1H11, module.blocks.2.sa.q_bias, '\n"
                    " 'module.blocks.2.sa.v_bias, module.blocks.2.sa.proj.bias, module.blocks.2.ca.v_bias, module.blocks.2.ca.mat_q.bias, module.blocks.2.ca.proj.bias, module.blocks.2.ffn.fc1.bias, '\n"
                    " 'module.blocks.2.ffn.fc2.bias, module.blocks.2.ca_norm.bias, module.blocks.3.ada_gss, module.blocks.3.sa.scale_mul_1H11, module.blocks.3.sa.q_bias, module.blocks.3.sa.v_bias, '\n"
                    " 'module.blocks.3.sa.proj.bias, module.blocks.3.ca.v_bias, module.blocks.3.ca.mat_q.bias, module.blocks.3.ca.proj.bias, module.blocks.3.ffn.fc1.bias, module.blocks.3.ffn.fc2.bias, '\n"
                    " 'module.blocks.3.ca_norm.bias, module.blocks.4.ada_gss, module.blocks.4.sa.scale_mul_1H11, module.blocks.4.sa.q_bias, module.blocks.4.sa.v_bias, module.blocks.4.sa.proj.bias, '\n"
                    " 'module.blocks.4.ca.v_bias, module.blocks.4.ca.mat_q.bias, module.blocks.4.ca.proj.bias, module.blocks.4.ffn.fc1.bias, module.blocks.4.ffn.fc2.bias, module.blocks.4.ca_norm.bias, '\n"
                    " 'module.blocks.5.ada_gss, module.blocks.5.sa.scale_mul_1H11, module.blocks.5.sa.q_bias, module.blocks.5.sa.v_bias, module.blocks.5.sa.proj.bias, module.blocks.5.ca.v_bias, '\n"
                    " 'module.blocks.5.ca.mat_q.bias, module.blocks.5.ca.proj.bias, module.blocks.5.ffn.fc1.bias, module.blocks.5.ffn.fc2.bias, module.blocks.5.ca_norm.bias, module.blocks.6.ada_gss, '\n"
                    " 'module.blocks.6.sa.scale_mul_1H11, module.blocks.6.sa.q_bias, module.blocks.6.sa.v_bias, module.blocks.6.sa.proj.bias, module.blocks.6.ca.v_bias, module.blocks.6.ca.mat_q.bias, '\n"
                    " 'module.blocks.6.ca.proj.bias, module.blocks.6.ffn.fc1.bias, module.blocks.6.ffn.fc2.bias, module.blocks.6.ca_norm.bias, module.blocks.7.ada_gss, module.blocks.7.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.7.sa.q_bias, module.blocks.7.sa.v_bias, module.blocks.7.sa.proj.bias, module.blocks.7.ca.v_bias, module.blocks.7.ca.mat_q.bias, module.blocks.7.ca.proj.bias, '\n"
                    " 'module.blocks.7.ffn.fc1.bias, module.blocks.7.ffn.fc2.bias, module.blocks.7.ca_norm.bias, module.blocks.8.ada_gss, module.blocks.8.sa.scale_mul_1H11, module.blocks.8.sa.q_bias, '\n"
                    " 'module.blocks.8.sa.v_bias, module.blocks.8.sa.proj.bias, module.blocks.8.ca.v_bias, module.blocks.8.ca.mat_q.bias, module.blocks.8.ca.proj.bias, module.blocks.8.ffn.fc1.bias, '\n"
                    " 'module.blocks.8.ffn.fc2.bias, module.blocks.8.ca_norm.bias, module.blocks.9.ada_gss, module.blocks.9.sa.scale_mul_1H11, module.blocks.9.sa.q_bias, module.blocks.9.sa.v_bias, '\n"
                    " 'module.blocks.9.sa.proj.bias, module.blocks.9.ca.v_bias, module.blocks.9.ca.mat_q.bias, module.blocks.9.ca.proj.bias, module.blocks.9.ffn.fc1.bias, module.blocks.9.ffn.fc2.bias, '\n"
                    " 'module.blocks.9.ca_norm.bias, module.blocks.10.ada_gss, module.blocks.10.sa.scale_mul_1H11, module.blocks.10.sa.q_bias, module.blocks.10.sa.v_bias, module.blocks.10.sa.proj.bias, '\n"
                    " 'module.blocks.10.ca.v_bias, module.blocks.10.ca.mat_q.bias, module.blocks.10.ca.proj.bias, module.blocks.10.ffn.fc1.bias, module.blocks.10.ffn.fc2.bias, module.blocks.10.ca_norm.bias, '\n"
                    " 'module.blocks.11.ada_gss, module.blocks.11.sa.scale_mul_1H11, module.blocks.11.sa.q_bias, module.blocks.11.sa.v_bias, module.blocks.11.sa.proj.bias, module.blocks.11.ca.v_bias, '\n"
                    " 'module.blocks.11.ca.mat_q.bias, module.blocks.11.ca.proj.bias, module.blocks.11.ffn.fc1.bias, module.blocks.11.ffn.fc2.bias, module.blocks.11.ca_norm.bias, module.blocks.12.ada_gss, '\n"
                    " 'module.blocks.12.sa.scale_mul_1H11, module.blocks.12.sa.q_bias, module.blocks.12.sa.v_bias, module.blocks.12.sa.proj.bias, module.blocks.12.ca.v_bias, module.blocks.12.ca.mat_q.bias, '\n"
                    " 'module.blocks.12.ca.proj.bias, module.blocks.12.ffn.fc1.bias, module.blocks.12.ffn.fc2.bias, module.blocks.12.ca_norm.bias, module.blocks.13.ada_gss, module.blocks.13.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.13.sa.q_bias, module.blocks.13.sa.v_bias, module.blocks.13.sa.proj.bias, module.blocks.13.ca.v_bias, module.blocks.13.ca.mat_q.bias, module.blocks.13.ca.proj.bias, '\n"
                    " 'module.blocks.13.ffn.fc1.bias, module.blocks.13.ffn.fc2.bias, module.blocks.13.ca_norm.bias, module.blocks.14.ada_gss, module.blocks.14.sa.scale_mul_1H11, module.blocks.14.sa.q_bias, '\n"
                    " 'module.blocks.14.sa.v_bias, module.blocks.14.sa.proj.bias, module.blocks.14.ca.v_bias, module.blocks.14.ca.mat_q.bias, module.blocks.14.ca.proj.bias, module.blocks.14.ffn.fc1.bias, '\n"
                    " 'module.blocks.14.ffn.fc2.bias, module.blocks.14.ca_norm.bias, module.blocks.15.ada_gss, module.blocks.15.sa.scale_mul_1H11, module.blocks.15.sa.q_bias, module.blocks.15.sa.v_bias, '\n"
                    " 'module.blocks.15.sa.proj.bias, module.blocks.15.ca.v_bias, module.blocks.15.ca.mat_q.bias, module.blocks.15.ca.proj.bias, module.blocks.15.ffn.fc1.bias, module.blocks.15.ffn.fc2.bias, '\n"
                    " 'module.blocks.15.ca_norm.bias, module.blocks.16.ada_gss, module.blocks.16.sa.scale_mul_1H11, module.blocks.16.sa.q_bias, module.blocks.16.sa.v_bias, module.blocks.16.sa.proj.bias, '\n"
                    " 'module.blocks.16.ca.v_bias, module.blocks.16.ca.mat_q.bias, module.blocks.16.ca.proj.bias, module.blocks.16.ffn.fc1.bias, module.blocks.16.ffn.fc2.bias, module.blocks.16.ca_norm.bias, '\n"
                    " 'module.blocks.17.ada_gss, module.blocks.17.sa.scale_mul_1H11, module.blocks.17.sa.q_bias, module.blocks.17.sa.v_bias, module.blocks.17.sa.proj.bias, module.blocks.17.ca.v_bias, '\n"
                    " 'module.blocks.17.ca.mat_q.bias, module.blocks.17.ca.proj.bias, module.blocks.17.ffn.fc1.bias, module.blocks.17.ffn.fc2.bias, module.blocks.17.ca_norm.bias, module.blocks.18.ada_gss, '\n"
                    " 'module.blocks.18.sa.scale_mul_1H11, module.blocks.18.sa.q_bias, module.blocks.18.sa.v_bias, module.blocks.18.sa.proj.bias, module.blocks.18.ca.v_bias, module.blocks.18.ca.mat_q.bias, '\n"
                    " 'module.blocks.18.ca.proj.bias, module.blocks.18.ffn.fc1.bias, module.blocks.18.ffn.fc2.bias, module.blocks.18.ca_norm.bias, module.blocks.19.ada_gss, module.blocks.19.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.19.sa.q_bias, module.blocks.19.sa.v_bias, module.blocks.19.sa.proj.bias, module.blocks.19.ca.v_bias, module.blocks.19.ca.mat_q.bias, module.blocks.19.ca.proj.bias, '\n"
                    " 'module.blocks.19.ffn.fc1.bias, module.blocks.19.ffn.fc2.bias, module.blocks.19.ca_norm.bias, module.blocks.20.ada_gss, module.blocks.20.sa.scale_mul_1H11, module.blocks.20.sa.q_bias, '\n"
                    " 'module.blocks.20.sa.v_bias, module.blocks.20.sa.proj.bias, module.blocks.20.ca.v_bias, module.blocks.20.ca.mat_q.bias, module.blocks.20.ca.proj.bias, module.blocks.20.ffn.fc1.bias, '\n"
                    " 'module.blocks.20.ffn.fc2.bias, module.blocks.20.ca_norm.bias, module.blocks.21.ada_gss, module.blocks.21.sa.scale_mul_1H11, module.blocks.21.sa.q_bias, module.blocks.21.sa.v_bias, '\n"
                    " 'module.blocks.21.sa.proj.bias, module.blocks.21.ca.v_bias, module.blocks.21.ca.mat_q.bias, module.blocks.21.ca.proj.bias, module.blocks.21.ffn.fc1.bias, module.blocks.21.ffn.fc2.bias, '\n"
                    " 'module.blocks.21.ca_norm.bias, module.blocks.22.ada_gss, module.blocks.22.sa.scale_mul_1H11, module.blocks.22.sa.q_bias, module.blocks.22.sa.v_bias, module.blocks.22.sa.proj.bias, '\n"
                    " 'module.blocks.22.ca.v_bias, module.blocks.22.ca.mat_q.bias, module.blocks.22.ca.proj.bias, module.blocks.22.ffn.fc1.bias, module.blocks.22.ffn.fc2.bias, module.blocks.22.ca_norm.bias, '\n"
                    " 'module.blocks.23.ada_gss, module.blocks.23.sa.scale_mul_1H11, module.blocks.23.sa.q_bias, module.blocks.23.sa.v_bias, module.blocks.23.sa.proj.bias, module.blocks.23.ca.v_bias, '\n"
                    " 'module.blocks.23.ca.mat_q.bias, module.blocks.23.ca.proj.bias, module.blocks.23.ffn.fc1.bias, module.blocks.23.ffn.fc2.bias, module.blocks.23.ca_norm.bias, module.blocks.24.ada_gss, '\n"
                    " 'module.blocks.24.sa.scale_mul_1H11, module.blocks.24.sa.q_bias, module.blocks.24.sa.v_bias, module.blocks.24.sa.proj.bias, module.blocks.24.ca.v_bias, module.blocks.24.ca.mat_q.bias, '\n"
                    " 'module.blocks.24.ca.proj.bias, module.blocks.24.ffn.fc1.bias, module.blocks.24.ffn.fc2.bias, module.blocks.24.ca_norm.bias, module.blocks.25.ada_gss, module.blocks.25.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.25.sa.q_bias, module.blocks.25.sa.v_bias, module.blocks.25.sa.proj.bias, module.blocks.25.ca.v_bias, module.blocks.25.ca.mat_q.bias, module.blocks.25.ca.proj.bias, '\n"
                    " 'module.blocks.25.ffn.fc1.bias, module.blocks.25.ffn.fc2.bias, module.blocks.25.ca_norm.bias, module.blocks.26.ada_gss, module.blocks.26.sa.scale_mul_1H11, module.blocks.26.sa.q_bias, '\n"
                    " 'module.blocks.26.sa.v_bias, module.blocks.26.sa.proj.bias, module.blocks.26.ca.v_bias, module.blocks.26.ca.mat_q.bias, module.blocks.26.ca.proj.bias, module.blocks.26.ffn.fc1.bias, '\n"
                    " 'module.blocks.26.ffn.fc2.bias, module.blocks.26.ca_norm.bias, module.blocks.27.ada_gss, module.blocks.27.sa.scale_mul_1H11, module.blocks.27.sa.q_bias, module.blocks.27.sa.v_bias, '\n"
                    " 'module.blocks.27.sa.proj.bias, module.blocks.27.ca.v_bias, module.blocks.27.ca.mat_q.bias, module.blocks.27.ca.proj.bias, module.blocks.27.ffn.fc1.bias, module.blocks.27.ffn.fc2.bias, '\n"
                    " 'module.blocks.27.ca_norm.bias, module.blocks.28.ada_gss, module.blocks.28.sa.scale_mul_1H11, module.blocks.28.sa.q_bias, module.blocks.28.sa.v_bias, module.blocks.28.sa.proj.bias, '\n"
                    " 'module.blocks.28.ca.v_bias, module.blocks.28.ca.mat_q.bias, module.blocks.28.ca.proj.bias, module.blocks.28.ffn.fc1.bias, module.blocks.28.ffn.fc2.bias, module.blocks.28.ca_norm.bias, '\n"
                    " 'module.blocks.29.ada_gss, module.blocks.29.sa.scale_mul_1H11, module.blocks.29.sa.q_bias, module.blocks.29.sa.v_bias, module.blocks.29.sa.proj.bias, module.blocks.29.ca.v_bias, '\n"
                    " 'module.blocks.29.ca.mat_q.bias, module.blocks.29.ca.proj.bias, module.blocks.29.ffn.fc1.bias, module.blocks.29.ffn.fc2.bias, module.blocks.29.ca_norm.bias, module.blocks.30.ada_gss, '\n"
                    " 'module.blocks.30.sa.scale_mul_1H11, module.blocks.30.sa.q_bias, module.blocks.30.sa.v_bias, module.blocks.30.sa.proj.bias, module.blocks.30.ca.v_bias, module.blocks.30.ca.mat_q.bias, '\n"
                    " 'module.blocks.30.ca.proj.bias, module.blocks.30.ffn.fc1.bias, module.blocks.30.ffn.fc2.bias, module.blocks.30.ca_norm.bias, module.blocks.31.ada_gss, module.blocks.31.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.31.sa.q_bias, module.blocks.31.sa.v_bias, module.blocks.31.sa.proj.bias, module.blocks.31.ca.v_bias, module.blocks.31.ca.mat_q.bias, module.blocks.31.ca.proj.bias, '\n"
                    " 'module.blocks.31.ffn.fc1.bias, module.blocks.31.ffn.fc2.bias, module.blocks.31.ca_norm.bias, module.blocks.32.ada_gss, module.blocks.32.sa.scale_mul_1H11, module.blocks.32.sa.q_bias, '\n"
                    " 'module.blocks.32.sa.v_bias, module.blocks.32.sa.proj.bias, module.blocks.32.ca.v_bias, module.blocks.32.ca.mat_q.bias, module.blocks.32.ca.proj.bias, module.blocks.32.ffn.fc1.bias, '\n"
                    " 'module.blocks.32.ffn.fc2.bias, module.blocks.32.ca_norm.bias, module.blocks.33.ada_gss, module.blocks.33.sa.scale_mul_1H11, module.blocks.33.sa.q_bias, module.blocks.33.sa.v_bias, '\n"
                    " 'module.blocks.33.sa.proj.bias, module.blocks.33.ca.v_bias, module.blocks.33.ca.mat_q.bias, module.blocks.33.ca.proj.bias, module.blocks.33.ffn.fc1.bias, module.blocks.33.ffn.fc2.bias, '\n"
                    " 'module.blocks.33.ca_norm.bias, module.blocks.34.ada_gss, module.blocks.34.sa.scale_mul_1H11, module.blocks.34.sa.q_bias, module.blocks.34.sa.v_bias, module.blocks.34.sa.proj.bias, '\n"
                    " 'module.blocks.34.ca.v_bias, module.blocks.34.ca.mat_q.bias, module.blocks.34.ca.proj.bias, module.blocks.34.ffn.fc1.bias, module.blocks.34.ffn.fc2.bias, module.blocks.34.ca_norm.bias, '\n"
                    " 'module.blocks.35.ada_gss, module.blocks.35.sa.scale_mul_1H11, module.blocks.35.sa.q_bias, module.blocks.35.sa.v_bias, module.blocks.35.sa.proj.bias, module.blocks.35.ca.v_bias, '\n"
                    " 'module.blocks.35.ca.mat_q.bias, module.blocks.35.ca.proj.bias, module.blocks.35.ffn.fc1.bias, module.blocks.35.ffn.fc2.bias, module.blocks.35.ca_norm.bias, module.blocks.36.ada_gss, '\n"
                    " 'module.blocks.36.sa.scale_mul_1H11, module.blocks.36.sa.q_bias, module.blocks.36.sa.v_bias, module.blocks.36.sa.proj.bias, module.blocks.36.ca.v_bias, module.blocks.36.ca.mat_q.bias, '\n"
                    " 'module.blocks.36.ca.proj.bias, module.blocks.36.ffn.fc1.bias, module.blocks.36.ffn.fc2.bias, module.blocks.36.ca_norm.bias, module.blocks.37.ada_gss, module.blocks.37.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.37.sa.q_bias, module.blocks.37.sa.v_bias, module.blocks.37.sa.proj.bias, module.blocks.37.ca.v_bias, module.blocks.37.ca.mat_q.bias, module.blocks.37.ca.proj.bias, '\n"
                    " 'module.blocks.37.ffn.fc1.bias, module.blocks.37.ffn.fc2.bias, module.blocks.37.ca_norm.bias, module.blocks.38.ada_gss, module.blocks.38.sa.scale_mul_1H11, module.blocks.38.sa.q_bias, '\n"
                    " 'module.blocks.38.sa.v_bias, module.blocks.38.sa.proj.bias, module.blocks.38.ca.v_bias, module.blocks.38.ca.mat_q.bias, module.blocks.38.ca.proj.bias, module.blocks.38.ffn.fc1.bias, '\n"
                    " 'module.blocks.38.ffn.fc2.bias, module.blocks.38.ca_norm.bias, module.blocks.39.ada_gss, module.blocks.39.sa.scale_mul_1H11, module.blocks.39.sa.q_bias, module.blocks.39.sa.v_bias, '\n"
                    " 'module.blocks.39.sa.proj.bias, module.blocks.39.ca.v_bias, module.blocks.39.ca.mat_q.bias, module.blocks.39.ca.proj.bias, module.blocks.39.ffn.fc1.bias, module.blocks.39.ffn.fc2.bias, '\n"
                    " 'module.blocks.39.ca_norm.bias, module.blocks.40.ada_gss, module.blocks.40.sa.scale_mul_1H11, module.blocks.40.sa.q_bias, module.blocks.40.sa.v_bias, module.blocks.40.sa.proj.bias, '\n"
                    " 'module.blocks.40.ca.v_bias, module.blocks.40.ca.mat_q.bias, module.blocks.40.ca.proj.bias, module.blocks.40.ffn.fc1.bias, module.blocks.40.ffn.fc2.bias, module.blocks.40.ca_norm.bias, '\n"
                    " 'module.blocks.41.ada_gss, module.blocks.41.sa.scale_mul_1H11, module.blocks.41.sa.q_bias, module.blocks.41.sa.v_bias, module.blocks.41.sa.proj.bias, module.blocks.41.ca.v_bias, '\n"
                    " 'module.blocks.41.ca.mat_q.bias, module.blocks.41.ca.proj.bias, module.blocks.41.ffn.fc1.bias, module.blocks.41.ffn.fc2.bias, module.blocks.41.ca_norm.bias, module.blocks.42.ada_gss, '\n"
                    " 'module.blocks.42.sa.scale_mul_1H11, module.blocks.42.sa.q_bias, module.blocks.42.sa.v_bias, module.blocks.42.sa.proj.bias, module.blocks.42.ca.v_bias, module.blocks.42.ca.mat_q.bias, '\n"
                    " 'module.blocks.42.ca.proj.bias, module.blocks.42.ffn.fc1.bias, module.blocks.42.ffn.fc2.bias, module.blocks.42.ca_norm.bias, module.blocks.43.ada_gss, module.blocks.43.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.43.sa.q_bias, module.blocks.43.sa.v_bias, module.blocks.43.sa.proj.bias, module.blocks.43.ca.v_bias, module.blocks.43.ca.mat_q.bias, module.blocks.43.ca.proj.bias, '\n"
                    " 'module.blocks.43.ffn.fc1.bias, module.blocks.43.ffn.fc2.bias, module.blocks.43.ca_norm.bias, module.blocks.44.ada_gss, module.blocks.44.sa.scale_mul_1H11, module.blocks.44.sa.q_bias, '\n"
                    " 'module.blocks.44.sa.v_bias, module.blocks.44.sa.proj.bias, module.blocks.44.ca.v_bias, module.blocks.44.ca.mat_q.bias, module.blocks.44.ca.proj.bias, module.blocks.44.ffn.fc1.bias, '\n"
                    " 'module.blocks.44.ffn.fc2.bias, module.blocks.44.ca_norm.bias, module.blocks.45.ada_gss, module.blocks.45.sa.scale_mul_1H11, module.blocks.45.sa.q_bias, module.blocks.45.sa.v_bias, '\n"
                    " 'module.blocks.45.sa.proj.bias, module.blocks.45.ca.v_bias, module.blocks.45.ca.mat_q.bias, module.blocks.45.ca.proj.bias, module.blocks.45.ffn.fc1.bias, module.blocks.45.ffn.fc2.bias, '\n"
                    " 'module.blocks.45.ca_norm.bias, module.blocks.46.ada_gss, module.blocks.46.sa.scale_mul_1H11, module.blocks.46.sa.q_bias, module.blocks.46.sa.v_bias, module.blocks.46.sa.proj.bias, '\n"
                    " 'module.blocks.46.ca.v_bias, module.blocks.46.ca.mat_q.bias, module.blocks.46.ca.proj.bias, module.blocks.46.ffn.fc1.bias, module.blocks.46.ffn.fc2.bias, module.blocks.46.ca_norm.bias, '\n"
                    " 'module.blocks.47.ada_gss, module.blocks.47.sa.scale_mul_1H11, module.blocks.47.sa.q_bias, module.blocks.47.sa.v_bias, module.blocks.47.sa.proj.bias, module.blocks.47.ca.v_bias, '\n"
                    " 'module.blocks.47.ca.mat_q.bias, module.blocks.47.ca.proj.bias, module.blocks.47.ffn.fc1.bias, module.blocks.47.ffn.fc2.bias, module.blocks.47.ca_norm.bias, module.blocks.48.ada_gss, '\n"
                    " 'module.blocks.48.sa.scale_mul_1H11, module.blocks.48.sa.q_bias, module.blocks.48.sa.v_bias, module.blocks.48.sa.proj.bias, module.blocks.48.ca.v_bias, module.blocks.48.ca.mat_q.bias, '\n"
                    " 'module.blocks.48.ca.proj.bias, module.blocks.48.ffn.fc1.bias, module.blocks.48.ffn.fc2.bias, module.blocks.48.ca_norm.bias, module.blocks.49.ada_gss, module.blocks.49.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.49.sa.q_bias, module.blocks.49.sa.v_bias, module.blocks.49.sa.proj.bias, module.blocks.49.ca.v_bias, module.blocks.49.ca.mat_q.bias, module.blocks.49.ca.proj.bias, '\n"
                    " 'module.blocks.49.ffn.fc1.bias, module.blocks.49.ffn.fc2.bias, module.blocks.49.ca_norm.bias, module.blocks.50.ada_gss, module.blocks.50.sa.scale_mul_1H11, module.blocks.50.sa.q_bias, '\n"
                    " 'module.blocks.50.sa.v_bias, module.blocks.50.sa.proj.bias, module.blocks.50.ca.v_bias, module.blocks.50.ca.mat_q.bias, module.blocks.50.ca.proj.bias, module.blocks.50.ffn.fc1.bias, '\n"
                    " 'module.blocks.50.ffn.fc2.bias, module.blocks.50.ca_norm.bias, module.blocks.51.ada_gss, module.blocks.51.sa.scale_mul_1H11, module.blocks.51.sa.q_bias, module.blocks.51.sa.v_bias, '\n"
                    " 'module.blocks.51.sa.proj.bias, module.blocks.51.ca.v_bias, module.blocks.51.ca.mat_q.bias, module.blocks.51.ca.proj.bias, module.blocks.51.ffn.fc1.bias, module.blocks.51.ffn.fc2.bias, '\n"
                    " 'module.blocks.51.ca_norm.bias, module.blocks.52.ada_gss, module.blocks.52.sa.scale_mul_1H11, module.blocks.52.sa.q_bias, module.blocks.52.sa.v_bias, module.blocks.52.sa.proj.bias, '\n"
                    " 'module.blocks.52.ca.v_bias, module.blocks.52.ca.mat_q.bias, module.blocks.52.ca.proj.bias, module.blocks.52.ffn.fc1.bias, module.blocks.52.ffn.fc2.bias, module.blocks.52.ca_norm.bias, '\n"
                    " 'module.blocks.53.ada_gss, module.blocks.53.sa.scale_mul_1H11, module.blocks.53.sa.q_bias, module.blocks.53.sa.v_bias, module.blocks.53.sa.proj.bias, module.blocks.53.ca.v_bias, '\n"
                    " 'module.blocks.53.ca.mat_q.bias, module.blocks.53.ca.proj.bias, module.blocks.53.ffn.fc1.bias, module.blocks.53.ffn.fc2.bias, module.blocks.53.ca_norm.bias, module.blocks.54.ada_gss, '\n"
                    " 'module.blocks.54.sa.scale_mul_1H11, module.blocks.54.sa.q_bias, module.blocks.54.sa.v_bias, module.blocks.54.sa.proj.bias, module.blocks.54.ca.v_bias, module.blocks.54.ca.mat_q.bias, '\n"
                    " 'module.blocks.54.ca.proj.bias, module.blocks.54.ffn.fc1.bias, module.blocks.54.ffn.fc2.bias, module.blocks.54.ca_norm.bias, module.blocks.55.ada_gss, module.blocks.55.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.55.sa.q_bias, module.blocks.55.sa.v_bias, module.blocks.55.sa.proj.bias, module.blocks.55.ca.v_bias, module.blocks.55.ca.mat_q.bias, module.blocks.55.ca.proj.bias, '\n"
                    " 'module.blocks.55.ffn.fc1.bias, module.blocks.55.ffn.fc2.bias, module.blocks.55.ca_norm.bias')",
          'wd_sc': 0.0}}

[09-01 23:02:56] (nity/utils/lr_control.py, line 116)=> [get_param_groups][rank0] type(model).__name__='NullDDP' count=1085, numel=8573892016
[09-01 23:02:56] (nity/utils/lr_control.py, line 117)=> 
[09-01 23:02:56] (inity_circuit_breaker.py, line 343)=> [vgpt] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.97), fused=True), opt_kw={'lr': 9.375e-05, 'weight_decay': 0}

[09-01 23:02:56] (inity_circuit_breaker.py, line 348)=> Loading T5 from google/flan-t5-xl...
[09-01 23:03:07] (_original/src/trainer.py, line  65)=> self.reweight_loss_by_scale: 1
[09-01 23:03:07] (inity_circuit_breaker.py, line 171)=> global bs=4, local bs=4
[09-01 23:03:07] (inity_circuit_breaker.py, line 172)=> initial args:
{
  local_out_path      : ./local_output
  data_path           : ./data
  bed                 : ./checkpoints
  vae_ckpt            : /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
  exp_name            : experiment
  ds                  : oi
  model               : infinity_8b
  short_cap_prob      : 0.2
  project_name        : infinity_circuit_breaker
  tf32                : True
  auto_resume         : True
  rush_resume         : 
  nowd                : 1
  enable_hybrid_shard : False
  inner_shard_degree  : 1
  zero                : 2
  buck                : chunk
  fsdp_orig           : True
  enable_checkpointing: full-block
  pad_to_multiplier   : 128
  log_every_iter      : False
  checkpoint_type     : torch
  seed                : None
  rand                : True
  task_id             : 123
  trial_id            : 123
  robust_run_id       : 0
  ckpt_trials         : []
  real_trial_id       : 123
  chunk_nodes         : 0
  is_master_node      : None
  log_txt_path        : ./local_output/log.txt
  t5_path             : google/flan-t5-xl
  online_t5           : True
  sdpa_mem            : True
  tfast               : 0
  model_alias         : 8b
  rms                 : False
  aln                 : 0.001
  alng                : 5e-06
  saln                : True
  haln                : True
  nm0                 : False
  tau                 : 1
  cos                 : True
  swi                 : False
  dp                  : -1
  drop                : 0.0
  hd                  : 0
  ca_gamma            : -1
  diva                : 1
  hd0                 : 0.02
  dec                 : 1
  cum                 : 3
  rwe                 : False
  tp                  : 0.0
  tk                  : 0.0
  tini                : 0.010416666666666666
  cfg                 : 0.1
  rand_uncond         : False
  ema                 : 0.9999
  tema                : 0
  fp16                : 2
  fuse                : False
  fused_norm          : False
  flash               : False
  xen                 : False
  use_flex_attn       : True
  stable              : False
  gblr                : 0.0001
  dblr                : 0.0001
  tblr                : 0.006
  glr                 : 1.5625e-06
  dlr                 : 1.5625e-06
  tlr                 : 9.375e-05
  gwd                 : 0.005
  dwd                 : 0.0005
  twd                 : 0.005
  gwde                : 0.005
  dwde                : 0.0005
  twde                : 0.005
  ls                  : 0.0
  lz                  : 0.0
  eq                  : 0
  ep                  : 100
  wp                  : 1e-08
  wp0                 : 0.005
  wpe                 : 1.0
  sche                : lin0
  log_freq            : 50
  gclip               : 6.0
  dclip               : 6.0
  tclip               : 5.0
  cdec                : False
  opt                 : adamw
  ada                 : 0.9_0.97
  dada                : 0.9_0.97
  oeps                : 0
  afuse               : True
  pn                  : 0.06M
  scale_schedule      : None
  patch_size          : None
  resos               : None
  data_load_reso      : None
  workers             : 4
  lbs                 : 4
  bs                  : 4
  batch_size          : 4
  glb_batch_size      : 4
  ac                  : 1
  r_accu              : 1.0
  norm_eps            : 1e-06
  tlen                : 512
  Ct5                 : 2048
  use_bit_label       : 1
  bitloss_type        : mean
  dynamic_resolution_across_gpus: 1
  enable_dynamic_length_prompt: 1
  use_streaming_dataset: 1
  iterable_data_buffersize: 30000
  save_model_iters_freq: 100
  noise_apply_layers  : 13
  noise_apply_strength: 0.3
  noise_apply_requant : 1
  rope2d_each_sa_layer: 1
  rope2d_normalized_by_hw: 2
  use_fsdp_model_ema  : 0
  add_lvl_embeding_only_first_block: 1
  reweight_loss_by_scale: 1
  always_training_scales: 100
  vae_type            : 14
  fake_vae_input      : False
  model_init_device   : cuda
  prefetch_factor     : 2
  apply_spatial_patchify: 1
  debug_bsc           : 0
  task_type           : t2i
  target_layers       : [10, 12, 14, 16, 18, 20]
  transform_layers    : [10, 12, 14, 16, 18, 20]
  lorra_alpha         : 5.0
  trainsets           : ['AlpacaSupervisedDataset', 'HarmfulDataset']
  valsets             : ['AlpacaSupervisedDataset', 'HarmfulDataset']
  adv_string          : Sure here's
  full_layers         : False
  harmful_prompts_path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
  sanitized_prompts_path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
  validation_ratio    : 0.1
  category            : hate
  lora_r              : 8
  lora_alpha          : 16
  lora_dropout        : 0.05
  lora_target_modules : ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
  lora_weight_path    : 
  lora_bias           : none
  q_lora              : False
  model_name_or_path  : meta-llama/Llama-2-7b-chat-hf
  adapter_name_or_path: 
  use_lora            : True
  cache_dir           : 
  optim               : adamw_torch
  model_max_length    : 512
  grouped_to_max_length: False
  use_refusal_retain  : True
  sc_train_subset     : ['']
  log_every           : 10
  sc_train_seq_type   : all_text
  coeff_schedule      : linear_converge
  sc_loss_type        : orig_act_dotprod
  branch              : master
  commit_id           : 
  commit_msg          : 
  cmd                 : --bed ./checkpoints --local_out_path ./local_output --project_name infinity_circuit_breaker --exp_name experiment --ep 100 --opt adamw --cum 3 --sche lin0 --fp16 2 --ada 0.9_0.97 --tini -1 --tclip 5 --flash 0 --alng 5e-06 --saln 1 --cos 1 --enable_checkpointing full-block --tblr 6e-3 --pn 0.06M --lbs 4 --workers 4 --short_cap_prob 0.2 --online_t5 1 --use_streaming_dataset 1 --iterable_data_buffersize 30000 --Ct5 2048 --t5_path google/flan-t5-xl --vae_type 14 --wp 0.00000001 --wpe 1 --dynamic_resolution_across_gpus 1 --enable_dynamic_length_prompt 1 --reweight_loss_by_scale 1 --add_lvl_embeding_only_first_block 1 --rope2d_each_sa_layer 1 --rope2d_normalized_by_hw 2 --use_fsdp_model_ema 0 --always_training_scales 100 --use_bit_label 1 --zero 2 --save_model_iters_freq 100 --log_freq 50 --checkpoint_type torch --prefetch_factor 2 --noise_apply_strength 0.3 --noise_apply_layers 13 --apply_spatial_patchify 1 --use_flex_attn True --pad 128 --data_path ./data --data_load_reso 512 --tlen 512 --workers 4 --prefetch_factor 2 --harmful_prompts_path /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts --sanitized_prompts_path /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts --validation_ratio 0.1 --category hate --target_layers 10,12,14,16,18,20 --transform_layers 10,12,14,16,18,20 --lorra_alpha 5.0 --trainsets AlpacaSupervisedDataset#HarmfulDataset --valsets AlpacaSupervisedDataset#HarmfulDataset --adv_string Sure here's --full_layers False --lora_r 8 --lora_alpha 16 --lora_dropout 0.05 --lora_target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj --lora_weight_path  --lora_bias none --q_lora False --model_name_or_path meta-llama/Llama-2-7b-chat-hf --adapter_name_or_path  --use_lora True --cache_dir  --optim adamw_torch --model_max_length 512 --grouped_to_max_length False --use_refusal_retain True --sc_train_subset  --log_every 10 --sc_train_seq_type all_text --coeff_schedule linear_converge --sc_loss_type orig_act_dotprod
  tag                 : UK
  acc_all             : None
  acc_real            : None
  acc_fake            : None
  last_Lnll           : None
  last_L1             : None
  last_Ld             : None
  last_wei_g          : None
  grad_boom           : None
  diff                : None
  diffs               : 
  diffs_ema           : None
  ca_performance      : 
  cur_phase           : 
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  iter_speed          : None
  img_per_day         : None
  max_nvidia_smi      : 0
  max_memory_allocated: None
  max_memory_reserved : None
  num_alloc_retries   : None
  MFU                 : None
  HFU                 : None
  dbg_modified        : False
  dbg_ks              : False
  dbg_ks_last         : None
  dbg                 : False
  ks                  : False
  nodata              : False
  nodata_tlen         : 320
  nova                : False
  prof                : 0
  prof_freq           : 50
  tos_profiler_file_prefix: vgpt_default/
  profall             : 0
  v_seed              : 0
  g_seed              : 0
}

[09-01 23:03:07] (inity_circuit_breaker.py, line 182)=> start_it=0, iters_train=100







=======================================================   RESTART [09-01 23:05:56]   =======================================================
[09-01 23:05:56] (inity_circuit_breaker.py, line 578)=> ============================================================
[09-01 23:05:56] (inity_circuit_breaker.py, line 579)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[09-01 23:05:56] (inity_circuit_breaker.py, line 580)=> Circuit Breaker Alpha: 0.1
[09-01 23:05:56] (inity_circuit_breaker.py, line 581)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[09-01 23:05:56] (inity_circuit_breaker.py, line 582)=> Circuit Breaker Enabled: True
[09-01 23:05:56] (inity_circuit_breaker.py, line 583)=> Number of Examples: 1000
[09-01 23:05:56] (inity_circuit_breaker.py, line 584)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[09-01 23:05:56] (inity_circuit_breaker.py, line 585)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[09-01 23:05:56] (inity_circuit_breaker.py, line 586)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[09-01 23:05:56] (inity_circuit_breaker.py, line 587)=> Validation Ratio: 0.1
[09-01 23:05:56] (inity_circuit_breaker.py, line 588)=> Category: hate
[09-01 23:05:56] (inity_circuit_breaker.py, line 589)=> Batch Size: 4
[09-01 23:05:56] (inity_circuit_breaker.py, line 590)=> Workers: 4
[09-01 23:05:56] (inity_circuit_breaker.py, line 591)=> Device: cuda:0
[09-01 23:05:56] (inity_circuit_breaker.py, line 592)=> Model: infinity_8b
[09-01 23:05:56] (inity_circuit_breaker.py, line 593)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-01 23:05:56] (inity_circuit_breaker.py, line 594)=> Rush Resume: 
[09-01 23:05:56] (inity_circuit_breaker.py, line 595)=> Selective Layers: 0,1,2,3,4,5
[09-01 23:05:56] (inity_circuit_breaker.py, line 596)=> ============================================================
[09-01 23:05:56] (inity_circuit_breaker.py, line 377)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[09-01 23:05:56] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-01 23:05:56] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-01 23:05:56] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[09-01 23:05:56] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-01 23:05:56] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-01 23:05:57] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[09-01 23:05:57] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[09-01 23:05:57] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[09-01 23:05:57] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[09-01 23:05:57] (inity_circuit_breaker.py, line 395)=> args.batch_size=4, vbs=6
[09-01 23:05:57] (inity_circuit_breaker.py, line 418)=> len(dataloader): 100, len(dataset): 100
[09-01 23:05:57] (inity_circuit_breaker.py, line 420)=> total_samples: 200
[09-01 23:05:57] (inity_circuit_breaker.py, line 421)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[09-01 23:05:57] (inity_circuit_breaker.py, line 140)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[09-01 23:05:57] (inity_circuit_breaker.py, line 142)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[09-01 23:05:57] (inity_circuit_breaker.py, line 146)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-01 23:06:00] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[09-01 23:06:00] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[09-01 23:06:00] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[09-01 23:06:00] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[09-01 23:06:02] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[09-01 23:06:04] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[09-01 23:06:04] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[09-01 23:06:04] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[09-01 23:06:04] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[09-01 23:06:04] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

[09-01 23:06:15] (ity/models/init_param.py, line  15)=> [init_weights] Infinity with std=0.02
[09-01 23:06:15] (inity_circuit_breaker.py, line 246)=> [PT] GPT model = Infinity(
  drop_path_rate=0.2
  (norm0_cond): Identity()
  (text_norm): FastRMSNorm(C=2048, eps=1e-06, elementwise_affine=True)
  (text_proj_for_sos): TextAttentivePool(
    (ca): CrossAttention(
      Cq=3072, Ckv=2048, cos_attn=False
      (mat_kv): Linear(in_features=2048, out_features=6144, bias=False)
      (proj): Linear(in_features=3072, out_features=3072, bias=True)
      (proj_drop): Identity()
    )
  )
  (text_proj_for_ca): Sequential(
    (0): Linear(in_features=2048, out_features=3072, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=3072, out_features=3072, bias=True)
  )
  (lvl_embed): Embedding(15, 3072)
  (norm0_ve): Identity()
  (word_embed): Linear(in_features=56, out_features=3072, bias=True)
  (shared_ada_lin): Sequential(
    (0): SiLU()
    (1): SharedAdaLin(in_features=3072, out_features=18432, bias=True)
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=3072, out_features=6144, bias=True)
    )
  )
  (head): Linear(in_features=3072, out_features=112, bias=True)
  (blocks): ModuleList(
    (0): CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): Identity()
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
    (1-55): 55 x CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): DropPath(...)
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
  )
)


[09-01 23:06:15] (inity_circuit_breaker.py, line 248)=> [PT][#para] VAE=81.62, VAE.quant=0.00
[09-01 23:06:15] (inity_circuit_breaker.py, line 251)=> [PT][#para] GPT=8573.89


[09-01 23:06:15] (nity/utils/lr_control.py, line  71)=> [get_param_groups][lr decay] with_lr_scale=False, lr_scale=0.0
[09-01 23:06:15] (nity/utils/lr_control.py, line 111)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': '[no scale]',
         'params': "('module.text_norm.weight, module.text_proj_for_sos.ca.mat_kv.weight, module.text_proj_for_sos.ca.proj.weight, module.text_proj_for_ca.0.weight, module.text_proj_for_ca.2.weight, '\n"
                   " 'module.word_embed.weight, module.shared_ada_lin.1.weight, module.head_nm.ada_lin.1.weight, module.head.weight, module.blocks.0.sa.mat_qkv.weight, module.blocks.0.sa.proj.weight, '\n"
                   " 'module.blocks.0.ca.mat_q.weight, module.blocks.0.ca.mat_kv.weight, module.blocks.0.ca.proj.weight, module.blocks.0.ffn.fc1.weight, module.blocks.0.ffn.fc2.weight, module.blocks.0.ca_norm.weight, '\n"
                   " 'module.blocks.1.sa.mat_qkv.weight, module.blocks.1.sa.proj.weight, module.blocks.1.ca.mat_q.weight, module.blocks.1.ca.mat_kv.weight, module.blocks.1.ca.proj.weight, '\n"
                   " 'module.blocks.1.ffn.fc1.weight, module.blocks.1.ffn.fc2.weight, module.blocks.1.ca_norm.weight, module.blocks.2.sa.mat_qkv.weight, module.blocks.2.sa.proj.weight, module.blocks.2.ca.mat_q.weight, '\n"
                   " 'module.blocks.2.ca.mat_kv.weight, module.blocks.2.ca.proj.weight, module.blocks.2.ffn.fc1.weight, module.blocks.2.ffn.fc2.weight, module.blocks.2.ca_norm.weight, module.blocks.3.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.3.sa.proj.weight, module.blocks.3.ca.mat_q.weight, module.blocks.3.ca.mat_kv.weight, module.blocks.3.ca.proj.weight, module.blocks.3.ffn.fc1.weight, module.blocks.3.ffn.fc2.weight, '\n"
                   " 'module.blocks.3.ca_norm.weight, module.blocks.4.sa.mat_qkv.weight, module.blocks.4.sa.proj.weight, module.blocks.4.ca.mat_q.weight, module.blocks.4.ca.mat_kv.weight, '\n"
                   " 'module.blocks.4.ca.proj.weight, module.blocks.4.ffn.fc1.weight, module.blocks.4.ffn.fc2.weight, module.blocks.4.ca_norm.weight, module.blocks.5.sa.mat_qkv.weight, module.blocks.5.sa.proj.weight, '\n"
                   " 'module.blocks.5.ca.mat_q.weight, module.blocks.5.ca.mat_kv.weight, module.blocks.5.ca.proj.weight, module.blocks.5.ffn.fc1.weight, module.blocks.5.ffn.fc2.weight, module.blocks.5.ca_norm.weight, '\n"
                   " 'module.blocks.6.sa.mat_qkv.weight, module.blocks.6.sa.proj.weight, module.blocks.6.ca.mat_q.weight, module.blocks.6.ca.mat_kv.weight, module.blocks.6.ca.proj.weight, '\n"
                   " 'module.blocks.6.ffn.fc1.weight, module.blocks.6.ffn.fc2.weight, module.blocks.6.ca_norm.weight, module.blocks.7.sa.mat_qkv.weight, module.blocks.7.sa.proj.weight, module.blocks.7.ca.mat_q.weight, '\n"
                   " 'module.blocks.7.ca.mat_kv.weight, module.blocks.7.ca.proj.weight, module.blocks.7.ffn.fc1.weight, module.blocks.7.ffn.fc2.weight, module.blocks.7.ca_norm.weight, module.blocks.8.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.8.sa.proj.weight, module.blocks.8.ca.mat_q.weight, module.blocks.8.ca.mat_kv.weight, module.blocks.8.ca.proj.weight, module.blocks.8.ffn.fc1.weight, module.blocks.8.ffn.fc2.weight, '\n"
                   " 'module.blocks.8.ca_norm.weight, module.blocks.9.sa.mat_qkv.weight, module.blocks.9.sa.proj.weight, module.blocks.9.ca.mat_q.weight, module.blocks.9.ca.mat_kv.weight, '\n"
                   " 'module.blocks.9.ca.proj.weight, module.blocks.9.ffn.fc1.weight, module.blocks.9.ffn.fc2.weight, module.blocks.9.ca_norm.weight, module.blocks.10.sa.mat_qkv.weight, module.blocks.10.sa.proj.weight, '\n"
                   " 'module.blocks.10.ca.mat_q.weight, module.blocks.10.ca.mat_kv.weight, module.blocks.10.ca.proj.weight, module.blocks.10.ffn.fc1.weight, module.blocks.10.ffn.fc2.weight, '\n"
                   " 'module.blocks.10.ca_norm.weight, module.blocks.11.sa.mat_qkv.weight, module.blocks.11.sa.proj.weight, module.blocks.11.ca.mat_q.weight, module.blocks.11.ca.mat_kv.weight, '\n"
                   " 'module.blocks.11.ca.proj.weight, module.blocks.11.ffn.fc1.weight, module.blocks.11.ffn.fc2.weight, module.blocks.11.ca_norm.weight, module.blocks.12.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.12.sa.proj.weight, module.blocks.12.ca.mat_q.weight, module.blocks.12.ca.mat_kv.weight, module.blocks.12.ca.proj.weight, module.blocks.12.ffn.fc1.weight, '\n"
                   " 'module.blocks.12.ffn.fc2.weight, module.blocks.12.ca_norm.weight, module.blocks.13.sa.mat_qkv.weight, module.blocks.13.sa.proj.weight, module.blocks.13.ca.mat_q.weight, '\n"
                   " 'module.blocks.13.ca.mat_kv.weight, module.blocks.13.ca.proj.weight, module.blocks.13.ffn.fc1.weight, module.blocks.13.ffn.fc2.weight, module.blocks.13.ca_norm.weight, '\n"
                   " 'module.blocks.14.sa.mat_qkv.weight, module.blocks.14.sa.proj.weight, module.blocks.14.ca.mat_q.weight, module.blocks.14.ca.mat_kv.weight, module.blocks.14.ca.proj.weight, '\n"
                   " 'module.blocks.14.ffn.fc1.weight, module.blocks.14.ffn.fc2.weight, module.blocks.14.ca_norm.weight, module.blocks.15.sa.mat_qkv.weight, module.blocks.15.sa.proj.weight, '\n"
                   " 'module.blocks.15.ca.mat_q.weight, module.blocks.15.ca.mat_kv.weight, module.blocks.15.ca.proj.weight, module.blocks.15.ffn.fc1.weight, module.blocks.15.ffn.fc2.weight, '\n"
                   " 'module.blocks.15.ca_norm.weight, module.blocks.16.sa.mat_qkv.weight, module.blocks.16.sa.proj.weight, module.blocks.16.ca.mat_q.weight, module.blocks.16.ca.mat_kv.weight, '\n"
                   " 'module.blocks.16.ca.proj.weight, module.blocks.16.ffn.fc1.weight, module.blocks.16.ffn.fc2.weight, module.blocks.16.ca_norm.weight, module.blocks.17.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.17.sa.proj.weight, module.blocks.17.ca.mat_q.weight, module.blocks.17.ca.mat_kv.weight, module.blocks.17.ca.proj.weight, module.blocks.17.ffn.fc1.weight, '\n"
                   " 'module.blocks.17.ffn.fc2.weight, module.blocks.17.ca_norm.weight, module.blocks.18.sa.mat_qkv.weight, module.blocks.18.sa.proj.weight, module.blocks.18.ca.mat_q.weight, '\n"
                   " 'module.blocks.18.ca.mat_kv.weight, module.blocks.18.ca.proj.weight, module.blocks.18.ffn.fc1.weight, module.blocks.18.ffn.fc2.weight, module.blocks.18.ca_norm.weight, '\n"
                   " 'module.blocks.19.sa.mat_qkv.weight, module.blocks.19.sa.proj.weight, module.blocks.19.ca.mat_q.weight, module.blocks.19.ca.mat_kv.weight, module.blocks.19.ca.proj.weight, '\n"
                   " 'module.blocks.19.ffn.fc1.weight, module.blocks.19.ffn.fc2.weight, module.blocks.19.ca_norm.weight, module.blocks.20.sa.mat_qkv.weight, module.blocks.20.sa.proj.weight, '\n"
                   " 'module.blocks.20.ca.mat_q.weight, module.blocks.20.ca.mat_kv.weight, module.blocks.20.ca.proj.weight, module.blocks.20.ffn.fc1.weight, module.blocks.20.ffn.fc2.weight, '\n"
                   " 'module.blocks.20.ca_norm.weight, module.blocks.21.sa.mat_qkv.weight, module.blocks.21.sa.proj.weight, module.blocks.21.ca.mat_q.weight, module.blocks.21.ca.mat_kv.weight, '\n"
                   " 'module.blocks.21.ca.proj.weight, module.blocks.21.ffn.fc1.weight, module.blocks.21.ffn.fc2.weight, module.blocks.21.ca_norm.weight, module.blocks.22.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.22.sa.proj.weight, module.blocks.22.ca.mat_q.weight, module.blocks.22.ca.mat_kv.weight, module.blocks.22.ca.proj.weight, module.blocks.22.ffn.fc1.weight, '\n"
                   " 'module.blocks.22.ffn.fc2.weight, module.blocks.22.ca_norm.weight, module.blocks.23.sa.mat_qkv.weight, module.blocks.23.sa.proj.weight, module.blocks.23.ca.mat_q.weight, '\n"
                   " 'module.blocks.23.ca.mat_kv.weight, module.blocks.23.ca.proj.weight, module.blocks.23.ffn.fc1.weight, module.blocks.23.ffn.fc2.weight, module.blocks.23.ca_norm.weight, '\n"
                   " 'module.blocks.24.sa.mat_qkv.weight, module.blocks.24.sa.proj.weight, module.blocks.24.ca.mat_q.weight, module.blocks.24.ca.mat_kv.weight, module.blocks.24.ca.proj.weight, '\n"
                   " 'module.blocks.24.ffn.fc1.weight, module.blocks.24.ffn.fc2.weight, module.blocks.24.ca_norm.weight, module.blocks.25.sa.mat_qkv.weight, module.blocks.25.sa.proj.weight, '\n"
                   " 'module.blocks.25.ca.mat_q.weight, module.blocks.25.ca.mat_kv.weight, module.blocks.25.ca.proj.weight, module.blocks.25.ffn.fc1.weight, module.blocks.25.ffn.fc2.weight, '\n"
                   " 'module.blocks.25.ca_norm.weight, module.blocks.26.sa.mat_qkv.weight, module.blocks.26.sa.proj.weight, module.blocks.26.ca.mat_q.weight, module.blocks.26.ca.mat_kv.weight, '\n"
                   " 'module.blocks.26.ca.proj.weight, module.blocks.26.ffn.fc1.weight, module.blocks.26.ffn.fc2.weight, module.blocks.26.ca_norm.weight, module.blocks.27.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.27.sa.proj.weight, module.blocks.27.ca.mat_q.weight, module.blocks.27.ca.mat_kv.weight, module.blocks.27.ca.proj.weight, module.blocks.27.ffn.fc1.weight, '\n"
                   " 'module.blocks.27.ffn.fc2.weight, module.blocks.27.ca_norm.weight, module.blocks.28.sa.mat_qkv.weight, module.blocks.28.sa.proj.weight, module.blocks.28.ca.mat_q.weight, '\n"
                   " 'module.blocks.28.ca.mat_kv.weight, module.blocks.28.ca.proj.weight, module.blocks.28.ffn.fc1.weight, module.blocks.28.ffn.fc2.weight, module.blocks.28.ca_norm.weight, '\n"
                   " 'module.blocks.29.sa.mat_qkv.weight, module.blocks.29.sa.proj.weight, module.blocks.29.ca.mat_q.weight, module.blocks.29.ca.mat_kv.weight, module.blocks.29.ca.proj.weight, '\n"
                   " 'module.blocks.29.ffn.fc1.weight, module.blocks.29.ffn.fc2.weight, module.blocks.29.ca_norm.weight, module.blocks.30.sa.mat_qkv.weight, module.blocks.30.sa.proj.weight, '\n"
                   " 'module.blocks.30.ca.mat_q.weight, module.blocks.30.ca.mat_kv.weight, module.blocks.30.ca.proj.weight, module.blocks.30.ffn.fc1.weight, module.blocks.30.ffn.fc2.weight, '\n"
                   " 'module.blocks.30.ca_norm.weight, module.blocks.31.sa.mat_qkv.weight, module.blocks.31.sa.proj.weight, module.blocks.31.ca.mat_q.weight, module.blocks.31.ca.mat_kv.weight, '\n"
                   " 'module.blocks.31.ca.proj.weight, module.blocks.31.ffn.fc1.weight, module.blocks.31.ffn.fc2.weight, module.blocks.31.ca_norm.weight, module.blocks.32.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.32.sa.proj.weight, module.blocks.32.ca.mat_q.weight, module.blocks.32.ca.mat_kv.weight, module.blocks.32.ca.proj.weight, module.blocks.32.ffn.fc1.weight, '\n"
                   " 'module.blocks.32.ffn.fc2.weight, module.blocks.32.ca_norm.weight, module.blocks.33.sa.mat_qkv.weight, module.blocks.33.sa.proj.weight, module.blocks.33.ca.mat_q.weight, '\n"
                   " 'module.blocks.33.ca.mat_kv.weight, module.blocks.33.ca.proj.weight, module.blocks.33.ffn.fc1.weight, module.blocks.33.ffn.fc2.weight, module.blocks.33.ca_norm.weight, '\n"
                   " 'module.blocks.34.sa.mat_qkv.weight, module.blocks.34.sa.proj.weight, module.blocks.34.ca.mat_q.weight, module.blocks.34.ca.mat_kv.weight, module.blocks.34.ca.proj.weight, '\n"
                   " 'module.blocks.34.ffn.fc1.weight, module.blocks.34.ffn.fc2.weight, module.blocks.34.ca_norm.weight, module.blocks.35.sa.mat_qkv.weight, module.blocks.35.sa.proj.weight, '\n"
                   " 'module.blocks.35.ca.mat_q.weight, module.blocks.35.ca.mat_kv.weight, module.blocks.35.ca.proj.weight, module.blocks.35.ffn.fc1.weight, module.blocks.35.ffn.fc2.weight, '\n"
                   " 'module.blocks.35.ca_norm.weight, module.blocks.36.sa.mat_qkv.weight, module.blocks.36.sa.proj.weight, module.blocks.36.ca.mat_q.weight, module.blocks.36.ca.mat_kv.weight, '\n"
                   " 'module.blocks.36.ca.proj.weight, module.blocks.36.ffn.fc1.weight, module.blocks.36.ffn.fc2.weight, module.blocks.36.ca_norm.weight, module.blocks.37.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.37.sa.proj.weight, module.blocks.37.ca.mat_q.weight, module.blocks.37.ca.mat_kv.weight, module.blocks.37.ca.proj.weight, module.blocks.37.ffn.fc1.weight, '\n"
                   " 'module.blocks.37.ffn.fc2.weight, module.blocks.37.ca_norm.weight, module.blocks.38.sa.mat_qkv.weight, module.blocks.38.sa.proj.weight, module.blocks.38.ca.mat_q.weight, '\n"
                   " 'module.blocks.38.ca.mat_kv.weight, module.blocks.38.ca.proj.weight, module.blocks.38.ffn.fc1.weight, module.blocks.38.ffn.fc2.weight, module.blocks.38.ca_norm.weight, '\n"
                   " 'module.blocks.39.sa.mat_qkv.weight, module.blocks.39.sa.proj.weight, module.blocks.39.ca.mat_q.weight, module.blocks.39.ca.mat_kv.weight, module.blocks.39.ca.proj.weight, '\n"
                   " 'module.blocks.39.ffn.fc1.weight, module.blocks.39.ffn.fc2.weight, module.blocks.39.ca_norm.weight, module.blocks.40.sa.mat_qkv.weight, module.blocks.40.sa.proj.weight, '\n"
                   " 'module.blocks.40.ca.mat_q.weight, module.blocks.40.ca.mat_kv.weight, module.blocks.40.ca.proj.weight, module.blocks.40.ffn.fc1.weight, module.blocks.40.ffn.fc2.weight, '\n"
                   " 'module.blocks.40.ca_norm.weight, module.blocks.41.sa.mat_qkv.weight, module.blocks.41.sa.proj.weight, module.blocks.41.ca.mat_q.weight, module.blocks.41.ca.mat_kv.weight, '\n"
                   " 'module.blocks.41.ca.proj.weight, module.blocks.41.ffn.fc1.weight, module.blocks.41.ffn.fc2.weight, module.blocks.41.ca_norm.weight, module.blocks.42.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.42.sa.proj.weight, module.blocks.42.ca.mat_q.weight, module.blocks.42.ca.mat_kv.weight, module.blocks.42.ca.proj.weight, module.blocks.42.ffn.fc1.weight, '\n"
                   " 'module.blocks.42.ffn.fc2.weight, module.blocks.42.ca_norm.weight, module.blocks.43.sa.mat_qkv.weight, module.blocks.43.sa.proj.weight, module.blocks.43.ca.mat_q.weight, '\n"
                   " 'module.blocks.43.ca.mat_kv.weight, module.blocks.43.ca.proj.weight, module.blocks.43.ffn.fc1.weight, module.blocks.43.ffn.fc2.weight, module.blocks.43.ca_norm.weight, '\n"
                   " 'module.blocks.44.sa.mat_qkv.weight, module.blocks.44.sa.proj.weight, module.blocks.44.ca.mat_q.weight, module.blocks.44.ca.mat_kv.weight, module.blocks.44.ca.proj.weight, '\n"
                   " 'module.blocks.44.ffn.fc1.weight, module.blocks.44.ffn.fc2.weight, module.blocks.44.ca_norm.weight, module.blocks.45.sa.mat_qkv.weight, module.blocks.45.sa.proj.weight, '\n"
                   " 'module.blocks.45.ca.mat_q.weight, module.blocks.45.ca.mat_kv.weight, module.blocks.45.ca.proj.weight, module.blocks.45.ffn.fc1.weight, module.blocks.45.ffn.fc2.weight, '\n"
                   " 'module.blocks.45.ca_norm.weight, module.blocks.46.sa.mat_qkv.weight, module.blocks.46.sa.proj.weight, module.blocks.46.ca.mat_q.weight, module.blocks.46.ca.mat_kv.weight, '\n"
                   " 'module.blocks.46.ca.proj.weight, module.blocks.46.ffn.fc1.weight, module.blocks.46.ffn.fc2.weight, module.blocks.46.ca_norm.weight, module.blocks.47.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.47.sa.proj.weight, module.blocks.47.ca.mat_q.weight, module.blocks.47.ca.mat_kv.weight, module.blocks.47.ca.proj.weight, module.blocks.47.ffn.fc1.weight, '\n"
                   " 'module.blocks.47.ffn.fc2.weight, module.blocks.47.ca_norm.weight, module.blocks.48.sa.mat_qkv.weight, module.blocks.48.sa.proj.weight, module.blocks.48.ca.mat_q.weight, '\n"
                   " 'module.blocks.48.ca.mat_kv.weight, module.blocks.48.ca.proj.weight, module.blocks.48.ffn.fc1.weight, module.blocks.48.ffn.fc2.weight, module.blocks.48.ca_norm.weight, '\n"
                   " 'module.blocks.49.sa.mat_qkv.weight, module.blocks.49.sa.proj.weight, module.blocks.49.ca.mat_q.weight, module.blocks.49.ca.mat_kv.weight, module.blocks.49.ca.proj.weight, '\n"
                   " 'module.blocks.49.ffn.fc1.weight, module.blocks.49.ffn.fc2.weight, module.blocks.49.ca_norm.weight, module.blocks.50.sa.mat_qkv.weight, module.blocks.50.sa.proj.weight, '\n"
                   " 'module.blocks.50.ca.mat_q.weight, module.blocks.50.ca.mat_kv.weight, module.blocks.50.ca.proj.weight, module.blocks.50.ffn.fc1.weight, module.blocks.50.ffn.fc2.weight, '\n"
                   " 'module.blocks.50.ca_norm.weight, module.blocks.51.sa.mat_qkv.weight, module.blocks.51.sa.proj.weight, module.blocks.51.ca.mat_q.weight, module.blocks.51.ca.mat_kv.weight, '\n"
                   " 'module.blocks.51.ca.proj.weight, module.blocks.51.ffn.fc1.weight, module.blocks.51.ffn.fc2.weight, module.blocks.51.ca_norm.weight, module.blocks.52.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.52.sa.proj.weight, module.blocks.52.ca.mat_q.weight, module.blocks.52.ca.mat_kv.weight, module.blocks.52.ca.proj.weight, module.blocks.52.ffn.fc1.weight, '\n"
                   " 'module.blocks.52.ffn.fc2.weight, module.blocks.52.ca_norm.weight, module.blocks.53.sa.mat_qkv.weight, module.blocks.53.sa.proj.weight, module.blocks.53.ca.mat_q.weight, '\n"
                   " 'module.blocks.53.ca.mat_kv.weight, module.blocks.53.ca.proj.weight, module.blocks.53.ffn.fc1.weight, module.blocks.53.ffn.fc2.weight, module.blocks.53.ca_norm.weight, '\n"
                   " 'module.blocks.54.sa.mat_qkv.weight, module.blocks.54.sa.proj.weight, module.blocks.54.ca.mat_q.weight, module.blocks.54.ca.mat_kv.weight, module.blocks.54.ca.proj.weight, '\n"
                   " 'module.blocks.54.ffn.fc1.weight, module.blocks.54.ffn.fc2.weight, module.blocks.54.ca_norm.weight, module.blocks.55.sa.mat_qkv.weight, module.blocks.55.sa.proj.weight, '\n"
                   " 'module.blocks.55.ca.mat_q.weight, module.blocks.55.ca.mat_kv.weight, module.blocks.55.ca.proj.weight, module.blocks.55.ffn.fc1.weight, module.blocks.55.ffn.fc2.weight, '\n"
                   " 'module.blocks.55.ca_norm.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': '[no scale]',
          'params': "('module.cfg_uncond, module.pos_start, module.text_proj_for_sos.ca.mat_q, module.text_proj_for_sos.ca.v_bias, module.text_proj_for_sos.ca.proj.bias, module.text_proj_for_ca.0.bias, '\n"
                    " 'module.text_proj_for_ca.2.bias, module.lvl_embed.weight, module.word_embed.bias, module.shared_ada_lin.1.bias, module.head_nm.ada_lin.1.bias, module.head.bias, module.blocks.0.ada_gss, '\n"
                    " 'module.blocks.0.sa.scale_mul_1H11, module.blocks.0.sa.q_bias, module.blocks.0.sa.v_bias, module.blocks.0.sa.proj.bias, module.blocks.0.ca.v_bias, module.blocks.0.ca.mat_q.bias, '\n"
                    " 'module.blocks.0.ca.proj.bias, module.blocks.0.ffn.fc1.bias, module.blocks.0.ffn.fc2.bias, module.blocks.0.ca_norm.bias, module.blocks.1.ada_gss, module.blocks.1.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.1.sa.q_bias, module.blocks.1.sa.v_bias, module.blocks.1.sa.proj.bias, module.blocks.1.ca.v_bias, module.blocks.1.ca.mat_q.bias, module.blocks.1.ca.proj.bias, '\n"
                    " 'module.blocks.1.ffn.fc1.bias, module.blocks.1.ffn.fc2.bias, module.blocks.1.ca_norm.bias, module.blocks.2.ada_gss, module.blocks.2.sa.scale_mul_1H11, module.blocks.2.sa.q_bias, '\n"
                    " 'module.blocks.2.sa.v_bias, module.blocks.2.sa.proj.bias, module.blocks.2.ca.v_bias, module.blocks.2.ca.mat_q.bias, module.blocks.2.ca.proj.bias, module.blocks.2.ffn.fc1.bias, '\n"
                    " 'module.blocks.2.ffn.fc2.bias, module.blocks.2.ca_norm.bias, module.blocks.3.ada_gss, module.blocks.3.sa.scale_mul_1H11, module.blocks.3.sa.q_bias, module.blocks.3.sa.v_bias, '\n"
                    " 'module.blocks.3.sa.proj.bias, module.blocks.3.ca.v_bias, module.blocks.3.ca.mat_q.bias, module.blocks.3.ca.proj.bias, module.blocks.3.ffn.fc1.bias, module.blocks.3.ffn.fc2.bias, '\n"
                    " 'module.blocks.3.ca_norm.bias, module.blocks.4.ada_gss, module.blocks.4.sa.scale_mul_1H11, module.blocks.4.sa.q_bias, module.blocks.4.sa.v_bias, module.blocks.4.sa.proj.bias, '\n"
                    " 'module.blocks.4.ca.v_bias, module.blocks.4.ca.mat_q.bias, module.blocks.4.ca.proj.bias, module.blocks.4.ffn.fc1.bias, module.blocks.4.ffn.fc2.bias, module.blocks.4.ca_norm.bias, '\n"
                    " 'module.blocks.5.ada_gss, module.blocks.5.sa.scale_mul_1H11, module.blocks.5.sa.q_bias, module.blocks.5.sa.v_bias, module.blocks.5.sa.proj.bias, module.blocks.5.ca.v_bias, '\n"
                    " 'module.blocks.5.ca.mat_q.bias, module.blocks.5.ca.proj.bias, module.blocks.5.ffn.fc1.bias, module.blocks.5.ffn.fc2.bias, module.blocks.5.ca_norm.bias, module.blocks.6.ada_gss, '\n"
                    " 'module.blocks.6.sa.scale_mul_1H11, module.blocks.6.sa.q_bias, module.blocks.6.sa.v_bias, module.blocks.6.sa.proj.bias, module.blocks.6.ca.v_bias, module.blocks.6.ca.mat_q.bias, '\n"
                    " 'module.blocks.6.ca.proj.bias, module.blocks.6.ffn.fc1.bias, module.blocks.6.ffn.fc2.bias, module.blocks.6.ca_norm.bias, module.blocks.7.ada_gss, module.blocks.7.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.7.sa.q_bias, module.blocks.7.sa.v_bias, module.blocks.7.sa.proj.bias, module.blocks.7.ca.v_bias, module.blocks.7.ca.mat_q.bias, module.blocks.7.ca.proj.bias, '\n"
                    " 'module.blocks.7.ffn.fc1.bias, module.blocks.7.ffn.fc2.bias, module.blocks.7.ca_norm.bias, module.blocks.8.ada_gss, module.blocks.8.sa.scale_mul_1H11, module.blocks.8.sa.q_bias, '\n"
                    " 'module.blocks.8.sa.v_bias, module.blocks.8.sa.proj.bias, module.blocks.8.ca.v_bias, module.blocks.8.ca.mat_q.bias, module.blocks.8.ca.proj.bias, module.blocks.8.ffn.fc1.bias, '\n"
                    " 'module.blocks.8.ffn.fc2.bias, module.blocks.8.ca_norm.bias, module.blocks.9.ada_gss, module.blocks.9.sa.scale_mul_1H11, module.blocks.9.sa.q_bias, module.blocks.9.sa.v_bias, '\n"
                    " 'module.blocks.9.sa.proj.bias, module.blocks.9.ca.v_bias, module.blocks.9.ca.mat_q.bias, module.blocks.9.ca.proj.bias, module.blocks.9.ffn.fc1.bias, module.blocks.9.ffn.fc2.bias, '\n"
                    " 'module.blocks.9.ca_norm.bias, module.blocks.10.ada_gss, module.blocks.10.sa.scale_mul_1H11, module.blocks.10.sa.q_bias, module.blocks.10.sa.v_bias, module.blocks.10.sa.proj.bias, '\n"
                    " 'module.blocks.10.ca.v_bias, module.blocks.10.ca.mat_q.bias, module.blocks.10.ca.proj.bias, module.blocks.10.ffn.fc1.bias, module.blocks.10.ffn.fc2.bias, module.blocks.10.ca_norm.bias, '\n"
                    " 'module.blocks.11.ada_gss, module.blocks.11.sa.scale_mul_1H11, module.blocks.11.sa.q_bias, module.blocks.11.sa.v_bias, module.blocks.11.sa.proj.bias, module.blocks.11.ca.v_bias, '\n"
                    " 'module.blocks.11.ca.mat_q.bias, module.blocks.11.ca.proj.bias, module.blocks.11.ffn.fc1.bias, module.blocks.11.ffn.fc2.bias, module.blocks.11.ca_norm.bias, module.blocks.12.ada_gss, '\n"
                    " 'module.blocks.12.sa.scale_mul_1H11, module.blocks.12.sa.q_bias, module.blocks.12.sa.v_bias, module.blocks.12.sa.proj.bias, module.blocks.12.ca.v_bias, module.blocks.12.ca.mat_q.bias, '\n"
                    " 'module.blocks.12.ca.proj.bias, module.blocks.12.ffn.fc1.bias, module.blocks.12.ffn.fc2.bias, module.blocks.12.ca_norm.bias, module.blocks.13.ada_gss, module.blocks.13.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.13.sa.q_bias, module.blocks.13.sa.v_bias, module.blocks.13.sa.proj.bias, module.blocks.13.ca.v_bias, module.blocks.13.ca.mat_q.bias, module.blocks.13.ca.proj.bias, '\n"
                    " 'module.blocks.13.ffn.fc1.bias, module.blocks.13.ffn.fc2.bias, module.blocks.13.ca_norm.bias, module.blocks.14.ada_gss, module.blocks.14.sa.scale_mul_1H11, module.blocks.14.sa.q_bias, '\n"
                    " 'module.blocks.14.sa.v_bias, module.blocks.14.sa.proj.bias, module.blocks.14.ca.v_bias, module.blocks.14.ca.mat_q.bias, module.blocks.14.ca.proj.bias, module.blocks.14.ffn.fc1.bias, '\n"
                    " 'module.blocks.14.ffn.fc2.bias, module.blocks.14.ca_norm.bias, module.blocks.15.ada_gss, module.blocks.15.sa.scale_mul_1H11, module.blocks.15.sa.q_bias, module.blocks.15.sa.v_bias, '\n"
                    " 'module.blocks.15.sa.proj.bias, module.blocks.15.ca.v_bias, module.blocks.15.ca.mat_q.bias, module.blocks.15.ca.proj.bias, module.blocks.15.ffn.fc1.bias, module.blocks.15.ffn.fc2.bias, '\n"
                    " 'module.blocks.15.ca_norm.bias, module.blocks.16.ada_gss, module.blocks.16.sa.scale_mul_1H11, module.blocks.16.sa.q_bias, module.blocks.16.sa.v_bias, module.blocks.16.sa.proj.bias, '\n"
                    " 'module.blocks.16.ca.v_bias, module.blocks.16.ca.mat_q.bias, module.blocks.16.ca.proj.bias, module.blocks.16.ffn.fc1.bias, module.blocks.16.ffn.fc2.bias, module.blocks.16.ca_norm.bias, '\n"
                    " 'module.blocks.17.ada_gss, module.blocks.17.sa.scale_mul_1H11, module.blocks.17.sa.q_bias, module.blocks.17.sa.v_bias, module.blocks.17.sa.proj.bias, module.blocks.17.ca.v_bias, '\n"
                    " 'module.blocks.17.ca.mat_q.bias, module.blocks.17.ca.proj.bias, module.blocks.17.ffn.fc1.bias, module.blocks.17.ffn.fc2.bias, module.blocks.17.ca_norm.bias, module.blocks.18.ada_gss, '\n"
                    " 'module.blocks.18.sa.scale_mul_1H11, module.blocks.18.sa.q_bias, module.blocks.18.sa.v_bias, module.blocks.18.sa.proj.bias, module.blocks.18.ca.v_bias, module.blocks.18.ca.mat_q.bias, '\n"
                    " 'module.blocks.18.ca.proj.bias, module.blocks.18.ffn.fc1.bias, module.blocks.18.ffn.fc2.bias, module.blocks.18.ca_norm.bias, module.blocks.19.ada_gss, module.blocks.19.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.19.sa.q_bias, module.blocks.19.sa.v_bias, module.blocks.19.sa.proj.bias, module.blocks.19.ca.v_bias, module.blocks.19.ca.mat_q.bias, module.blocks.19.ca.proj.bias, '\n"
                    " 'module.blocks.19.ffn.fc1.bias, module.blocks.19.ffn.fc2.bias, module.blocks.19.ca_norm.bias, module.blocks.20.ada_gss, module.blocks.20.sa.scale_mul_1H11, module.blocks.20.sa.q_bias, '\n"
                    " 'module.blocks.20.sa.v_bias, module.blocks.20.sa.proj.bias, module.blocks.20.ca.v_bias, module.blocks.20.ca.mat_q.bias, module.blocks.20.ca.proj.bias, module.blocks.20.ffn.fc1.bias, '\n"
                    " 'module.blocks.20.ffn.fc2.bias, module.blocks.20.ca_norm.bias, module.blocks.21.ada_gss, module.blocks.21.sa.scale_mul_1H11, module.blocks.21.sa.q_bias, module.blocks.21.sa.v_bias, '\n"
                    " 'module.blocks.21.sa.proj.bias, module.blocks.21.ca.v_bias, module.blocks.21.ca.mat_q.bias, module.blocks.21.ca.proj.bias, module.blocks.21.ffn.fc1.bias, module.blocks.21.ffn.fc2.bias, '\n"
                    " 'module.blocks.21.ca_norm.bias, module.blocks.22.ada_gss, module.blocks.22.sa.scale_mul_1H11, module.blocks.22.sa.q_bias, module.blocks.22.sa.v_bias, module.blocks.22.sa.proj.bias, '\n"
                    " 'module.blocks.22.ca.v_bias, module.blocks.22.ca.mat_q.bias, module.blocks.22.ca.proj.bias, module.blocks.22.ffn.fc1.bias, module.blocks.22.ffn.fc2.bias, module.blocks.22.ca_norm.bias, '\n"
                    " 'module.blocks.23.ada_gss, module.blocks.23.sa.scale_mul_1H11, module.blocks.23.sa.q_bias, module.blocks.23.sa.v_bias, module.blocks.23.sa.proj.bias, module.blocks.23.ca.v_bias, '\n"
                    " 'module.blocks.23.ca.mat_q.bias, module.blocks.23.ca.proj.bias, module.blocks.23.ffn.fc1.bias, module.blocks.23.ffn.fc2.bias, module.blocks.23.ca_norm.bias, module.blocks.24.ada_gss, '\n"
                    " 'module.blocks.24.sa.scale_mul_1H11, module.blocks.24.sa.q_bias, module.blocks.24.sa.v_bias, module.blocks.24.sa.proj.bias, module.blocks.24.ca.v_bias, module.blocks.24.ca.mat_q.bias, '\n"
                    " 'module.blocks.24.ca.proj.bias, module.blocks.24.ffn.fc1.bias, module.blocks.24.ffn.fc2.bias, module.blocks.24.ca_norm.bias, module.blocks.25.ada_gss, module.blocks.25.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.25.sa.q_bias, module.blocks.25.sa.v_bias, module.blocks.25.sa.proj.bias, module.blocks.25.ca.v_bias, module.blocks.25.ca.mat_q.bias, module.blocks.25.ca.proj.bias, '\n"
                    " 'module.blocks.25.ffn.fc1.bias, module.blocks.25.ffn.fc2.bias, module.blocks.25.ca_norm.bias, module.blocks.26.ada_gss, module.blocks.26.sa.scale_mul_1H11, module.blocks.26.sa.q_bias, '\n"
                    " 'module.blocks.26.sa.v_bias, module.blocks.26.sa.proj.bias, module.blocks.26.ca.v_bias, module.blocks.26.ca.mat_q.bias, module.blocks.26.ca.proj.bias, module.blocks.26.ffn.fc1.bias, '\n"
                    " 'module.blocks.26.ffn.fc2.bias, module.blocks.26.ca_norm.bias, module.blocks.27.ada_gss, module.blocks.27.sa.scale_mul_1H11, module.blocks.27.sa.q_bias, module.blocks.27.sa.v_bias, '\n"
                    " 'module.blocks.27.sa.proj.bias, module.blocks.27.ca.v_bias, module.blocks.27.ca.mat_q.bias, module.blocks.27.ca.proj.bias, module.blocks.27.ffn.fc1.bias, module.blocks.27.ffn.fc2.bias, '\n"
                    " 'module.blocks.27.ca_norm.bias, module.blocks.28.ada_gss, module.blocks.28.sa.scale_mul_1H11, module.blocks.28.sa.q_bias, module.blocks.28.sa.v_bias, module.blocks.28.sa.proj.bias, '\n"
                    " 'module.blocks.28.ca.v_bias, module.blocks.28.ca.mat_q.bias, module.blocks.28.ca.proj.bias, module.blocks.28.ffn.fc1.bias, module.blocks.28.ffn.fc2.bias, module.blocks.28.ca_norm.bias, '\n"
                    " 'module.blocks.29.ada_gss, module.blocks.29.sa.scale_mul_1H11, module.blocks.29.sa.q_bias, module.blocks.29.sa.v_bias, module.blocks.29.sa.proj.bias, module.blocks.29.ca.v_bias, '\n"
                    " 'module.blocks.29.ca.mat_q.bias, module.blocks.29.ca.proj.bias, module.blocks.29.ffn.fc1.bias, module.blocks.29.ffn.fc2.bias, module.blocks.29.ca_norm.bias, module.blocks.30.ada_gss, '\n"
                    " 'module.blocks.30.sa.scale_mul_1H11, module.blocks.30.sa.q_bias, module.blocks.30.sa.v_bias, module.blocks.30.sa.proj.bias, module.blocks.30.ca.v_bias, module.blocks.30.ca.mat_q.bias, '\n"
                    " 'module.blocks.30.ca.proj.bias, module.blocks.30.ffn.fc1.bias, module.blocks.30.ffn.fc2.bias, module.blocks.30.ca_norm.bias, module.blocks.31.ada_gss, module.blocks.31.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.31.sa.q_bias, module.blocks.31.sa.v_bias, module.blocks.31.sa.proj.bias, module.blocks.31.ca.v_bias, module.blocks.31.ca.mat_q.bias, module.blocks.31.ca.proj.bias, '\n"
                    " 'module.blocks.31.ffn.fc1.bias, module.blocks.31.ffn.fc2.bias, module.blocks.31.ca_norm.bias, module.blocks.32.ada_gss, module.blocks.32.sa.scale_mul_1H11, module.blocks.32.sa.q_bias, '\n"
                    " 'module.blocks.32.sa.v_bias, module.blocks.32.sa.proj.bias, module.blocks.32.ca.v_bias, module.blocks.32.ca.mat_q.bias, module.blocks.32.ca.proj.bias, module.blocks.32.ffn.fc1.bias, '\n"
                    " 'module.blocks.32.ffn.fc2.bias, module.blocks.32.ca_norm.bias, module.blocks.33.ada_gss, module.blocks.33.sa.scale_mul_1H11, module.blocks.33.sa.q_bias, module.blocks.33.sa.v_bias, '\n"
                    " 'module.blocks.33.sa.proj.bias, module.blocks.33.ca.v_bias, module.blocks.33.ca.mat_q.bias, module.blocks.33.ca.proj.bias, module.blocks.33.ffn.fc1.bias, module.blocks.33.ffn.fc2.bias, '\n"
                    " 'module.blocks.33.ca_norm.bias, module.blocks.34.ada_gss, module.blocks.34.sa.scale_mul_1H11, module.blocks.34.sa.q_bias, module.blocks.34.sa.v_bias, module.blocks.34.sa.proj.bias, '\n"
                    " 'module.blocks.34.ca.v_bias, module.blocks.34.ca.mat_q.bias, module.blocks.34.ca.proj.bias, module.blocks.34.ffn.fc1.bias, module.blocks.34.ffn.fc2.bias, module.blocks.34.ca_norm.bias, '\n"
                    " 'module.blocks.35.ada_gss, module.blocks.35.sa.scale_mul_1H11, module.blocks.35.sa.q_bias, module.blocks.35.sa.v_bias, module.blocks.35.sa.proj.bias, module.blocks.35.ca.v_bias, '\n"
                    " 'module.blocks.35.ca.mat_q.bias, module.blocks.35.ca.proj.bias, module.blocks.35.ffn.fc1.bias, module.blocks.35.ffn.fc2.bias, module.blocks.35.ca_norm.bias, module.blocks.36.ada_gss, '\n"
                    " 'module.blocks.36.sa.scale_mul_1H11, module.blocks.36.sa.q_bias, module.blocks.36.sa.v_bias, module.blocks.36.sa.proj.bias, module.blocks.36.ca.v_bias, module.blocks.36.ca.mat_q.bias, '\n"
                    " 'module.blocks.36.ca.proj.bias, module.blocks.36.ffn.fc1.bias, module.blocks.36.ffn.fc2.bias, module.blocks.36.ca_norm.bias, module.blocks.37.ada_gss, module.blocks.37.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.37.sa.q_bias, module.blocks.37.sa.v_bias, module.blocks.37.sa.proj.bias, module.blocks.37.ca.v_bias, module.blocks.37.ca.mat_q.bias, module.blocks.37.ca.proj.bias, '\n"
                    " 'module.blocks.37.ffn.fc1.bias, module.blocks.37.ffn.fc2.bias, module.blocks.37.ca_norm.bias, module.blocks.38.ada_gss, module.blocks.38.sa.scale_mul_1H11, module.blocks.38.sa.q_bias, '\n"
                    " 'module.blocks.38.sa.v_bias, module.blocks.38.sa.proj.bias, module.blocks.38.ca.v_bias, module.blocks.38.ca.mat_q.bias, module.blocks.38.ca.proj.bias, module.blocks.38.ffn.fc1.bias, '\n"
                    " 'module.blocks.38.ffn.fc2.bias, module.blocks.38.ca_norm.bias, module.blocks.39.ada_gss, module.blocks.39.sa.scale_mul_1H11, module.blocks.39.sa.q_bias, module.blocks.39.sa.v_bias, '\n"
                    " 'module.blocks.39.sa.proj.bias, module.blocks.39.ca.v_bias, module.blocks.39.ca.mat_q.bias, module.blocks.39.ca.proj.bias, module.blocks.39.ffn.fc1.bias, module.blocks.39.ffn.fc2.bias, '\n"
                    " 'module.blocks.39.ca_norm.bias, module.blocks.40.ada_gss, module.blocks.40.sa.scale_mul_1H11, module.blocks.40.sa.q_bias, module.blocks.40.sa.v_bias, module.blocks.40.sa.proj.bias, '\n"
                    " 'module.blocks.40.ca.v_bias, module.blocks.40.ca.mat_q.bias, module.blocks.40.ca.proj.bias, module.blocks.40.ffn.fc1.bias, module.blocks.40.ffn.fc2.bias, module.blocks.40.ca_norm.bias, '\n"
                    " 'module.blocks.41.ada_gss, module.blocks.41.sa.scale_mul_1H11, module.blocks.41.sa.q_bias, module.blocks.41.sa.v_bias, module.blocks.41.sa.proj.bias, module.blocks.41.ca.v_bias, '\n"
                    " 'module.blocks.41.ca.mat_q.bias, module.blocks.41.ca.proj.bias, module.blocks.41.ffn.fc1.bias, module.blocks.41.ffn.fc2.bias, module.blocks.41.ca_norm.bias, module.blocks.42.ada_gss, '\n"
                    " 'module.blocks.42.sa.scale_mul_1H11, module.blocks.42.sa.q_bias, module.blocks.42.sa.v_bias, module.blocks.42.sa.proj.bias, module.blocks.42.ca.v_bias, module.blocks.42.ca.mat_q.bias, '\n"
                    " 'module.blocks.42.ca.proj.bias, module.blocks.42.ffn.fc1.bias, module.blocks.42.ffn.fc2.bias, module.blocks.42.ca_norm.bias, module.blocks.43.ada_gss, module.blocks.43.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.43.sa.q_bias, module.blocks.43.sa.v_bias, module.blocks.43.sa.proj.bias, module.blocks.43.ca.v_bias, module.blocks.43.ca.mat_q.bias, module.blocks.43.ca.proj.bias, '\n"
                    " 'module.blocks.43.ffn.fc1.bias, module.blocks.43.ffn.fc2.bias, module.blocks.43.ca_norm.bias, module.blocks.44.ada_gss, module.blocks.44.sa.scale_mul_1H11, module.blocks.44.sa.q_bias, '\n"
                    " 'module.blocks.44.sa.v_bias, module.blocks.44.sa.proj.bias, module.blocks.44.ca.v_bias, module.blocks.44.ca.mat_q.bias, module.blocks.44.ca.proj.bias, module.blocks.44.ffn.fc1.bias, '\n"
                    " 'module.blocks.44.ffn.fc2.bias, module.blocks.44.ca_norm.bias, module.blocks.45.ada_gss, module.blocks.45.sa.scale_mul_1H11, module.blocks.45.sa.q_bias, module.blocks.45.sa.v_bias, '\n"
                    " 'module.blocks.45.sa.proj.bias, module.blocks.45.ca.v_bias, module.blocks.45.ca.mat_q.bias, module.blocks.45.ca.proj.bias, module.blocks.45.ffn.fc1.bias, module.blocks.45.ffn.fc2.bias, '\n"
                    " 'module.blocks.45.ca_norm.bias, module.blocks.46.ada_gss, module.blocks.46.sa.scale_mul_1H11, module.blocks.46.sa.q_bias, module.blocks.46.sa.v_bias, module.blocks.46.sa.proj.bias, '\n"
                    " 'module.blocks.46.ca.v_bias, module.blocks.46.ca.mat_q.bias, module.blocks.46.ca.proj.bias, module.blocks.46.ffn.fc1.bias, module.blocks.46.ffn.fc2.bias, module.blocks.46.ca_norm.bias, '\n"
                    " 'module.blocks.47.ada_gss, module.blocks.47.sa.scale_mul_1H11, module.blocks.47.sa.q_bias, module.blocks.47.sa.v_bias, module.blocks.47.sa.proj.bias, module.blocks.47.ca.v_bias, '\n"
                    " 'module.blocks.47.ca.mat_q.bias, module.blocks.47.ca.proj.bias, module.blocks.47.ffn.fc1.bias, module.blocks.47.ffn.fc2.bias, module.blocks.47.ca_norm.bias, module.blocks.48.ada_gss, '\n"
                    " 'module.blocks.48.sa.scale_mul_1H11, module.blocks.48.sa.q_bias, module.blocks.48.sa.v_bias, module.blocks.48.sa.proj.bias, module.blocks.48.ca.v_bias, module.blocks.48.ca.mat_q.bias, '\n"
                    " 'module.blocks.48.ca.proj.bias, module.blocks.48.ffn.fc1.bias, module.blocks.48.ffn.fc2.bias, module.blocks.48.ca_norm.bias, module.blocks.49.ada_gss, module.blocks.49.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.49.sa.q_bias, module.blocks.49.sa.v_bias, module.blocks.49.sa.proj.bias, module.blocks.49.ca.v_bias, module.blocks.49.ca.mat_q.bias, module.blocks.49.ca.proj.bias, '\n"
                    " 'module.blocks.49.ffn.fc1.bias, module.blocks.49.ffn.fc2.bias, module.blocks.49.ca_norm.bias, module.blocks.50.ada_gss, module.blocks.50.sa.scale_mul_1H11, module.blocks.50.sa.q_bias, '\n"
                    " 'module.blocks.50.sa.v_bias, module.blocks.50.sa.proj.bias, module.blocks.50.ca.v_bias, module.blocks.50.ca.mat_q.bias, module.blocks.50.ca.proj.bias, module.blocks.50.ffn.fc1.bias, '\n"
                    " 'module.blocks.50.ffn.fc2.bias, module.blocks.50.ca_norm.bias, module.blocks.51.ada_gss, module.blocks.51.sa.scale_mul_1H11, module.blocks.51.sa.q_bias, module.blocks.51.sa.v_bias, '\n"
                    " 'module.blocks.51.sa.proj.bias, module.blocks.51.ca.v_bias, module.blocks.51.ca.mat_q.bias, module.blocks.51.ca.proj.bias, module.blocks.51.ffn.fc1.bias, module.blocks.51.ffn.fc2.bias, '\n"
                    " 'module.blocks.51.ca_norm.bias, module.blocks.52.ada_gss, module.blocks.52.sa.scale_mul_1H11, module.blocks.52.sa.q_bias, module.blocks.52.sa.v_bias, module.blocks.52.sa.proj.bias, '\n"
                    " 'module.blocks.52.ca.v_bias, module.blocks.52.ca.mat_q.bias, module.blocks.52.ca.proj.bias, module.blocks.52.ffn.fc1.bias, module.blocks.52.ffn.fc2.bias, module.blocks.52.ca_norm.bias, '\n"
                    " 'module.blocks.53.ada_gss, module.blocks.53.sa.scale_mul_1H11, module.blocks.53.sa.q_bias, module.blocks.53.sa.v_bias, module.blocks.53.sa.proj.bias, module.blocks.53.ca.v_bias, '\n"
                    " 'module.blocks.53.ca.mat_q.bias, module.blocks.53.ca.proj.bias, module.blocks.53.ffn.fc1.bias, module.blocks.53.ffn.fc2.bias, module.blocks.53.ca_norm.bias, module.blocks.54.ada_gss, '\n"
                    " 'module.blocks.54.sa.scale_mul_1H11, module.blocks.54.sa.q_bias, module.blocks.54.sa.v_bias, module.blocks.54.sa.proj.bias, module.blocks.54.ca.v_bias, module.blocks.54.ca.mat_q.bias, '\n"
                    " 'module.blocks.54.ca.proj.bias, module.blocks.54.ffn.fc1.bias, module.blocks.54.ffn.fc2.bias, module.blocks.54.ca_norm.bias, module.blocks.55.ada_gss, module.blocks.55.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.55.sa.q_bias, module.blocks.55.sa.v_bias, module.blocks.55.sa.proj.bias, module.blocks.55.ca.v_bias, module.blocks.55.ca.mat_q.bias, module.blocks.55.ca.proj.bias, '\n"
                    " 'module.blocks.55.ffn.fc1.bias, module.blocks.55.ffn.fc2.bias, module.blocks.55.ca_norm.bias')",
          'wd_sc': 0.0}}

[09-01 23:06:15] (nity/utils/lr_control.py, line 116)=> [get_param_groups][rank0] type(model).__name__='NullDDP' count=1085, numel=8573892016
[09-01 23:06:15] (nity/utils/lr_control.py, line 117)=> 
[09-01 23:06:15] (inity_circuit_breaker.py, line 343)=> [vgpt] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.97), fused=True), opt_kw={'lr': 9.375e-05, 'weight_decay': 0}

[09-01 23:06:15] (inity_circuit_breaker.py, line 348)=> Loading T5 from google/flan-t5-xl...
[09-01 23:06:17] (_original/src/trainer.py, line  65)=> self.reweight_loss_by_scale: 1
[09-01 23:06:17] (inity_circuit_breaker.py, line 171)=> global bs=4, local bs=4
[09-01 23:06:17] (inity_circuit_breaker.py, line 172)=> initial args:
{
  local_out_path      : ./local_output
  data_path           : ./data
  bed                 : ./checkpoints
  vae_ckpt            : /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
  exp_name            : experiment
  ds                  : oi
  model               : infinity_8b
  short_cap_prob      : 0.2
  project_name        : infinity_circuit_breaker
  tf32                : True
  auto_resume         : True
  rush_resume         : 
  nowd                : 1
  enable_hybrid_shard : False
  inner_shard_degree  : 1
  zero                : 2
  buck                : chunk
  fsdp_orig           : True
  enable_checkpointing: full-block
  pad_to_multiplier   : 128
  log_every_iter      : False
  checkpoint_type     : torch
  seed                : None
  rand                : True
  task_id             : 123
  trial_id            : 123
  robust_run_id       : 0
  ckpt_trials         : []
  real_trial_id       : 123
  chunk_nodes         : 0
  is_master_node      : None
  log_txt_path        : ./local_output/log.txt
  t5_path             : google/flan-t5-xl
  online_t5           : True
  sdpa_mem            : True
  tfast               : 0
  model_alias         : 8b
  rms                 : False
  aln                 : 0.001
  alng                : 5e-06
  saln                : True
  haln                : True
  nm0                 : False
  tau                 : 1
  cos                 : True
  swi                 : False
  dp                  : -1
  drop                : 0.0
  hd                  : 0
  ca_gamma            : -1
  diva                : 1
  hd0                 : 0.02
  dec                 : 1
  cum                 : 3
  rwe                 : False
  tp                  : 0.0
  tk                  : 0.0
  tini                : 0.010416666666666666
  cfg                 : 0.1
  rand_uncond         : False
  ema                 : 0.9999
  tema                : 0
  fp16                : 2
  fuse                : False
  fused_norm          : False
  flash               : False
  xen                 : False
  use_flex_attn       : True
  stable              : False
  gblr                : 0.0001
  dblr                : 0.0001
  tblr                : 0.006
  glr                 : 1.5625e-06
  dlr                 : 1.5625e-06
  tlr                 : 9.375e-05
  gwd                 : 0.005
  dwd                 : 0.0005
  twd                 : 0.005
  gwde                : 0.005
  dwde                : 0.0005
  twde                : 0.005
  ls                  : 0.0
  lz                  : 0.0
  eq                  : 0
  ep                  : 100
  wp                  : 1e-08
  wp0                 : 0.005
  wpe                 : 1.0
  sche                : lin0
  log_freq            : 50
  gclip               : 6.0
  dclip               : 6.0
  tclip               : 5.0
  cdec                : False
  opt                 : adamw
  ada                 : 0.9_0.97
  dada                : 0.9_0.97
  oeps                : 0
  afuse               : True
  pn                  : 0.06M
  scale_schedule      : None
  patch_size          : None
  resos               : None
  data_load_reso      : None
  workers             : 4
  lbs                 : 4
  bs                  : 4
  batch_size          : 4
  glb_batch_size      : 4
  ac                  : 1
  r_accu              : 1.0
  norm_eps            : 1e-06
  tlen                : 512
  Ct5                 : 2048
  use_bit_label       : 1
  bitloss_type        : mean
  dynamic_resolution_across_gpus: 1
  enable_dynamic_length_prompt: 1
  use_streaming_dataset: 1
  iterable_data_buffersize: 30000
  save_model_iters_freq: 100
  noise_apply_layers  : 13
  noise_apply_strength: 0.3
  noise_apply_requant : 1
  rope2d_each_sa_layer: 1
  rope2d_normalized_by_hw: 2
  use_fsdp_model_ema  : 0
  add_lvl_embeding_only_first_block: 1
  reweight_loss_by_scale: 1
  always_training_scales: 100
  vae_type            : 14
  fake_vae_input      : False
  model_init_device   : cuda
  prefetch_factor     : 2
  apply_spatial_patchify: 1
  debug_bsc           : 0
  task_type           : t2i
  target_layers       : [10, 12, 14, 16, 18, 20]
  transform_layers    : [10, 12, 14, 16, 18, 20]
  lorra_alpha         : 5.0
  trainsets           : ['AlpacaSupervisedDataset', 'HarmfulDataset']
  valsets             : ['AlpacaSupervisedDataset', 'HarmfulDataset']
  adv_string          : Sure here's
  full_layers         : False
  harmful_prompts_path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
  sanitized_prompts_path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
  validation_ratio    : 0.1
  category            : hate
  lora_r              : 8
  lora_alpha          : 16
  lora_dropout        : 0.05
  lora_target_modules : ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
  lora_weight_path    : 
  lora_bias           : none
  q_lora              : False
  model_name_or_path  : meta-llama/Llama-2-7b-chat-hf
  adapter_name_or_path: 
  use_lora            : True
  cache_dir           : 
  optim               : adamw_torch
  model_max_length    : 512
  grouped_to_max_length: False
  use_refusal_retain  : True
  sc_train_subset     : ['']
  log_every           : 10
  sc_train_seq_type   : all_text
  coeff_schedule      : linear_converge
  sc_loss_type        : orig_act_dotprod
  branch              : master
  commit_id           : 
  commit_msg          : 
  cmd                 : --bed ./checkpoints --local_out_path ./local_output --project_name infinity_circuit_breaker --exp_name experiment --ep 100 --opt adamw --cum 3 --sche lin0 --fp16 2 --ada 0.9_0.97 --tini -1 --tclip 5 --flash 0 --alng 5e-06 --saln 1 --cos 1 --enable_checkpointing full-block --tblr 6e-3 --pn 0.06M --lbs 4 --workers 4 --short_cap_prob 0.2 --online_t5 1 --use_streaming_dataset 1 --iterable_data_buffersize 30000 --Ct5 2048 --t5_path google/flan-t5-xl --vae_type 14 --wp 0.00000001 --wpe 1 --dynamic_resolution_across_gpus 1 --enable_dynamic_length_prompt 1 --reweight_loss_by_scale 1 --add_lvl_embeding_only_first_block 1 --rope2d_each_sa_layer 1 --rope2d_normalized_by_hw 2 --use_fsdp_model_ema 0 --always_training_scales 100 --use_bit_label 1 --zero 2 --save_model_iters_freq 100 --log_freq 50 --checkpoint_type torch --prefetch_factor 2 --noise_apply_strength 0.3 --noise_apply_layers 13 --apply_spatial_patchify 1 --use_flex_attn True --pad 128 --data_path ./data --data_load_reso 512 --tlen 512 --workers 4 --prefetch_factor 2 --harmful_prompts_path /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts --sanitized_prompts_path /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts --validation_ratio 0.1 --category hate --target_layers 10,12,14,16,18,20 --transform_layers 10,12,14,16,18,20 --lorra_alpha 5.0 --trainsets AlpacaSupervisedDataset#HarmfulDataset --valsets AlpacaSupervisedDataset#HarmfulDataset --adv_string Sure here's --full_layers False --lora_r 8 --lora_alpha 16 --lora_dropout 0.05 --lora_target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj --lora_weight_path  --lora_bias none --q_lora False --model_name_or_path meta-llama/Llama-2-7b-chat-hf --adapter_name_or_path  --use_lora True --cache_dir  --optim adamw_torch --model_max_length 512 --grouped_to_max_length False --use_refusal_retain True --sc_train_subset  --log_every 10 --sc_train_seq_type all_text --coeff_schedule linear_converge --sc_loss_type orig_act_dotprod
  tag                 : UK
  acc_all             : None
  acc_real            : None
  acc_fake            : None
  last_Lnll           : None
  last_L1             : None
  last_Ld             : None
  last_wei_g          : None
  grad_boom           : None
  diff                : None
  diffs               : 
  diffs_ema           : None
  ca_performance      : 
  cur_phase           : 
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  iter_speed          : None
  img_per_day         : None
  max_nvidia_smi      : 0
  max_memory_allocated: None
  max_memory_reserved : None
  num_alloc_retries   : None
  MFU                 : None
  HFU                 : None
  dbg_modified        : False
  dbg_ks              : False
  dbg_ks_last         : None
  dbg                 : False
  ks                  : False
  nodata              : False
  nodata_tlen         : 320
  nova                : False
  prof                : 0
  prof_freq           : 50
  tos_profiler_file_prefix: vgpt_default/
  profall             : 0
  v_seed              : 0
  g_seed              : 0
}

[09-01 23:06:17] (inity_circuit_breaker.py, line 182)=> start_it=0, iters_train=100







=======================================================   RESTART [09-01 23:17:49]   =======================================================
[09-01 23:17:49] (inity_circuit_breaker.py, line 718)=> ============================================================
[09-01 23:17:49] (inity_circuit_breaker.py, line 719)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[09-01 23:17:49] (inity_circuit_breaker.py, line 720)=> Circuit Breaker Alpha: 0.1
[09-01 23:17:49] (inity_circuit_breaker.py, line 721)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[09-01 23:17:49] (inity_circuit_breaker.py, line 722)=> Circuit Breaker Enabled: True
[09-01 23:17:49] (inity_circuit_breaker.py, line 723)=> Number of Examples: 1000
[09-01 23:17:49] (inity_circuit_breaker.py, line 724)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[09-01 23:17:49] (inity_circuit_breaker.py, line 725)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[09-01 23:17:49] (inity_circuit_breaker.py, line 726)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[09-01 23:17:49] (inity_circuit_breaker.py, line 727)=> Validation Ratio: 0.1
[09-01 23:17:49] (inity_circuit_breaker.py, line 728)=> Category: hate
[09-01 23:17:49] (inity_circuit_breaker.py, line 729)=> Batch Size: 4
[09-01 23:17:49] (inity_circuit_breaker.py, line 730)=> Workers: 4
[09-01 23:17:49] (inity_circuit_breaker.py, line 731)=> Device: cuda:0
[09-01 23:17:49] (inity_circuit_breaker.py, line 732)=> Model: infinity_8b
[09-01 23:17:49] (inity_circuit_breaker.py, line 733)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-01 23:17:49] (inity_circuit_breaker.py, line 734)=> Rush Resume: 
[09-01 23:17:49] (inity_circuit_breaker.py, line 735)=> Selective Layers: 0,1,2,3,4,5
[09-01 23:17:49] (inity_circuit_breaker.py, line 736)=> ============================================================
[09-01 23:17:49] (inity_circuit_breaker.py, line 377)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[09-01 23:17:49] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-01 23:17:49] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-01 23:17:49] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[09-01 23:17:49] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-01 23:17:49] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-01 23:17:50] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[09-01 23:17:50] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[09-01 23:17:50] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[09-01 23:17:50] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[09-01 23:17:50] (inity_circuit_breaker.py, line 395)=> args.batch_size=4, vbs=6
[09-01 23:17:50] (inity_circuit_breaker.py, line 418)=> len(dataloader): 100, len(dataset): 100
[09-01 23:17:50] (inity_circuit_breaker.py, line 420)=> total_samples: 200
[09-01 23:17:50] (inity_circuit_breaker.py, line 421)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[09-01 23:17:50] (inity_circuit_breaker.py, line 140)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[09-01 23:17:50] (inity_circuit_breaker.py, line 142)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[09-01 23:17:50] (inity_circuit_breaker.py, line 146)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-01 23:17:52] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[09-01 23:17:52] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[09-01 23:17:52] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[09-01 23:17:52] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[09-01 23:17:55] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[09-01 23:17:56] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[09-01 23:17:56] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[09-01 23:17:56] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[09-01 23:17:56] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[09-01 23:17:56] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

[09-01 23:18:07] (ity/models/init_param.py, line  15)=> [init_weights] Infinity with std=0.02
[09-01 23:18:07] (inity_circuit_breaker.py, line 246)=> [PT] GPT model = Infinity(
  drop_path_rate=0.2
  (norm0_cond): Identity()
  (text_norm): FastRMSNorm(C=2048, eps=1e-06, elementwise_affine=True)
  (text_proj_for_sos): TextAttentivePool(
    (ca): CrossAttention(
      Cq=3072, Ckv=2048, cos_attn=False
      (mat_kv): Linear(in_features=2048, out_features=6144, bias=False)
      (proj): Linear(in_features=3072, out_features=3072, bias=True)
      (proj_drop): Identity()
    )
  )
  (text_proj_for_ca): Sequential(
    (0): Linear(in_features=2048, out_features=3072, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=3072, out_features=3072, bias=True)
  )
  (lvl_embed): Embedding(15, 3072)
  (norm0_ve): Identity()
  (word_embed): Linear(in_features=56, out_features=3072, bias=True)
  (shared_ada_lin): Sequential(
    (0): SiLU()
    (1): SharedAdaLin(in_features=3072, out_features=18432, bias=True)
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=3072, out_features=6144, bias=True)
    )
  )
  (head): Linear(in_features=3072, out_features=112, bias=True)
  (blocks): ModuleList(
    (0): CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): Identity()
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
    (1-55): 55 x CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): DropPath(...)
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
  )
)


[09-01 23:18:07] (inity_circuit_breaker.py, line 248)=> [PT][#para] VAE=81.62, VAE.quant=0.00
[09-01 23:18:07] (inity_circuit_breaker.py, line 251)=> [PT][#para] GPT=8573.89


[09-01 23:18:07] (nity/utils/lr_control.py, line  71)=> [get_param_groups][lr decay] with_lr_scale=False, lr_scale=0.0
[09-01 23:18:07] (nity/utils/lr_control.py, line 111)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': '[no scale]',
         'params': "('module.text_norm.weight, module.text_proj_for_sos.ca.mat_kv.weight, module.text_proj_for_sos.ca.proj.weight, module.text_proj_for_ca.0.weight, module.text_proj_for_ca.2.weight, '\n"
                   " 'module.word_embed.weight, module.shared_ada_lin.1.weight, module.head_nm.ada_lin.1.weight, module.head.weight, module.blocks.0.sa.mat_qkv.weight, module.blocks.0.sa.proj.weight, '\n"
                   " 'module.blocks.0.ca.mat_q.weight, module.blocks.0.ca.mat_kv.weight, module.blocks.0.ca.proj.weight, module.blocks.0.ffn.fc1.weight, module.blocks.0.ffn.fc2.weight, module.blocks.0.ca_norm.weight, '\n"
                   " 'module.blocks.1.sa.mat_qkv.weight, module.blocks.1.sa.proj.weight, module.blocks.1.ca.mat_q.weight, module.blocks.1.ca.mat_kv.weight, module.blocks.1.ca.proj.weight, '\n"
                   " 'module.blocks.1.ffn.fc1.weight, module.blocks.1.ffn.fc2.weight, module.blocks.1.ca_norm.weight, module.blocks.2.sa.mat_qkv.weight, module.blocks.2.sa.proj.weight, module.blocks.2.ca.mat_q.weight, '\n"
                   " 'module.blocks.2.ca.mat_kv.weight, module.blocks.2.ca.proj.weight, module.blocks.2.ffn.fc1.weight, module.blocks.2.ffn.fc2.weight, module.blocks.2.ca_norm.weight, module.blocks.3.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.3.sa.proj.weight, module.blocks.3.ca.mat_q.weight, module.blocks.3.ca.mat_kv.weight, module.blocks.3.ca.proj.weight, module.blocks.3.ffn.fc1.weight, module.blocks.3.ffn.fc2.weight, '\n"
                   " 'module.blocks.3.ca_norm.weight, module.blocks.4.sa.mat_qkv.weight, module.blocks.4.sa.proj.weight, module.blocks.4.ca.mat_q.weight, module.blocks.4.ca.mat_kv.weight, '\n"
                   " 'module.blocks.4.ca.proj.weight, module.blocks.4.ffn.fc1.weight, module.blocks.4.ffn.fc2.weight, module.blocks.4.ca_norm.weight, module.blocks.5.sa.mat_qkv.weight, module.blocks.5.sa.proj.weight, '\n"
                   " 'module.blocks.5.ca.mat_q.weight, module.blocks.5.ca.mat_kv.weight, module.blocks.5.ca.proj.weight, module.blocks.5.ffn.fc1.weight, module.blocks.5.ffn.fc2.weight, module.blocks.5.ca_norm.weight, '\n"
                   " 'module.blocks.6.sa.mat_qkv.weight, module.blocks.6.sa.proj.weight, module.blocks.6.ca.mat_q.weight, module.blocks.6.ca.mat_kv.weight, module.blocks.6.ca.proj.weight, '\n"
                   " 'module.blocks.6.ffn.fc1.weight, module.blocks.6.ffn.fc2.weight, module.blocks.6.ca_norm.weight, module.blocks.7.sa.mat_qkv.weight, module.blocks.7.sa.proj.weight, module.blocks.7.ca.mat_q.weight, '\n"
                   " 'module.blocks.7.ca.mat_kv.weight, module.blocks.7.ca.proj.weight, module.blocks.7.ffn.fc1.weight, module.blocks.7.ffn.fc2.weight, module.blocks.7.ca_norm.weight, module.blocks.8.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.8.sa.proj.weight, module.blocks.8.ca.mat_q.weight, module.blocks.8.ca.mat_kv.weight, module.blocks.8.ca.proj.weight, module.blocks.8.ffn.fc1.weight, module.blocks.8.ffn.fc2.weight, '\n"
                   " 'module.blocks.8.ca_norm.weight, module.blocks.9.sa.mat_qkv.weight, module.blocks.9.sa.proj.weight, module.blocks.9.ca.mat_q.weight, module.blocks.9.ca.mat_kv.weight, '\n"
                   " 'module.blocks.9.ca.proj.weight, module.blocks.9.ffn.fc1.weight, module.blocks.9.ffn.fc2.weight, module.blocks.9.ca_norm.weight, module.blocks.10.sa.mat_qkv.weight, module.blocks.10.sa.proj.weight, '\n"
                   " 'module.blocks.10.ca.mat_q.weight, module.blocks.10.ca.mat_kv.weight, module.blocks.10.ca.proj.weight, module.blocks.10.ffn.fc1.weight, module.blocks.10.ffn.fc2.weight, '\n"
                   " 'module.blocks.10.ca_norm.weight, module.blocks.11.sa.mat_qkv.weight, module.blocks.11.sa.proj.weight, module.blocks.11.ca.mat_q.weight, module.blocks.11.ca.mat_kv.weight, '\n"
                   " 'module.blocks.11.ca.proj.weight, module.blocks.11.ffn.fc1.weight, module.blocks.11.ffn.fc2.weight, module.blocks.11.ca_norm.weight, module.blocks.12.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.12.sa.proj.weight, module.blocks.12.ca.mat_q.weight, module.blocks.12.ca.mat_kv.weight, module.blocks.12.ca.proj.weight, module.blocks.12.ffn.fc1.weight, '\n"
                   " 'module.blocks.12.ffn.fc2.weight, module.blocks.12.ca_norm.weight, module.blocks.13.sa.mat_qkv.weight, module.blocks.13.sa.proj.weight, module.blocks.13.ca.mat_q.weight, '\n"
                   " 'module.blocks.13.ca.mat_kv.weight, module.blocks.13.ca.proj.weight, module.blocks.13.ffn.fc1.weight, module.blocks.13.ffn.fc2.weight, module.blocks.13.ca_norm.weight, '\n"
                   " 'module.blocks.14.sa.mat_qkv.weight, module.blocks.14.sa.proj.weight, module.blocks.14.ca.mat_q.weight, module.blocks.14.ca.mat_kv.weight, module.blocks.14.ca.proj.weight, '\n"
                   " 'module.blocks.14.ffn.fc1.weight, module.blocks.14.ffn.fc2.weight, module.blocks.14.ca_norm.weight, module.blocks.15.sa.mat_qkv.weight, module.blocks.15.sa.proj.weight, '\n"
                   " 'module.blocks.15.ca.mat_q.weight, module.blocks.15.ca.mat_kv.weight, module.blocks.15.ca.proj.weight, module.blocks.15.ffn.fc1.weight, module.blocks.15.ffn.fc2.weight, '\n"
                   " 'module.blocks.15.ca_norm.weight, module.blocks.16.sa.mat_qkv.weight, module.blocks.16.sa.proj.weight, module.blocks.16.ca.mat_q.weight, module.blocks.16.ca.mat_kv.weight, '\n"
                   " 'module.blocks.16.ca.proj.weight, module.blocks.16.ffn.fc1.weight, module.blocks.16.ffn.fc2.weight, module.blocks.16.ca_norm.weight, module.blocks.17.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.17.sa.proj.weight, module.blocks.17.ca.mat_q.weight, module.blocks.17.ca.mat_kv.weight, module.blocks.17.ca.proj.weight, module.blocks.17.ffn.fc1.weight, '\n"
                   " 'module.blocks.17.ffn.fc2.weight, module.blocks.17.ca_norm.weight, module.blocks.18.sa.mat_qkv.weight, module.blocks.18.sa.proj.weight, module.blocks.18.ca.mat_q.weight, '\n"
                   " 'module.blocks.18.ca.mat_kv.weight, module.blocks.18.ca.proj.weight, module.blocks.18.ffn.fc1.weight, module.blocks.18.ffn.fc2.weight, module.blocks.18.ca_norm.weight, '\n"
                   " 'module.blocks.19.sa.mat_qkv.weight, module.blocks.19.sa.proj.weight, module.blocks.19.ca.mat_q.weight, module.blocks.19.ca.mat_kv.weight, module.blocks.19.ca.proj.weight, '\n"
                   " 'module.blocks.19.ffn.fc1.weight, module.blocks.19.ffn.fc2.weight, module.blocks.19.ca_norm.weight, module.blocks.20.sa.mat_qkv.weight, module.blocks.20.sa.proj.weight, '\n"
                   " 'module.blocks.20.ca.mat_q.weight, module.blocks.20.ca.mat_kv.weight, module.blocks.20.ca.proj.weight, module.blocks.20.ffn.fc1.weight, module.blocks.20.ffn.fc2.weight, '\n"
                   " 'module.blocks.20.ca_norm.weight, module.blocks.21.sa.mat_qkv.weight, module.blocks.21.sa.proj.weight, module.blocks.21.ca.mat_q.weight, module.blocks.21.ca.mat_kv.weight, '\n"
                   " 'module.blocks.21.ca.proj.weight, module.blocks.21.ffn.fc1.weight, module.blocks.21.ffn.fc2.weight, module.blocks.21.ca_norm.weight, module.blocks.22.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.22.sa.proj.weight, module.blocks.22.ca.mat_q.weight, module.blocks.22.ca.mat_kv.weight, module.blocks.22.ca.proj.weight, module.blocks.22.ffn.fc1.weight, '\n"
                   " 'module.blocks.22.ffn.fc2.weight, module.blocks.22.ca_norm.weight, module.blocks.23.sa.mat_qkv.weight, module.blocks.23.sa.proj.weight, module.blocks.23.ca.mat_q.weight, '\n"
                   " 'module.blocks.23.ca.mat_kv.weight, module.blocks.23.ca.proj.weight, module.blocks.23.ffn.fc1.weight, module.blocks.23.ffn.fc2.weight, module.blocks.23.ca_norm.weight, '\n"
                   " 'module.blocks.24.sa.mat_qkv.weight, module.blocks.24.sa.proj.weight, module.blocks.24.ca.mat_q.weight, module.blocks.24.ca.mat_kv.weight, module.blocks.24.ca.proj.weight, '\n"
                   " 'module.blocks.24.ffn.fc1.weight, module.blocks.24.ffn.fc2.weight, module.blocks.24.ca_norm.weight, module.blocks.25.sa.mat_qkv.weight, module.blocks.25.sa.proj.weight, '\n"
                   " 'module.blocks.25.ca.mat_q.weight, module.blocks.25.ca.mat_kv.weight, module.blocks.25.ca.proj.weight, module.blocks.25.ffn.fc1.weight, module.blocks.25.ffn.fc2.weight, '\n"
                   " 'module.blocks.25.ca_norm.weight, module.blocks.26.sa.mat_qkv.weight, module.blocks.26.sa.proj.weight, module.blocks.26.ca.mat_q.weight, module.blocks.26.ca.mat_kv.weight, '\n"
                   " 'module.blocks.26.ca.proj.weight, module.blocks.26.ffn.fc1.weight, module.blocks.26.ffn.fc2.weight, module.blocks.26.ca_norm.weight, module.blocks.27.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.27.sa.proj.weight, module.blocks.27.ca.mat_q.weight, module.blocks.27.ca.mat_kv.weight, module.blocks.27.ca.proj.weight, module.blocks.27.ffn.fc1.weight, '\n"
                   " 'module.blocks.27.ffn.fc2.weight, module.blocks.27.ca_norm.weight, module.blocks.28.sa.mat_qkv.weight, module.blocks.28.sa.proj.weight, module.blocks.28.ca.mat_q.weight, '\n"
                   " 'module.blocks.28.ca.mat_kv.weight, module.blocks.28.ca.proj.weight, module.blocks.28.ffn.fc1.weight, module.blocks.28.ffn.fc2.weight, module.blocks.28.ca_norm.weight, '\n"
                   " 'module.blocks.29.sa.mat_qkv.weight, module.blocks.29.sa.proj.weight, module.blocks.29.ca.mat_q.weight, module.blocks.29.ca.mat_kv.weight, module.blocks.29.ca.proj.weight, '\n"
                   " 'module.blocks.29.ffn.fc1.weight, module.blocks.29.ffn.fc2.weight, module.blocks.29.ca_norm.weight, module.blocks.30.sa.mat_qkv.weight, module.blocks.30.sa.proj.weight, '\n"
                   " 'module.blocks.30.ca.mat_q.weight, module.blocks.30.ca.mat_kv.weight, module.blocks.30.ca.proj.weight, module.blocks.30.ffn.fc1.weight, module.blocks.30.ffn.fc2.weight, '\n"
                   " 'module.blocks.30.ca_norm.weight, module.blocks.31.sa.mat_qkv.weight, module.blocks.31.sa.proj.weight, module.blocks.31.ca.mat_q.weight, module.blocks.31.ca.mat_kv.weight, '\n"
                   " 'module.blocks.31.ca.proj.weight, module.blocks.31.ffn.fc1.weight, module.blocks.31.ffn.fc2.weight, module.blocks.31.ca_norm.weight, module.blocks.32.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.32.sa.proj.weight, module.blocks.32.ca.mat_q.weight, module.blocks.32.ca.mat_kv.weight, module.blocks.32.ca.proj.weight, module.blocks.32.ffn.fc1.weight, '\n"
                   " 'module.blocks.32.ffn.fc2.weight, module.blocks.32.ca_norm.weight, module.blocks.33.sa.mat_qkv.weight, module.blocks.33.sa.proj.weight, module.blocks.33.ca.mat_q.weight, '\n"
                   " 'module.blocks.33.ca.mat_kv.weight, module.blocks.33.ca.proj.weight, module.blocks.33.ffn.fc1.weight, module.blocks.33.ffn.fc2.weight, module.blocks.33.ca_norm.weight, '\n"
                   " 'module.blocks.34.sa.mat_qkv.weight, module.blocks.34.sa.proj.weight, module.blocks.34.ca.mat_q.weight, module.blocks.34.ca.mat_kv.weight, module.blocks.34.ca.proj.weight, '\n"
                   " 'module.blocks.34.ffn.fc1.weight, module.blocks.34.ffn.fc2.weight, module.blocks.34.ca_norm.weight, module.blocks.35.sa.mat_qkv.weight, module.blocks.35.sa.proj.weight, '\n"
                   " 'module.blocks.35.ca.mat_q.weight, module.blocks.35.ca.mat_kv.weight, module.blocks.35.ca.proj.weight, module.blocks.35.ffn.fc1.weight, module.blocks.35.ffn.fc2.weight, '\n"
                   " 'module.blocks.35.ca_norm.weight, module.blocks.36.sa.mat_qkv.weight, module.blocks.36.sa.proj.weight, module.blocks.36.ca.mat_q.weight, module.blocks.36.ca.mat_kv.weight, '\n"
                   " 'module.blocks.36.ca.proj.weight, module.blocks.36.ffn.fc1.weight, module.blocks.36.ffn.fc2.weight, module.blocks.36.ca_norm.weight, module.blocks.37.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.37.sa.proj.weight, module.blocks.37.ca.mat_q.weight, module.blocks.37.ca.mat_kv.weight, module.blocks.37.ca.proj.weight, module.blocks.37.ffn.fc1.weight, '\n"
                   " 'module.blocks.37.ffn.fc2.weight, module.blocks.37.ca_norm.weight, module.blocks.38.sa.mat_qkv.weight, module.blocks.38.sa.proj.weight, module.blocks.38.ca.mat_q.weight, '\n"
                   " 'module.blocks.38.ca.mat_kv.weight, module.blocks.38.ca.proj.weight, module.blocks.38.ffn.fc1.weight, module.blocks.38.ffn.fc2.weight, module.blocks.38.ca_norm.weight, '\n"
                   " 'module.blocks.39.sa.mat_qkv.weight, module.blocks.39.sa.proj.weight, module.blocks.39.ca.mat_q.weight, module.blocks.39.ca.mat_kv.weight, module.blocks.39.ca.proj.weight, '\n"
                   " 'module.blocks.39.ffn.fc1.weight, module.blocks.39.ffn.fc2.weight, module.blocks.39.ca_norm.weight, module.blocks.40.sa.mat_qkv.weight, module.blocks.40.sa.proj.weight, '\n"
                   " 'module.blocks.40.ca.mat_q.weight, module.blocks.40.ca.mat_kv.weight, module.blocks.40.ca.proj.weight, module.blocks.40.ffn.fc1.weight, module.blocks.40.ffn.fc2.weight, '\n"
                   " 'module.blocks.40.ca_norm.weight, module.blocks.41.sa.mat_qkv.weight, module.blocks.41.sa.proj.weight, module.blocks.41.ca.mat_q.weight, module.blocks.41.ca.mat_kv.weight, '\n"
                   " 'module.blocks.41.ca.proj.weight, module.blocks.41.ffn.fc1.weight, module.blocks.41.ffn.fc2.weight, module.blocks.41.ca_norm.weight, module.blocks.42.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.42.sa.proj.weight, module.blocks.42.ca.mat_q.weight, module.blocks.42.ca.mat_kv.weight, module.blocks.42.ca.proj.weight, module.blocks.42.ffn.fc1.weight, '\n"
                   " 'module.blocks.42.ffn.fc2.weight, module.blocks.42.ca_norm.weight, module.blocks.43.sa.mat_qkv.weight, module.blocks.43.sa.proj.weight, module.blocks.43.ca.mat_q.weight, '\n"
                   " 'module.blocks.43.ca.mat_kv.weight, module.blocks.43.ca.proj.weight, module.blocks.43.ffn.fc1.weight, module.blocks.43.ffn.fc2.weight, module.blocks.43.ca_norm.weight, '\n"
                   " 'module.blocks.44.sa.mat_qkv.weight, module.blocks.44.sa.proj.weight, module.blocks.44.ca.mat_q.weight, module.blocks.44.ca.mat_kv.weight, module.blocks.44.ca.proj.weight, '\n"
                   " 'module.blocks.44.ffn.fc1.weight, module.blocks.44.ffn.fc2.weight, module.blocks.44.ca_norm.weight, module.blocks.45.sa.mat_qkv.weight, module.blocks.45.sa.proj.weight, '\n"
                   " 'module.blocks.45.ca.mat_q.weight, module.blocks.45.ca.mat_kv.weight, module.blocks.45.ca.proj.weight, module.blocks.45.ffn.fc1.weight, module.blocks.45.ffn.fc2.weight, '\n"
                   " 'module.blocks.45.ca_norm.weight, module.blocks.46.sa.mat_qkv.weight, module.blocks.46.sa.proj.weight, module.blocks.46.ca.mat_q.weight, module.blocks.46.ca.mat_kv.weight, '\n"
                   " 'module.blocks.46.ca.proj.weight, module.blocks.46.ffn.fc1.weight, module.blocks.46.ffn.fc2.weight, module.blocks.46.ca_norm.weight, module.blocks.47.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.47.sa.proj.weight, module.blocks.47.ca.mat_q.weight, module.blocks.47.ca.mat_kv.weight, module.blocks.47.ca.proj.weight, module.blocks.47.ffn.fc1.weight, '\n"
                   " 'module.blocks.47.ffn.fc2.weight, module.blocks.47.ca_norm.weight, module.blocks.48.sa.mat_qkv.weight, module.blocks.48.sa.proj.weight, module.blocks.48.ca.mat_q.weight, '\n"
                   " 'module.blocks.48.ca.mat_kv.weight, module.blocks.48.ca.proj.weight, module.blocks.48.ffn.fc1.weight, module.blocks.48.ffn.fc2.weight, module.blocks.48.ca_norm.weight, '\n"
                   " 'module.blocks.49.sa.mat_qkv.weight, module.blocks.49.sa.proj.weight, module.blocks.49.ca.mat_q.weight, module.blocks.49.ca.mat_kv.weight, module.blocks.49.ca.proj.weight, '\n"
                   " 'module.blocks.49.ffn.fc1.weight, module.blocks.49.ffn.fc2.weight, module.blocks.49.ca_norm.weight, module.blocks.50.sa.mat_qkv.weight, module.blocks.50.sa.proj.weight, '\n"
                   " 'module.blocks.50.ca.mat_q.weight, module.blocks.50.ca.mat_kv.weight, module.blocks.50.ca.proj.weight, module.blocks.50.ffn.fc1.weight, module.blocks.50.ffn.fc2.weight, '\n"
                   " 'module.blocks.50.ca_norm.weight, module.blocks.51.sa.mat_qkv.weight, module.blocks.51.sa.proj.weight, module.blocks.51.ca.mat_q.weight, module.blocks.51.ca.mat_kv.weight, '\n"
                   " 'module.blocks.51.ca.proj.weight, module.blocks.51.ffn.fc1.weight, module.blocks.51.ffn.fc2.weight, module.blocks.51.ca_norm.weight, module.blocks.52.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.52.sa.proj.weight, module.blocks.52.ca.mat_q.weight, module.blocks.52.ca.mat_kv.weight, module.blocks.52.ca.proj.weight, module.blocks.52.ffn.fc1.weight, '\n"
                   " 'module.blocks.52.ffn.fc2.weight, module.blocks.52.ca_norm.weight, module.blocks.53.sa.mat_qkv.weight, module.blocks.53.sa.proj.weight, module.blocks.53.ca.mat_q.weight, '\n"
                   " 'module.blocks.53.ca.mat_kv.weight, module.blocks.53.ca.proj.weight, module.blocks.53.ffn.fc1.weight, module.blocks.53.ffn.fc2.weight, module.blocks.53.ca_norm.weight, '\n"
                   " 'module.blocks.54.sa.mat_qkv.weight, module.blocks.54.sa.proj.weight, module.blocks.54.ca.mat_q.weight, module.blocks.54.ca.mat_kv.weight, module.blocks.54.ca.proj.weight, '\n"
                   " 'module.blocks.54.ffn.fc1.weight, module.blocks.54.ffn.fc2.weight, module.blocks.54.ca_norm.weight, module.blocks.55.sa.mat_qkv.weight, module.blocks.55.sa.proj.weight, '\n"
                   " 'module.blocks.55.ca.mat_q.weight, module.blocks.55.ca.mat_kv.weight, module.blocks.55.ca.proj.weight, module.blocks.55.ffn.fc1.weight, module.blocks.55.ffn.fc2.weight, '\n"
                   " 'module.blocks.55.ca_norm.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': '[no scale]',
          'params': "('module.cfg_uncond, module.pos_start, module.text_proj_for_sos.ca.mat_q, module.text_proj_for_sos.ca.v_bias, module.text_proj_for_sos.ca.proj.bias, module.text_proj_for_ca.0.bias, '\n"
                    " 'module.text_proj_for_ca.2.bias, module.lvl_embed.weight, module.word_embed.bias, module.shared_ada_lin.1.bias, module.head_nm.ada_lin.1.bias, module.head.bias, module.blocks.0.ada_gss, '\n"
                    " 'module.blocks.0.sa.scale_mul_1H11, module.blocks.0.sa.q_bias, module.blocks.0.sa.v_bias, module.blocks.0.sa.proj.bias, module.blocks.0.ca.v_bias, module.blocks.0.ca.mat_q.bias, '\n"
                    " 'module.blocks.0.ca.proj.bias, module.blocks.0.ffn.fc1.bias, module.blocks.0.ffn.fc2.bias, module.blocks.0.ca_norm.bias, module.blocks.1.ada_gss, module.blocks.1.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.1.sa.q_bias, module.blocks.1.sa.v_bias, module.blocks.1.sa.proj.bias, module.blocks.1.ca.v_bias, module.blocks.1.ca.mat_q.bias, module.blocks.1.ca.proj.bias, '\n"
                    " 'module.blocks.1.ffn.fc1.bias, module.blocks.1.ffn.fc2.bias, module.blocks.1.ca_norm.bias, module.blocks.2.ada_gss, module.blocks.2.sa.scale_mul_1H11, module.blocks.2.sa.q_bias, '\n"
                    " 'module.blocks.2.sa.v_bias, module.blocks.2.sa.proj.bias, module.blocks.2.ca.v_bias, module.blocks.2.ca.mat_q.bias, module.blocks.2.ca.proj.bias, module.blocks.2.ffn.fc1.bias, '\n"
                    " 'module.blocks.2.ffn.fc2.bias, module.blocks.2.ca_norm.bias, module.blocks.3.ada_gss, module.blocks.3.sa.scale_mul_1H11, module.blocks.3.sa.q_bias, module.blocks.3.sa.v_bias, '\n"
                    " 'module.blocks.3.sa.proj.bias, module.blocks.3.ca.v_bias, module.blocks.3.ca.mat_q.bias, module.blocks.3.ca.proj.bias, module.blocks.3.ffn.fc1.bias, module.blocks.3.ffn.fc2.bias, '\n"
                    " 'module.blocks.3.ca_norm.bias, module.blocks.4.ada_gss, module.blocks.4.sa.scale_mul_1H11, module.blocks.4.sa.q_bias, module.blocks.4.sa.v_bias, module.blocks.4.sa.proj.bias, '\n"
                    " 'module.blocks.4.ca.v_bias, module.blocks.4.ca.mat_q.bias, module.blocks.4.ca.proj.bias, module.blocks.4.ffn.fc1.bias, module.blocks.4.ffn.fc2.bias, module.blocks.4.ca_norm.bias, '\n"
                    " 'module.blocks.5.ada_gss, module.blocks.5.sa.scale_mul_1H11, module.blocks.5.sa.q_bias, module.blocks.5.sa.v_bias, module.blocks.5.sa.proj.bias, module.blocks.5.ca.v_bias, '\n"
                    " 'module.blocks.5.ca.mat_q.bias, module.blocks.5.ca.proj.bias, module.blocks.5.ffn.fc1.bias, module.blocks.5.ffn.fc2.bias, module.blocks.5.ca_norm.bias, module.blocks.6.ada_gss, '\n"
                    " 'module.blocks.6.sa.scale_mul_1H11, module.blocks.6.sa.q_bias, module.blocks.6.sa.v_bias, module.blocks.6.sa.proj.bias, module.blocks.6.ca.v_bias, module.blocks.6.ca.mat_q.bias, '\n"
                    " 'module.blocks.6.ca.proj.bias, module.blocks.6.ffn.fc1.bias, module.blocks.6.ffn.fc2.bias, module.blocks.6.ca_norm.bias, module.blocks.7.ada_gss, module.blocks.7.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.7.sa.q_bias, module.blocks.7.sa.v_bias, module.blocks.7.sa.proj.bias, module.blocks.7.ca.v_bias, module.blocks.7.ca.mat_q.bias, module.blocks.7.ca.proj.bias, '\n"
                    " 'module.blocks.7.ffn.fc1.bias, module.blocks.7.ffn.fc2.bias, module.blocks.7.ca_norm.bias, module.blocks.8.ada_gss, module.blocks.8.sa.scale_mul_1H11, module.blocks.8.sa.q_bias, '\n"
                    " 'module.blocks.8.sa.v_bias, module.blocks.8.sa.proj.bias, module.blocks.8.ca.v_bias, module.blocks.8.ca.mat_q.bias, module.blocks.8.ca.proj.bias, module.blocks.8.ffn.fc1.bias, '\n"
                    " 'module.blocks.8.ffn.fc2.bias, module.blocks.8.ca_norm.bias, module.blocks.9.ada_gss, module.blocks.9.sa.scale_mul_1H11, module.blocks.9.sa.q_bias, module.blocks.9.sa.v_bias, '\n"
                    " 'module.blocks.9.sa.proj.bias, module.blocks.9.ca.v_bias, module.blocks.9.ca.mat_q.bias, module.blocks.9.ca.proj.bias, module.blocks.9.ffn.fc1.bias, module.blocks.9.ffn.fc2.bias, '\n"
                    " 'module.blocks.9.ca_norm.bias, module.blocks.10.ada_gss, module.blocks.10.sa.scale_mul_1H11, module.blocks.10.sa.q_bias, module.blocks.10.sa.v_bias, module.blocks.10.sa.proj.bias, '\n"
                    " 'module.blocks.10.ca.v_bias, module.blocks.10.ca.mat_q.bias, module.blocks.10.ca.proj.bias, module.blocks.10.ffn.fc1.bias, module.blocks.10.ffn.fc2.bias, module.blocks.10.ca_norm.bias, '\n"
                    " 'module.blocks.11.ada_gss, module.blocks.11.sa.scale_mul_1H11, module.blocks.11.sa.q_bias, module.blocks.11.sa.v_bias, module.blocks.11.sa.proj.bias, module.blocks.11.ca.v_bias, '\n"
                    " 'module.blocks.11.ca.mat_q.bias, module.blocks.11.ca.proj.bias, module.blocks.11.ffn.fc1.bias, module.blocks.11.ffn.fc2.bias, module.blocks.11.ca_norm.bias, module.blocks.12.ada_gss, '\n"
                    " 'module.blocks.12.sa.scale_mul_1H11, module.blocks.12.sa.q_bias, module.blocks.12.sa.v_bias, module.blocks.12.sa.proj.bias, module.blocks.12.ca.v_bias, module.blocks.12.ca.mat_q.bias, '\n"
                    " 'module.blocks.12.ca.proj.bias, module.blocks.12.ffn.fc1.bias, module.blocks.12.ffn.fc2.bias, module.blocks.12.ca_norm.bias, module.blocks.13.ada_gss, module.blocks.13.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.13.sa.q_bias, module.blocks.13.sa.v_bias, module.blocks.13.sa.proj.bias, module.blocks.13.ca.v_bias, module.blocks.13.ca.mat_q.bias, module.blocks.13.ca.proj.bias, '\n"
                    " 'module.blocks.13.ffn.fc1.bias, module.blocks.13.ffn.fc2.bias, module.blocks.13.ca_norm.bias, module.blocks.14.ada_gss, module.blocks.14.sa.scale_mul_1H11, module.blocks.14.sa.q_bias, '\n"
                    " 'module.blocks.14.sa.v_bias, module.blocks.14.sa.proj.bias, module.blocks.14.ca.v_bias, module.blocks.14.ca.mat_q.bias, module.blocks.14.ca.proj.bias, module.blocks.14.ffn.fc1.bias, '\n"
                    " 'module.blocks.14.ffn.fc2.bias, module.blocks.14.ca_norm.bias, module.blocks.15.ada_gss, module.blocks.15.sa.scale_mul_1H11, module.blocks.15.sa.q_bias, module.blocks.15.sa.v_bias, '\n"
                    " 'module.blocks.15.sa.proj.bias, module.blocks.15.ca.v_bias, module.blocks.15.ca.mat_q.bias, module.blocks.15.ca.proj.bias, module.blocks.15.ffn.fc1.bias, module.blocks.15.ffn.fc2.bias, '\n"
                    " 'module.blocks.15.ca_norm.bias, module.blocks.16.ada_gss, module.blocks.16.sa.scale_mul_1H11, module.blocks.16.sa.q_bias, module.blocks.16.sa.v_bias, module.blocks.16.sa.proj.bias, '\n"
                    " 'module.blocks.16.ca.v_bias, module.blocks.16.ca.mat_q.bias, module.blocks.16.ca.proj.bias, module.blocks.16.ffn.fc1.bias, module.blocks.16.ffn.fc2.bias, module.blocks.16.ca_norm.bias, '\n"
                    " 'module.blocks.17.ada_gss, module.blocks.17.sa.scale_mul_1H11, module.blocks.17.sa.q_bias, module.blocks.17.sa.v_bias, module.blocks.17.sa.proj.bias, module.blocks.17.ca.v_bias, '\n"
                    " 'module.blocks.17.ca.mat_q.bias, module.blocks.17.ca.proj.bias, module.blocks.17.ffn.fc1.bias, module.blocks.17.ffn.fc2.bias, module.blocks.17.ca_norm.bias, module.blocks.18.ada_gss, '\n"
                    " 'module.blocks.18.sa.scale_mul_1H11, module.blocks.18.sa.q_bias, module.blocks.18.sa.v_bias, module.blocks.18.sa.proj.bias, module.blocks.18.ca.v_bias, module.blocks.18.ca.mat_q.bias, '\n"
                    " 'module.blocks.18.ca.proj.bias, module.blocks.18.ffn.fc1.bias, module.blocks.18.ffn.fc2.bias, module.blocks.18.ca_norm.bias, module.blocks.19.ada_gss, module.blocks.19.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.19.sa.q_bias, module.blocks.19.sa.v_bias, module.blocks.19.sa.proj.bias, module.blocks.19.ca.v_bias, module.blocks.19.ca.mat_q.bias, module.blocks.19.ca.proj.bias, '\n"
                    " 'module.blocks.19.ffn.fc1.bias, module.blocks.19.ffn.fc2.bias, module.blocks.19.ca_norm.bias, module.blocks.20.ada_gss, module.blocks.20.sa.scale_mul_1H11, module.blocks.20.sa.q_bias, '\n"
                    " 'module.blocks.20.sa.v_bias, module.blocks.20.sa.proj.bias, module.blocks.20.ca.v_bias, module.blocks.20.ca.mat_q.bias, module.blocks.20.ca.proj.bias, module.blocks.20.ffn.fc1.bias, '\n"
                    " 'module.blocks.20.ffn.fc2.bias, module.blocks.20.ca_norm.bias, module.blocks.21.ada_gss, module.blocks.21.sa.scale_mul_1H11, module.blocks.21.sa.q_bias, module.blocks.21.sa.v_bias, '\n"
                    " 'module.blocks.21.sa.proj.bias, module.blocks.21.ca.v_bias, module.blocks.21.ca.mat_q.bias, module.blocks.21.ca.proj.bias, module.blocks.21.ffn.fc1.bias, module.blocks.21.ffn.fc2.bias, '\n"
                    " 'module.blocks.21.ca_norm.bias, module.blocks.22.ada_gss, module.blocks.22.sa.scale_mul_1H11, module.blocks.22.sa.q_bias, module.blocks.22.sa.v_bias, module.blocks.22.sa.proj.bias, '\n"
                    " 'module.blocks.22.ca.v_bias, module.blocks.22.ca.mat_q.bias, module.blocks.22.ca.proj.bias, module.blocks.22.ffn.fc1.bias, module.blocks.22.ffn.fc2.bias, module.blocks.22.ca_norm.bias, '\n"
                    " 'module.blocks.23.ada_gss, module.blocks.23.sa.scale_mul_1H11, module.blocks.23.sa.q_bias, module.blocks.23.sa.v_bias, module.blocks.23.sa.proj.bias, module.blocks.23.ca.v_bias, '\n"
                    " 'module.blocks.23.ca.mat_q.bias, module.blocks.23.ca.proj.bias, module.blocks.23.ffn.fc1.bias, module.blocks.23.ffn.fc2.bias, module.blocks.23.ca_norm.bias, module.blocks.24.ada_gss, '\n"
                    " 'module.blocks.24.sa.scale_mul_1H11, module.blocks.24.sa.q_bias, module.blocks.24.sa.v_bias, module.blocks.24.sa.proj.bias, module.blocks.24.ca.v_bias, module.blocks.24.ca.mat_q.bias, '\n"
                    " 'module.blocks.24.ca.proj.bias, module.blocks.24.ffn.fc1.bias, module.blocks.24.ffn.fc2.bias, module.blocks.24.ca_norm.bias, module.blocks.25.ada_gss, module.blocks.25.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.25.sa.q_bias, module.blocks.25.sa.v_bias, module.blocks.25.sa.proj.bias, module.blocks.25.ca.v_bias, module.blocks.25.ca.mat_q.bias, module.blocks.25.ca.proj.bias, '\n"
                    " 'module.blocks.25.ffn.fc1.bias, module.blocks.25.ffn.fc2.bias, module.blocks.25.ca_norm.bias, module.blocks.26.ada_gss, module.blocks.26.sa.scale_mul_1H11, module.blocks.26.sa.q_bias, '\n"
                    " 'module.blocks.26.sa.v_bias, module.blocks.26.sa.proj.bias, module.blocks.26.ca.v_bias, module.blocks.26.ca.mat_q.bias, module.blocks.26.ca.proj.bias, module.blocks.26.ffn.fc1.bias, '\n"
                    " 'module.blocks.26.ffn.fc2.bias, module.blocks.26.ca_norm.bias, module.blocks.27.ada_gss, module.blocks.27.sa.scale_mul_1H11, module.blocks.27.sa.q_bias, module.blocks.27.sa.v_bias, '\n"
                    " 'module.blocks.27.sa.proj.bias, module.blocks.27.ca.v_bias, module.blocks.27.ca.mat_q.bias, module.blocks.27.ca.proj.bias, module.blocks.27.ffn.fc1.bias, module.blocks.27.ffn.fc2.bias, '\n"
                    " 'module.blocks.27.ca_norm.bias, module.blocks.28.ada_gss, module.blocks.28.sa.scale_mul_1H11, module.blocks.28.sa.q_bias, module.blocks.28.sa.v_bias, module.blocks.28.sa.proj.bias, '\n"
                    " 'module.blocks.28.ca.v_bias, module.blocks.28.ca.mat_q.bias, module.blocks.28.ca.proj.bias, module.blocks.28.ffn.fc1.bias, module.blocks.28.ffn.fc2.bias, module.blocks.28.ca_norm.bias, '\n"
                    " 'module.blocks.29.ada_gss, module.blocks.29.sa.scale_mul_1H11, module.blocks.29.sa.q_bias, module.blocks.29.sa.v_bias, module.blocks.29.sa.proj.bias, module.blocks.29.ca.v_bias, '\n"
                    " 'module.blocks.29.ca.mat_q.bias, module.blocks.29.ca.proj.bias, module.blocks.29.ffn.fc1.bias, module.blocks.29.ffn.fc2.bias, module.blocks.29.ca_norm.bias, module.blocks.30.ada_gss, '\n"
                    " 'module.blocks.30.sa.scale_mul_1H11, module.blocks.30.sa.q_bias, module.blocks.30.sa.v_bias, module.blocks.30.sa.proj.bias, module.blocks.30.ca.v_bias, module.blocks.30.ca.mat_q.bias, '\n"
                    " 'module.blocks.30.ca.proj.bias, module.blocks.30.ffn.fc1.bias, module.blocks.30.ffn.fc2.bias, module.blocks.30.ca_norm.bias, module.blocks.31.ada_gss, module.blocks.31.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.31.sa.q_bias, module.blocks.31.sa.v_bias, module.blocks.31.sa.proj.bias, module.blocks.31.ca.v_bias, module.blocks.31.ca.mat_q.bias, module.blocks.31.ca.proj.bias, '\n"
                    " 'module.blocks.31.ffn.fc1.bias, module.blocks.31.ffn.fc2.bias, module.blocks.31.ca_norm.bias, module.blocks.32.ada_gss, module.blocks.32.sa.scale_mul_1H11, module.blocks.32.sa.q_bias, '\n"
                    " 'module.blocks.32.sa.v_bias, module.blocks.32.sa.proj.bias, module.blocks.32.ca.v_bias, module.blocks.32.ca.mat_q.bias, module.blocks.32.ca.proj.bias, module.blocks.32.ffn.fc1.bias, '\n"
                    " 'module.blocks.32.ffn.fc2.bias, module.blocks.32.ca_norm.bias, module.blocks.33.ada_gss, module.blocks.33.sa.scale_mul_1H11, module.blocks.33.sa.q_bias, module.blocks.33.sa.v_bias, '\n"
                    " 'module.blocks.33.sa.proj.bias, module.blocks.33.ca.v_bias, module.blocks.33.ca.mat_q.bias, module.blocks.33.ca.proj.bias, module.blocks.33.ffn.fc1.bias, module.blocks.33.ffn.fc2.bias, '\n"
                    " 'module.blocks.33.ca_norm.bias, module.blocks.34.ada_gss, module.blocks.34.sa.scale_mul_1H11, module.blocks.34.sa.q_bias, module.blocks.34.sa.v_bias, module.blocks.34.sa.proj.bias, '\n"
                    " 'module.blocks.34.ca.v_bias, module.blocks.34.ca.mat_q.bias, module.blocks.34.ca.proj.bias, module.blocks.34.ffn.fc1.bias, module.blocks.34.ffn.fc2.bias, module.blocks.34.ca_norm.bias, '\n"
                    " 'module.blocks.35.ada_gss, module.blocks.35.sa.scale_mul_1H11, module.blocks.35.sa.q_bias, module.blocks.35.sa.v_bias, module.blocks.35.sa.proj.bias, module.blocks.35.ca.v_bias, '\n"
                    " 'module.blocks.35.ca.mat_q.bias, module.blocks.35.ca.proj.bias, module.blocks.35.ffn.fc1.bias, module.blocks.35.ffn.fc2.bias, module.blocks.35.ca_norm.bias, module.blocks.36.ada_gss, '\n"
                    " 'module.blocks.36.sa.scale_mul_1H11, module.blocks.36.sa.q_bias, module.blocks.36.sa.v_bias, module.blocks.36.sa.proj.bias, module.blocks.36.ca.v_bias, module.blocks.36.ca.mat_q.bias, '\n"
                    " 'module.blocks.36.ca.proj.bias, module.blocks.36.ffn.fc1.bias, module.blocks.36.ffn.fc2.bias, module.blocks.36.ca_norm.bias, module.blocks.37.ada_gss, module.blocks.37.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.37.sa.q_bias, module.blocks.37.sa.v_bias, module.blocks.37.sa.proj.bias, module.blocks.37.ca.v_bias, module.blocks.37.ca.mat_q.bias, module.blocks.37.ca.proj.bias, '\n"
                    " 'module.blocks.37.ffn.fc1.bias, module.blocks.37.ffn.fc2.bias, module.blocks.37.ca_norm.bias, module.blocks.38.ada_gss, module.blocks.38.sa.scale_mul_1H11, module.blocks.38.sa.q_bias, '\n"
                    " 'module.blocks.38.sa.v_bias, module.blocks.38.sa.proj.bias, module.blocks.38.ca.v_bias, module.blocks.38.ca.mat_q.bias, module.blocks.38.ca.proj.bias, module.blocks.38.ffn.fc1.bias, '\n"
                    " 'module.blocks.38.ffn.fc2.bias, module.blocks.38.ca_norm.bias, module.blocks.39.ada_gss, module.blocks.39.sa.scale_mul_1H11, module.blocks.39.sa.q_bias, module.blocks.39.sa.v_bias, '\n"
                    " 'module.blocks.39.sa.proj.bias, module.blocks.39.ca.v_bias, module.blocks.39.ca.mat_q.bias, module.blocks.39.ca.proj.bias, module.blocks.39.ffn.fc1.bias, module.blocks.39.ffn.fc2.bias, '\n"
                    " 'module.blocks.39.ca_norm.bias, module.blocks.40.ada_gss, module.blocks.40.sa.scale_mul_1H11, module.blocks.40.sa.q_bias, module.blocks.40.sa.v_bias, module.blocks.40.sa.proj.bias, '\n"
                    " 'module.blocks.40.ca.v_bias, module.blocks.40.ca.mat_q.bias, module.blocks.40.ca.proj.bias, module.blocks.40.ffn.fc1.bias, module.blocks.40.ffn.fc2.bias, module.blocks.40.ca_norm.bias, '\n"
                    " 'module.blocks.41.ada_gss, module.blocks.41.sa.scale_mul_1H11, module.blocks.41.sa.q_bias, module.blocks.41.sa.v_bias, module.blocks.41.sa.proj.bias, module.blocks.41.ca.v_bias, '\n"
                    " 'module.blocks.41.ca.mat_q.bias, module.blocks.41.ca.proj.bias, module.blocks.41.ffn.fc1.bias, module.blocks.41.ffn.fc2.bias, module.blocks.41.ca_norm.bias, module.blocks.42.ada_gss, '\n"
                    " 'module.blocks.42.sa.scale_mul_1H11, module.blocks.42.sa.q_bias, module.blocks.42.sa.v_bias, module.blocks.42.sa.proj.bias, module.blocks.42.ca.v_bias, module.blocks.42.ca.mat_q.bias, '\n"
                    " 'module.blocks.42.ca.proj.bias, module.blocks.42.ffn.fc1.bias, module.blocks.42.ffn.fc2.bias, module.blocks.42.ca_norm.bias, module.blocks.43.ada_gss, module.blocks.43.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.43.sa.q_bias, module.blocks.43.sa.v_bias, module.blocks.43.sa.proj.bias, module.blocks.43.ca.v_bias, module.blocks.43.ca.mat_q.bias, module.blocks.43.ca.proj.bias, '\n"
                    " 'module.blocks.43.ffn.fc1.bias, module.blocks.43.ffn.fc2.bias, module.blocks.43.ca_norm.bias, module.blocks.44.ada_gss, module.blocks.44.sa.scale_mul_1H11, module.blocks.44.sa.q_bias, '\n"
                    " 'module.blocks.44.sa.v_bias, module.blocks.44.sa.proj.bias, module.blocks.44.ca.v_bias, module.blocks.44.ca.mat_q.bias, module.blocks.44.ca.proj.bias, module.blocks.44.ffn.fc1.bias, '\n"
                    " 'module.blocks.44.ffn.fc2.bias, module.blocks.44.ca_norm.bias, module.blocks.45.ada_gss, module.blocks.45.sa.scale_mul_1H11, module.blocks.45.sa.q_bias, module.blocks.45.sa.v_bias, '\n"
                    " 'module.blocks.45.sa.proj.bias, module.blocks.45.ca.v_bias, module.blocks.45.ca.mat_q.bias, module.blocks.45.ca.proj.bias, module.blocks.45.ffn.fc1.bias, module.blocks.45.ffn.fc2.bias, '\n"
                    " 'module.blocks.45.ca_norm.bias, module.blocks.46.ada_gss, module.blocks.46.sa.scale_mul_1H11, module.blocks.46.sa.q_bias, module.blocks.46.sa.v_bias, module.blocks.46.sa.proj.bias, '\n"
                    " 'module.blocks.46.ca.v_bias, module.blocks.46.ca.mat_q.bias, module.blocks.46.ca.proj.bias, module.blocks.46.ffn.fc1.bias, module.blocks.46.ffn.fc2.bias, module.blocks.46.ca_norm.bias, '\n"
                    " 'module.blocks.47.ada_gss, module.blocks.47.sa.scale_mul_1H11, module.blocks.47.sa.q_bias, module.blocks.47.sa.v_bias, module.blocks.47.sa.proj.bias, module.blocks.47.ca.v_bias, '\n"
                    " 'module.blocks.47.ca.mat_q.bias, module.blocks.47.ca.proj.bias, module.blocks.47.ffn.fc1.bias, module.blocks.47.ffn.fc2.bias, module.blocks.47.ca_norm.bias, module.blocks.48.ada_gss, '\n"
                    " 'module.blocks.48.sa.scale_mul_1H11, module.blocks.48.sa.q_bias, module.blocks.48.sa.v_bias, module.blocks.48.sa.proj.bias, module.blocks.48.ca.v_bias, module.blocks.48.ca.mat_q.bias, '\n"
                    " 'module.blocks.48.ca.proj.bias, module.blocks.48.ffn.fc1.bias, module.blocks.48.ffn.fc2.bias, module.blocks.48.ca_norm.bias, module.blocks.49.ada_gss, module.blocks.49.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.49.sa.q_bias, module.blocks.49.sa.v_bias, module.blocks.49.sa.proj.bias, module.blocks.49.ca.v_bias, module.blocks.49.ca.mat_q.bias, module.blocks.49.ca.proj.bias, '\n"
                    " 'module.blocks.49.ffn.fc1.bias, module.blocks.49.ffn.fc2.bias, module.blocks.49.ca_norm.bias, module.blocks.50.ada_gss, module.blocks.50.sa.scale_mul_1H11, module.blocks.50.sa.q_bias, '\n"
                    " 'module.blocks.50.sa.v_bias, module.blocks.50.sa.proj.bias, module.blocks.50.ca.v_bias, module.blocks.50.ca.mat_q.bias, module.blocks.50.ca.proj.bias, module.blocks.50.ffn.fc1.bias, '\n"
                    " 'module.blocks.50.ffn.fc2.bias, module.blocks.50.ca_norm.bias, module.blocks.51.ada_gss, module.blocks.51.sa.scale_mul_1H11, module.blocks.51.sa.q_bias, module.blocks.51.sa.v_bias, '\n"
                    " 'module.blocks.51.sa.proj.bias, module.blocks.51.ca.v_bias, module.blocks.51.ca.mat_q.bias, module.blocks.51.ca.proj.bias, module.blocks.51.ffn.fc1.bias, module.blocks.51.ffn.fc2.bias, '\n"
                    " 'module.blocks.51.ca_norm.bias, module.blocks.52.ada_gss, module.blocks.52.sa.scale_mul_1H11, module.blocks.52.sa.q_bias, module.blocks.52.sa.v_bias, module.blocks.52.sa.proj.bias, '\n"
                    " 'module.blocks.52.ca.v_bias, module.blocks.52.ca.mat_q.bias, module.blocks.52.ca.proj.bias, module.blocks.52.ffn.fc1.bias, module.blocks.52.ffn.fc2.bias, module.blocks.52.ca_norm.bias, '\n"
                    " 'module.blocks.53.ada_gss, module.blocks.53.sa.scale_mul_1H11, module.blocks.53.sa.q_bias, module.blocks.53.sa.v_bias, module.blocks.53.sa.proj.bias, module.blocks.53.ca.v_bias, '\n"
                    " 'module.blocks.53.ca.mat_q.bias, module.blocks.53.ca.proj.bias, module.blocks.53.ffn.fc1.bias, module.blocks.53.ffn.fc2.bias, module.blocks.53.ca_norm.bias, module.blocks.54.ada_gss, '\n"
                    " 'module.blocks.54.sa.scale_mul_1H11, module.blocks.54.sa.q_bias, module.blocks.54.sa.v_bias, module.blocks.54.sa.proj.bias, module.blocks.54.ca.v_bias, module.blocks.54.ca.mat_q.bias, '\n"
                    " 'module.blocks.54.ca.proj.bias, module.blocks.54.ffn.fc1.bias, module.blocks.54.ffn.fc2.bias, module.blocks.54.ca_norm.bias, module.blocks.55.ada_gss, module.blocks.55.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.55.sa.q_bias, module.blocks.55.sa.v_bias, module.blocks.55.sa.proj.bias, module.blocks.55.ca.v_bias, module.blocks.55.ca.mat_q.bias, module.blocks.55.ca.proj.bias, '\n"
                    " 'module.blocks.55.ffn.fc1.bias, module.blocks.55.ffn.fc2.bias, module.blocks.55.ca_norm.bias')",
          'wd_sc': 0.0}}

[09-01 23:18:07] (nity/utils/lr_control.py, line 116)=> [get_param_groups][rank0] type(model).__name__='NullDDP' count=1085, numel=8573892016
[09-01 23:18:07] (nity/utils/lr_control.py, line 117)=> 
[09-01 23:18:07] (inity_circuit_breaker.py, line 343)=> [vgpt] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.97), fused=True), opt_kw={'lr': 9.375e-05, 'weight_decay': 0}

[09-01 23:18:07] (inity_circuit_breaker.py, line 348)=> Loading T5 from google/flan-t5-xl...
[09-01 23:18:09] (_original/src/trainer.py, line  65)=> self.reweight_loss_by_scale: 1
[09-01 23:18:09] (inity_circuit_breaker.py, line 171)=> global bs=4, local bs=4
[09-01 23:18:09] (inity_circuit_breaker.py, line 172)=> initial args:
{
  local_out_path      : ./local_output
  data_path           : ./data
  bed                 : ./checkpoints
  vae_ckpt            : /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
  exp_name            : experiment
  ds                  : oi
  model               : infinity_8b
  short_cap_prob      : 0.2
  project_name        : infinity_circuit_breaker
  tf32                : True
  auto_resume         : True
  rush_resume         : 
  nowd                : 1
  enable_hybrid_shard : False
  inner_shard_degree  : 1
  zero                : 2
  buck                : chunk
  fsdp_orig           : True
  enable_checkpointing: full-block
  pad_to_multiplier   : 128
  log_every_iter      : False
  checkpoint_type     : torch
  seed                : None
  rand                : True
  task_id             : 123
  trial_id            : 123
  robust_run_id       : 0
  ckpt_trials         : []
  real_trial_id       : 123
  chunk_nodes         : 0
  is_master_node      : None
  log_txt_path        : ./local_output/log.txt
  t5_path             : google/flan-t5-xl
  online_t5           : True
  sdpa_mem            : True
  tfast               : 0
  model_alias         : 8b
  rms                 : False
  aln                 : 0.001
  alng                : 5e-06
  saln                : True
  haln                : True
  nm0                 : False
  tau                 : 1
  cos                 : True
  swi                 : False
  dp                  : -1
  drop                : 0.0
  hd                  : 0
  ca_gamma            : -1
  diva                : 1
  hd0                 : 0.02
  dec                 : 1
  cum                 : 3
  rwe                 : False
  tp                  : 0.0
  tk                  : 0.0
  tini                : 0.010416666666666666
  cfg                 : 0.1
  rand_uncond         : False
  ema                 : 0.9999
  tema                : 0
  fp16                : 2
  fuse                : False
  fused_norm          : False
  flash               : False
  xen                 : False
  use_flex_attn       : True
  stable              : False
  gblr                : 0.0001
  dblr                : 0.0001
  tblr                : 0.006
  glr                 : 1.5625e-06
  dlr                 : 1.5625e-06
  tlr                 : 9.375e-05
  gwd                 : 0.005
  dwd                 : 0.0005
  twd                 : 0.005
  gwde                : 0.005
  dwde                : 0.0005
  twde                : 0.005
  ls                  : 0.0
  lz                  : 0.0
  eq                  : 0
  ep                  : 100
  wp                  : 1e-08
  wp0                 : 0.005
  wpe                 : 1.0
  sche                : lin0
  log_freq            : 50
  gclip               : 6.0
  dclip               : 6.0
  tclip               : 5.0
  cdec                : False
  opt                 : adamw
  ada                 : 0.9_0.97
  dada                : 0.9_0.97
  oeps                : 0
  afuse               : True
  pn                  : 0.06M
  scale_schedule      : None
  patch_size          : None
  resos               : None
  data_load_reso      : None
  workers             : 4
  lbs                 : 4
  bs                  : 4
  batch_size          : 4
  glb_batch_size      : 4
  ac                  : 1
  r_accu              : 1.0
  norm_eps            : 1e-06
  tlen                : 512
  Ct5                 : 2048
  use_bit_label       : 1
  bitloss_type        : mean
  dynamic_resolution_across_gpus: 1
  enable_dynamic_length_prompt: 1
  use_streaming_dataset: 1
  iterable_data_buffersize: 30000
  save_model_iters_freq: 100
  noise_apply_layers  : 13
  noise_apply_strength: 0.3
  noise_apply_requant : 1
  rope2d_each_sa_layer: 1
  rope2d_normalized_by_hw: 2
  use_fsdp_model_ema  : 0
  add_lvl_embeding_only_first_block: 1
  reweight_loss_by_scale: 1
  always_training_scales: 100
  vae_type            : 14
  fake_vae_input      : False
  model_init_device   : cuda
  prefetch_factor     : 2
  apply_spatial_patchify: 1
  debug_bsc           : 0
  task_type           : t2i
  target_layers       : [10, 12, 14, 16, 18, 20]
  transform_layers    : [10, 12, 14, 16, 18, 20]
  lorra_alpha         : 5.0
  trainsets           : ['AlpacaSupervisedDataset', 'HarmfulDataset']
  valsets             : ['AlpacaSupervisedDataset', 'HarmfulDataset']
  adv_string          : Sure here's
  full_layers         : False
  harmful_prompts_path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
  sanitized_prompts_path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
  validation_ratio    : 0.1
  category            : hate
  lora_r              : 8
  lora_alpha          : 16
  lora_dropout        : 0.05
  lora_target_modules : ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
  lora_weight_path    : 
  lora_bias           : none
  q_lora              : False
  model_name_or_path  : meta-llama/Llama-2-7b-chat-hf
  adapter_name_or_path: 
  use_lora            : True
  cache_dir           : 
  optim               : adamw_torch
  model_max_length    : 512
  grouped_to_max_length: False
  use_refusal_retain  : True
  sc_train_subset     : ['']
  log_every           : 10
  sc_train_seq_type   : all_text
  coeff_schedule      : linear_converge
  sc_loss_type        : orig_act_dotprod
  branch              : master
  commit_id           : 
  commit_msg          : 
  cmd                 : --bed ./checkpoints --local_out_path ./local_output --project_name infinity_circuit_breaker --exp_name experiment --ep 100 --opt adamw --cum 3 --sche lin0 --fp16 2 --ada 0.9_0.97 --tini -1 --tclip 5 --flash 0 --alng 5e-06 --saln 1 --cos 1 --enable_checkpointing full-block --tblr 6e-3 --pn 0.06M --lbs 4 --workers 4 --short_cap_prob 0.2 --online_t5 1 --use_streaming_dataset 1 --iterable_data_buffersize 30000 --Ct5 2048 --t5_path google/flan-t5-xl --vae_type 14 --wp 0.00000001 --wpe 1 --dynamic_resolution_across_gpus 1 --enable_dynamic_length_prompt 1 --reweight_loss_by_scale 1 --add_lvl_embeding_only_first_block 1 --rope2d_each_sa_layer 1 --rope2d_normalized_by_hw 2 --use_fsdp_model_ema 0 --always_training_scales 100 --use_bit_label 1 --zero 2 --save_model_iters_freq 100 --log_freq 50 --checkpoint_type torch --prefetch_factor 2 --noise_apply_strength 0.3 --noise_apply_layers 13 --apply_spatial_patchify 1 --use_flex_attn True --pad 128 --data_path ./data --data_load_reso 512 --tlen 512 --workers 4 --prefetch_factor 2 --harmful_prompts_path /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts --sanitized_prompts_path /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts --validation_ratio 0.1 --category hate --target_layers 10,12,14,16,18,20 --transform_layers 10,12,14,16,18,20 --lorra_alpha 5.0 --trainsets AlpacaSupervisedDataset#HarmfulDataset --valsets AlpacaSupervisedDataset#HarmfulDataset --adv_string Sure here's --full_layers False --lora_r 8 --lora_alpha 16 --lora_dropout 0.05 --lora_target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj --lora_weight_path  --lora_bias none --q_lora False --model_name_or_path meta-llama/Llama-2-7b-chat-hf --adapter_name_or_path  --use_lora True --cache_dir  --optim adamw_torch --model_max_length 512 --grouped_to_max_length False --use_refusal_retain True --sc_train_subset  --log_every 10 --sc_train_seq_type all_text --coeff_schedule linear_converge --sc_loss_type orig_act_dotprod
  tag                 : UK
  acc_all             : None
  acc_real            : None
  acc_fake            : None
  last_Lnll           : None
  last_L1             : None
  last_Ld             : None
  last_wei_g          : None
  grad_boom           : None
  diff                : None
  diffs               : 
  diffs_ema           : None
  ca_performance      : 
  cur_phase           : 
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  iter_speed          : None
  img_per_day         : None
  max_nvidia_smi      : 0
  max_memory_allocated: None
  max_memory_reserved : None
  num_alloc_retries   : None
  MFU                 : None
  HFU                 : None
  dbg_modified        : False
  dbg_ks              : False
  dbg_ks_last         : None
  dbg                 : False
  ks                  : False
  nodata              : False
  nodata_tlen         : 320
  nova                : False
  prof                : 0
  prof_freq           : 50
  tos_profiler_file_prefix: vgpt_default/
  profall             : 0
  v_seed              : 0
  g_seed              : 0
}

[09-01 23:18:09] (inity_circuit_breaker.py, line 182)=> start_it=0, iters_train=100
[09-01 23:18:09] (inity_circuit_breaker.py, line 466)=> [PT info]  from ep0 it0, acc_str: [no acc str], diffs: ,    =======>  bed: ./checkpoints  <=======








=======================================================   RESTART [09-01 23:24:58]   =======================================================
[09-01 23:24:58] (inity_circuit_breaker.py, line 718)=> ============================================================
[09-01 23:24:58] (inity_circuit_breaker.py, line 719)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[09-01 23:24:58] (inity_circuit_breaker.py, line 720)=> Circuit Breaker Alpha: 0.1
[09-01 23:24:58] (inity_circuit_breaker.py, line 721)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[09-01 23:24:58] (inity_circuit_breaker.py, line 722)=> Circuit Breaker Enabled: True
[09-01 23:24:58] (inity_circuit_breaker.py, line 723)=> Number of Examples: 1000
[09-01 23:24:58] (inity_circuit_breaker.py, line 724)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[09-01 23:24:58] (inity_circuit_breaker.py, line 725)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[09-01 23:24:58] (inity_circuit_breaker.py, line 726)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[09-01 23:24:58] (inity_circuit_breaker.py, line 727)=> Validation Ratio: 0.1
[09-01 23:24:58] (inity_circuit_breaker.py, line 728)=> Category: hate
[09-01 23:24:58] (inity_circuit_breaker.py, line 729)=> Batch Size: 4
[09-01 23:24:58] (inity_circuit_breaker.py, line 730)=> Workers: 4
[09-01 23:24:58] (inity_circuit_breaker.py, line 731)=> Device: cuda:0
[09-01 23:24:58] (inity_circuit_breaker.py, line 732)=> Model: infinity_8b
[09-01 23:24:58] (inity_circuit_breaker.py, line 733)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-01 23:24:58] (inity_circuit_breaker.py, line 734)=> Rush Resume: 
[09-01 23:24:58] (inity_circuit_breaker.py, line 735)=> Selective Layers: 0,1,2,3,4,5
[09-01 23:24:58] (inity_circuit_breaker.py, line 736)=> ============================================================
[09-01 23:24:58] (inity_circuit_breaker.py, line 377)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[09-01 23:24:58] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-01 23:24:58] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-01 23:24:58] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[09-01 23:24:58] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-01 23:24:58] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-01 23:24:59] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[09-01 23:24:59] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[09-01 23:24:59] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[09-01 23:24:59] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[09-01 23:24:59] (inity_circuit_breaker.py, line 395)=> args.batch_size=4, vbs=6
[09-01 23:24:59] (inity_circuit_breaker.py, line 418)=> len(dataloader): 100, len(dataset): 100
[09-01 23:24:59] (inity_circuit_breaker.py, line 420)=> total_samples: 200
[09-01 23:24:59] (inity_circuit_breaker.py, line 421)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[09-01 23:24:59] (inity_circuit_breaker.py, line 140)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[09-01 23:24:59] (inity_circuit_breaker.py, line 142)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[09-01 23:24:59] (inity_circuit_breaker.py, line 146)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-01 23:25:01] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[09-01 23:25:01] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[09-01 23:25:01] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[09-01 23:25:01] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[09-01 23:25:04] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[09-01 23:25:06] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[09-01 23:25:06] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[09-01 23:25:06] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[09-01 23:25:06] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[09-01 23:25:06] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

[09-01 23:25:16] (ity/models/init_param.py, line  15)=> [init_weights] Infinity with std=0.02
[09-01 23:25:17] (inity_circuit_breaker.py, line 246)=> [PT] GPT model = Infinity(
  drop_path_rate=0.2
  (norm0_cond): Identity()
  (text_norm): FastRMSNorm(C=2048, eps=1e-06, elementwise_affine=True)
  (text_proj_for_sos): TextAttentivePool(
    (ca): CrossAttention(
      Cq=3072, Ckv=2048, cos_attn=False
      (mat_kv): Linear(in_features=2048, out_features=6144, bias=False)
      (proj): Linear(in_features=3072, out_features=3072, bias=True)
      (proj_drop): Identity()
    )
  )
  (text_proj_for_ca): Sequential(
    (0): Linear(in_features=2048, out_features=3072, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=3072, out_features=3072, bias=True)
  )
  (lvl_embed): Embedding(15, 3072)
  (norm0_ve): Identity()
  (word_embed): Linear(in_features=56, out_features=3072, bias=True)
  (shared_ada_lin): Sequential(
    (0): SiLU()
    (1): SharedAdaLin(in_features=3072, out_features=18432, bias=True)
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=3072, out_features=6144, bias=True)
    )
  )
  (head): Linear(in_features=3072, out_features=112, bias=True)
  (blocks): ModuleList(
    (0): CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): Identity()
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
    (1-55): 55 x CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): DropPath(...)
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
  )
)


[09-01 23:25:17] (inity_circuit_breaker.py, line 248)=> [PT][#para] VAE=81.62, VAE.quant=0.00
[09-01 23:25:17] (inity_circuit_breaker.py, line 251)=> [PT][#para] GPT=8573.89


[09-01 23:25:17] (nity/utils/lr_control.py, line  71)=> [get_param_groups][lr decay] with_lr_scale=False, lr_scale=0.0
[09-01 23:25:17] (nity/utils/lr_control.py, line 111)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': '[no scale]',
         'params': "('module.text_norm.weight, module.text_proj_for_sos.ca.mat_kv.weight, module.text_proj_for_sos.ca.proj.weight, module.text_proj_for_ca.0.weight, module.text_proj_for_ca.2.weight, '\n"
                   " 'module.word_embed.weight, module.shared_ada_lin.1.weight, module.head_nm.ada_lin.1.weight, module.head.weight, module.blocks.0.sa.mat_qkv.weight, module.blocks.0.sa.proj.weight, '\n"
                   " 'module.blocks.0.ca.mat_q.weight, module.blocks.0.ca.mat_kv.weight, module.blocks.0.ca.proj.weight, module.blocks.0.ffn.fc1.weight, module.blocks.0.ffn.fc2.weight, module.blocks.0.ca_norm.weight, '\n"
                   " 'module.blocks.1.sa.mat_qkv.weight, module.blocks.1.sa.proj.weight, module.blocks.1.ca.mat_q.weight, module.blocks.1.ca.mat_kv.weight, module.blocks.1.ca.proj.weight, '\n"
                   " 'module.blocks.1.ffn.fc1.weight, module.blocks.1.ffn.fc2.weight, module.blocks.1.ca_norm.weight, module.blocks.2.sa.mat_qkv.weight, module.blocks.2.sa.proj.weight, module.blocks.2.ca.mat_q.weight, '\n"
                   " 'module.blocks.2.ca.mat_kv.weight, module.blocks.2.ca.proj.weight, module.blocks.2.ffn.fc1.weight, module.blocks.2.ffn.fc2.weight, module.blocks.2.ca_norm.weight, module.blocks.3.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.3.sa.proj.weight, module.blocks.3.ca.mat_q.weight, module.blocks.3.ca.mat_kv.weight, module.blocks.3.ca.proj.weight, module.blocks.3.ffn.fc1.weight, module.blocks.3.ffn.fc2.weight, '\n"
                   " 'module.blocks.3.ca_norm.weight, module.blocks.4.sa.mat_qkv.weight, module.blocks.4.sa.proj.weight, module.blocks.4.ca.mat_q.weight, module.blocks.4.ca.mat_kv.weight, '\n"
                   " 'module.blocks.4.ca.proj.weight, module.blocks.4.ffn.fc1.weight, module.blocks.4.ffn.fc2.weight, module.blocks.4.ca_norm.weight, module.blocks.5.sa.mat_qkv.weight, module.blocks.5.sa.proj.weight, '\n"
                   " 'module.blocks.5.ca.mat_q.weight, module.blocks.5.ca.mat_kv.weight, module.blocks.5.ca.proj.weight, module.blocks.5.ffn.fc1.weight, module.blocks.5.ffn.fc2.weight, module.blocks.5.ca_norm.weight, '\n"
                   " 'module.blocks.6.sa.mat_qkv.weight, module.blocks.6.sa.proj.weight, module.blocks.6.ca.mat_q.weight, module.blocks.6.ca.mat_kv.weight, module.blocks.6.ca.proj.weight, '\n"
                   " 'module.blocks.6.ffn.fc1.weight, module.blocks.6.ffn.fc2.weight, module.blocks.6.ca_norm.weight, module.blocks.7.sa.mat_qkv.weight, module.blocks.7.sa.proj.weight, module.blocks.7.ca.mat_q.weight, '\n"
                   " 'module.blocks.7.ca.mat_kv.weight, module.blocks.7.ca.proj.weight, module.blocks.7.ffn.fc1.weight, module.blocks.7.ffn.fc2.weight, module.blocks.7.ca_norm.weight, module.blocks.8.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.8.sa.proj.weight, module.blocks.8.ca.mat_q.weight, module.blocks.8.ca.mat_kv.weight, module.blocks.8.ca.proj.weight, module.blocks.8.ffn.fc1.weight, module.blocks.8.ffn.fc2.weight, '\n"
                   " 'module.blocks.8.ca_norm.weight, module.blocks.9.sa.mat_qkv.weight, module.blocks.9.sa.proj.weight, module.blocks.9.ca.mat_q.weight, module.blocks.9.ca.mat_kv.weight, '\n"
                   " 'module.blocks.9.ca.proj.weight, module.blocks.9.ffn.fc1.weight, module.blocks.9.ffn.fc2.weight, module.blocks.9.ca_norm.weight, module.blocks.10.sa.mat_qkv.weight, module.blocks.10.sa.proj.weight, '\n"
                   " 'module.blocks.10.ca.mat_q.weight, module.blocks.10.ca.mat_kv.weight, module.blocks.10.ca.proj.weight, module.blocks.10.ffn.fc1.weight, module.blocks.10.ffn.fc2.weight, '\n"
                   " 'module.blocks.10.ca_norm.weight, module.blocks.11.sa.mat_qkv.weight, module.blocks.11.sa.proj.weight, module.blocks.11.ca.mat_q.weight, module.blocks.11.ca.mat_kv.weight, '\n"
                   " 'module.blocks.11.ca.proj.weight, module.blocks.11.ffn.fc1.weight, module.blocks.11.ffn.fc2.weight, module.blocks.11.ca_norm.weight, module.blocks.12.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.12.sa.proj.weight, module.blocks.12.ca.mat_q.weight, module.blocks.12.ca.mat_kv.weight, module.blocks.12.ca.proj.weight, module.blocks.12.ffn.fc1.weight, '\n"
                   " 'module.blocks.12.ffn.fc2.weight, module.blocks.12.ca_norm.weight, module.blocks.13.sa.mat_qkv.weight, module.blocks.13.sa.proj.weight, module.blocks.13.ca.mat_q.weight, '\n"
                   " 'module.blocks.13.ca.mat_kv.weight, module.blocks.13.ca.proj.weight, module.blocks.13.ffn.fc1.weight, module.blocks.13.ffn.fc2.weight, module.blocks.13.ca_norm.weight, '\n"
                   " 'module.blocks.14.sa.mat_qkv.weight, module.blocks.14.sa.proj.weight, module.blocks.14.ca.mat_q.weight, module.blocks.14.ca.mat_kv.weight, module.blocks.14.ca.proj.weight, '\n"
                   " 'module.blocks.14.ffn.fc1.weight, module.blocks.14.ffn.fc2.weight, module.blocks.14.ca_norm.weight, module.blocks.15.sa.mat_qkv.weight, module.blocks.15.sa.proj.weight, '\n"
                   " 'module.blocks.15.ca.mat_q.weight, module.blocks.15.ca.mat_kv.weight, module.blocks.15.ca.proj.weight, module.blocks.15.ffn.fc1.weight, module.blocks.15.ffn.fc2.weight, '\n"
                   " 'module.blocks.15.ca_norm.weight, module.blocks.16.sa.mat_qkv.weight, module.blocks.16.sa.proj.weight, module.blocks.16.ca.mat_q.weight, module.blocks.16.ca.mat_kv.weight, '\n"
                   " 'module.blocks.16.ca.proj.weight, module.blocks.16.ffn.fc1.weight, module.blocks.16.ffn.fc2.weight, module.blocks.16.ca_norm.weight, module.blocks.17.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.17.sa.proj.weight, module.blocks.17.ca.mat_q.weight, module.blocks.17.ca.mat_kv.weight, module.blocks.17.ca.proj.weight, module.blocks.17.ffn.fc1.weight, '\n"
                   " 'module.blocks.17.ffn.fc2.weight, module.blocks.17.ca_norm.weight, module.blocks.18.sa.mat_qkv.weight, module.blocks.18.sa.proj.weight, module.blocks.18.ca.mat_q.weight, '\n"
                   " 'module.blocks.18.ca.mat_kv.weight, module.blocks.18.ca.proj.weight, module.blocks.18.ffn.fc1.weight, module.blocks.18.ffn.fc2.weight, module.blocks.18.ca_norm.weight, '\n"
                   " 'module.blocks.19.sa.mat_qkv.weight, module.blocks.19.sa.proj.weight, module.blocks.19.ca.mat_q.weight, module.blocks.19.ca.mat_kv.weight, module.blocks.19.ca.proj.weight, '\n"
                   " 'module.blocks.19.ffn.fc1.weight, module.blocks.19.ffn.fc2.weight, module.blocks.19.ca_norm.weight, module.blocks.20.sa.mat_qkv.weight, module.blocks.20.sa.proj.weight, '\n"
                   " 'module.blocks.20.ca.mat_q.weight, module.blocks.20.ca.mat_kv.weight, module.blocks.20.ca.proj.weight, module.blocks.20.ffn.fc1.weight, module.blocks.20.ffn.fc2.weight, '\n"
                   " 'module.blocks.20.ca_norm.weight, module.blocks.21.sa.mat_qkv.weight, module.blocks.21.sa.proj.weight, module.blocks.21.ca.mat_q.weight, module.blocks.21.ca.mat_kv.weight, '\n"
                   " 'module.blocks.21.ca.proj.weight, module.blocks.21.ffn.fc1.weight, module.blocks.21.ffn.fc2.weight, module.blocks.21.ca_norm.weight, module.blocks.22.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.22.sa.proj.weight, module.blocks.22.ca.mat_q.weight, module.blocks.22.ca.mat_kv.weight, module.blocks.22.ca.proj.weight, module.blocks.22.ffn.fc1.weight, '\n"
                   " 'module.blocks.22.ffn.fc2.weight, module.blocks.22.ca_norm.weight, module.blocks.23.sa.mat_qkv.weight, module.blocks.23.sa.proj.weight, module.blocks.23.ca.mat_q.weight, '\n"
                   " 'module.blocks.23.ca.mat_kv.weight, module.blocks.23.ca.proj.weight, module.blocks.23.ffn.fc1.weight, module.blocks.23.ffn.fc2.weight, module.blocks.23.ca_norm.weight, '\n"
                   " 'module.blocks.24.sa.mat_qkv.weight, module.blocks.24.sa.proj.weight, module.blocks.24.ca.mat_q.weight, module.blocks.24.ca.mat_kv.weight, module.blocks.24.ca.proj.weight, '\n"
                   " 'module.blocks.24.ffn.fc1.weight, module.blocks.24.ffn.fc2.weight, module.blocks.24.ca_norm.weight, module.blocks.25.sa.mat_qkv.weight, module.blocks.25.sa.proj.weight, '\n"
                   " 'module.blocks.25.ca.mat_q.weight, module.blocks.25.ca.mat_kv.weight, module.blocks.25.ca.proj.weight, module.blocks.25.ffn.fc1.weight, module.blocks.25.ffn.fc2.weight, '\n"
                   " 'module.blocks.25.ca_norm.weight, module.blocks.26.sa.mat_qkv.weight, module.blocks.26.sa.proj.weight, module.blocks.26.ca.mat_q.weight, module.blocks.26.ca.mat_kv.weight, '\n"
                   " 'module.blocks.26.ca.proj.weight, module.blocks.26.ffn.fc1.weight, module.blocks.26.ffn.fc2.weight, module.blocks.26.ca_norm.weight, module.blocks.27.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.27.sa.proj.weight, module.blocks.27.ca.mat_q.weight, module.blocks.27.ca.mat_kv.weight, module.blocks.27.ca.proj.weight, module.blocks.27.ffn.fc1.weight, '\n"
                   " 'module.blocks.27.ffn.fc2.weight, module.blocks.27.ca_norm.weight, module.blocks.28.sa.mat_qkv.weight, module.blocks.28.sa.proj.weight, module.blocks.28.ca.mat_q.weight, '\n"
                   " 'module.blocks.28.ca.mat_kv.weight, module.blocks.28.ca.proj.weight, module.blocks.28.ffn.fc1.weight, module.blocks.28.ffn.fc2.weight, module.blocks.28.ca_norm.weight, '\n"
                   " 'module.blocks.29.sa.mat_qkv.weight, module.blocks.29.sa.proj.weight, module.blocks.29.ca.mat_q.weight, module.blocks.29.ca.mat_kv.weight, module.blocks.29.ca.proj.weight, '\n"
                   " 'module.blocks.29.ffn.fc1.weight, module.blocks.29.ffn.fc2.weight, module.blocks.29.ca_norm.weight, module.blocks.30.sa.mat_qkv.weight, module.blocks.30.sa.proj.weight, '\n"
                   " 'module.blocks.30.ca.mat_q.weight, module.blocks.30.ca.mat_kv.weight, module.blocks.30.ca.proj.weight, module.blocks.30.ffn.fc1.weight, module.blocks.30.ffn.fc2.weight, '\n"
                   " 'module.blocks.30.ca_norm.weight, module.blocks.31.sa.mat_qkv.weight, module.blocks.31.sa.proj.weight, module.blocks.31.ca.mat_q.weight, module.blocks.31.ca.mat_kv.weight, '\n"
                   " 'module.blocks.31.ca.proj.weight, module.blocks.31.ffn.fc1.weight, module.blocks.31.ffn.fc2.weight, module.blocks.31.ca_norm.weight, module.blocks.32.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.32.sa.proj.weight, module.blocks.32.ca.mat_q.weight, module.blocks.32.ca.mat_kv.weight, module.blocks.32.ca.proj.weight, module.blocks.32.ffn.fc1.weight, '\n"
                   " 'module.blocks.32.ffn.fc2.weight, module.blocks.32.ca_norm.weight, module.blocks.33.sa.mat_qkv.weight, module.blocks.33.sa.proj.weight, module.blocks.33.ca.mat_q.weight, '\n"
                   " 'module.blocks.33.ca.mat_kv.weight, module.blocks.33.ca.proj.weight, module.blocks.33.ffn.fc1.weight, module.blocks.33.ffn.fc2.weight, module.blocks.33.ca_norm.weight, '\n"
                   " 'module.blocks.34.sa.mat_qkv.weight, module.blocks.34.sa.proj.weight, module.blocks.34.ca.mat_q.weight, module.blocks.34.ca.mat_kv.weight, module.blocks.34.ca.proj.weight, '\n"
                   " 'module.blocks.34.ffn.fc1.weight, module.blocks.34.ffn.fc2.weight, module.blocks.34.ca_norm.weight, module.blocks.35.sa.mat_qkv.weight, module.blocks.35.sa.proj.weight, '\n"
                   " 'module.blocks.35.ca.mat_q.weight, module.blocks.35.ca.mat_kv.weight, module.blocks.35.ca.proj.weight, module.blocks.35.ffn.fc1.weight, module.blocks.35.ffn.fc2.weight, '\n"
                   " 'module.blocks.35.ca_norm.weight, module.blocks.36.sa.mat_qkv.weight, module.blocks.36.sa.proj.weight, module.blocks.36.ca.mat_q.weight, module.blocks.36.ca.mat_kv.weight, '\n"
                   " 'module.blocks.36.ca.proj.weight, module.blocks.36.ffn.fc1.weight, module.blocks.36.ffn.fc2.weight, module.blocks.36.ca_norm.weight, module.blocks.37.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.37.sa.proj.weight, module.blocks.37.ca.mat_q.weight, module.blocks.37.ca.mat_kv.weight, module.blocks.37.ca.proj.weight, module.blocks.37.ffn.fc1.weight, '\n"
                   " 'module.blocks.37.ffn.fc2.weight, module.blocks.37.ca_norm.weight, module.blocks.38.sa.mat_qkv.weight, module.blocks.38.sa.proj.weight, module.blocks.38.ca.mat_q.weight, '\n"
                   " 'module.blocks.38.ca.mat_kv.weight, module.blocks.38.ca.proj.weight, module.blocks.38.ffn.fc1.weight, module.blocks.38.ffn.fc2.weight, module.blocks.38.ca_norm.weight, '\n"
                   " 'module.blocks.39.sa.mat_qkv.weight, module.blocks.39.sa.proj.weight, module.blocks.39.ca.mat_q.weight, module.blocks.39.ca.mat_kv.weight, module.blocks.39.ca.proj.weight, '\n"
                   " 'module.blocks.39.ffn.fc1.weight, module.blocks.39.ffn.fc2.weight, module.blocks.39.ca_norm.weight, module.blocks.40.sa.mat_qkv.weight, module.blocks.40.sa.proj.weight, '\n"
                   " 'module.blocks.40.ca.mat_q.weight, module.blocks.40.ca.mat_kv.weight, module.blocks.40.ca.proj.weight, module.blocks.40.ffn.fc1.weight, module.blocks.40.ffn.fc2.weight, '\n"
                   " 'module.blocks.40.ca_norm.weight, module.blocks.41.sa.mat_qkv.weight, module.blocks.41.sa.proj.weight, module.blocks.41.ca.mat_q.weight, module.blocks.41.ca.mat_kv.weight, '\n"
                   " 'module.blocks.41.ca.proj.weight, module.blocks.41.ffn.fc1.weight, module.blocks.41.ffn.fc2.weight, module.blocks.41.ca_norm.weight, module.blocks.42.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.42.sa.proj.weight, module.blocks.42.ca.mat_q.weight, module.blocks.42.ca.mat_kv.weight, module.blocks.42.ca.proj.weight, module.blocks.42.ffn.fc1.weight, '\n"
                   " 'module.blocks.42.ffn.fc2.weight, module.blocks.42.ca_norm.weight, module.blocks.43.sa.mat_qkv.weight, module.blocks.43.sa.proj.weight, module.blocks.43.ca.mat_q.weight, '\n"
                   " 'module.blocks.43.ca.mat_kv.weight, module.blocks.43.ca.proj.weight, module.blocks.43.ffn.fc1.weight, module.blocks.43.ffn.fc2.weight, module.blocks.43.ca_norm.weight, '\n"
                   " 'module.blocks.44.sa.mat_qkv.weight, module.blocks.44.sa.proj.weight, module.blocks.44.ca.mat_q.weight, module.blocks.44.ca.mat_kv.weight, module.blocks.44.ca.proj.weight, '\n"
                   " 'module.blocks.44.ffn.fc1.weight, module.blocks.44.ffn.fc2.weight, module.blocks.44.ca_norm.weight, module.blocks.45.sa.mat_qkv.weight, module.blocks.45.sa.proj.weight, '\n"
                   " 'module.blocks.45.ca.mat_q.weight, module.blocks.45.ca.mat_kv.weight, module.blocks.45.ca.proj.weight, module.blocks.45.ffn.fc1.weight, module.blocks.45.ffn.fc2.weight, '\n"
                   " 'module.blocks.45.ca_norm.weight, module.blocks.46.sa.mat_qkv.weight, module.blocks.46.sa.proj.weight, module.blocks.46.ca.mat_q.weight, module.blocks.46.ca.mat_kv.weight, '\n"
                   " 'module.blocks.46.ca.proj.weight, module.blocks.46.ffn.fc1.weight, module.blocks.46.ffn.fc2.weight, module.blocks.46.ca_norm.weight, module.blocks.47.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.47.sa.proj.weight, module.blocks.47.ca.mat_q.weight, module.blocks.47.ca.mat_kv.weight, module.blocks.47.ca.proj.weight, module.blocks.47.ffn.fc1.weight, '\n"
                   " 'module.blocks.47.ffn.fc2.weight, module.blocks.47.ca_norm.weight, module.blocks.48.sa.mat_qkv.weight, module.blocks.48.sa.proj.weight, module.blocks.48.ca.mat_q.weight, '\n"
                   " 'module.blocks.48.ca.mat_kv.weight, module.blocks.48.ca.proj.weight, module.blocks.48.ffn.fc1.weight, module.blocks.48.ffn.fc2.weight, module.blocks.48.ca_norm.weight, '\n"
                   " 'module.blocks.49.sa.mat_qkv.weight, module.blocks.49.sa.proj.weight, module.blocks.49.ca.mat_q.weight, module.blocks.49.ca.mat_kv.weight, module.blocks.49.ca.proj.weight, '\n"
                   " 'module.blocks.49.ffn.fc1.weight, module.blocks.49.ffn.fc2.weight, module.blocks.49.ca_norm.weight, module.blocks.50.sa.mat_qkv.weight, module.blocks.50.sa.proj.weight, '\n"
                   " 'module.blocks.50.ca.mat_q.weight, module.blocks.50.ca.mat_kv.weight, module.blocks.50.ca.proj.weight, module.blocks.50.ffn.fc1.weight, module.blocks.50.ffn.fc2.weight, '\n"
                   " 'module.blocks.50.ca_norm.weight, module.blocks.51.sa.mat_qkv.weight, module.blocks.51.sa.proj.weight, module.blocks.51.ca.mat_q.weight, module.blocks.51.ca.mat_kv.weight, '\n"
                   " 'module.blocks.51.ca.proj.weight, module.blocks.51.ffn.fc1.weight, module.blocks.51.ffn.fc2.weight, module.blocks.51.ca_norm.weight, module.blocks.52.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.52.sa.proj.weight, module.blocks.52.ca.mat_q.weight, module.blocks.52.ca.mat_kv.weight, module.blocks.52.ca.proj.weight, module.blocks.52.ffn.fc1.weight, '\n"
                   " 'module.blocks.52.ffn.fc2.weight, module.blocks.52.ca_norm.weight, module.blocks.53.sa.mat_qkv.weight, module.blocks.53.sa.proj.weight, module.blocks.53.ca.mat_q.weight, '\n"
                   " 'module.blocks.53.ca.mat_kv.weight, module.blocks.53.ca.proj.weight, module.blocks.53.ffn.fc1.weight, module.blocks.53.ffn.fc2.weight, module.blocks.53.ca_norm.weight, '\n"
                   " 'module.blocks.54.sa.mat_qkv.weight, module.blocks.54.sa.proj.weight, module.blocks.54.ca.mat_q.weight, module.blocks.54.ca.mat_kv.weight, module.blocks.54.ca.proj.weight, '\n"
                   " 'module.blocks.54.ffn.fc1.weight, module.blocks.54.ffn.fc2.weight, module.blocks.54.ca_norm.weight, module.blocks.55.sa.mat_qkv.weight, module.blocks.55.sa.proj.weight, '\n"
                   " 'module.blocks.55.ca.mat_q.weight, module.blocks.55.ca.mat_kv.weight, module.blocks.55.ca.proj.weight, module.blocks.55.ffn.fc1.weight, module.blocks.55.ffn.fc2.weight, '\n"
                   " 'module.blocks.55.ca_norm.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': '[no scale]',
          'params': "('module.cfg_uncond, module.pos_start, module.text_proj_for_sos.ca.mat_q, module.text_proj_for_sos.ca.v_bias, module.text_proj_for_sos.ca.proj.bias, module.text_proj_for_ca.0.bias, '\n"
                    " 'module.text_proj_for_ca.2.bias, module.lvl_embed.weight, module.word_embed.bias, module.shared_ada_lin.1.bias, module.head_nm.ada_lin.1.bias, module.head.bias, module.blocks.0.ada_gss, '\n"
                    " 'module.blocks.0.sa.scale_mul_1H11, module.blocks.0.sa.q_bias, module.blocks.0.sa.v_bias, module.blocks.0.sa.proj.bias, module.blocks.0.ca.v_bias, module.blocks.0.ca.mat_q.bias, '\n"
                    " 'module.blocks.0.ca.proj.bias, module.blocks.0.ffn.fc1.bias, module.blocks.0.ffn.fc2.bias, module.blocks.0.ca_norm.bias, module.blocks.1.ada_gss, module.blocks.1.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.1.sa.q_bias, module.blocks.1.sa.v_bias, module.blocks.1.sa.proj.bias, module.blocks.1.ca.v_bias, module.blocks.1.ca.mat_q.bias, module.blocks.1.ca.proj.bias, '\n"
                    " 'module.blocks.1.ffn.fc1.bias, module.blocks.1.ffn.fc2.bias, module.blocks.1.ca_norm.bias, module.blocks.2.ada_gss, module.blocks.2.sa.scale_mul_1H11, module.blocks.2.sa.q_bias, '\n"
                    " 'module.blocks.2.sa.v_bias, module.blocks.2.sa.proj.bias, module.blocks.2.ca.v_bias, module.blocks.2.ca.mat_q.bias, module.blocks.2.ca.proj.bias, module.blocks.2.ffn.fc1.bias, '\n"
                    " 'module.blocks.2.ffn.fc2.bias, module.blocks.2.ca_norm.bias, module.blocks.3.ada_gss, module.blocks.3.sa.scale_mul_1H11, module.blocks.3.sa.q_bias, module.blocks.3.sa.v_bias, '\n"
                    " 'module.blocks.3.sa.proj.bias, module.blocks.3.ca.v_bias, module.blocks.3.ca.mat_q.bias, module.blocks.3.ca.proj.bias, module.blocks.3.ffn.fc1.bias, module.blocks.3.ffn.fc2.bias, '\n"
                    " 'module.blocks.3.ca_norm.bias, module.blocks.4.ada_gss, module.blocks.4.sa.scale_mul_1H11, module.blocks.4.sa.q_bias, module.blocks.4.sa.v_bias, module.blocks.4.sa.proj.bias, '\n"
                    " 'module.blocks.4.ca.v_bias, module.blocks.4.ca.mat_q.bias, module.blocks.4.ca.proj.bias, module.blocks.4.ffn.fc1.bias, module.blocks.4.ffn.fc2.bias, module.blocks.4.ca_norm.bias, '\n"
                    " 'module.blocks.5.ada_gss, module.blocks.5.sa.scale_mul_1H11, module.blocks.5.sa.q_bias, module.blocks.5.sa.v_bias, module.blocks.5.sa.proj.bias, module.blocks.5.ca.v_bias, '\n"
                    " 'module.blocks.5.ca.mat_q.bias, module.blocks.5.ca.proj.bias, module.blocks.5.ffn.fc1.bias, module.blocks.5.ffn.fc2.bias, module.blocks.5.ca_norm.bias, module.blocks.6.ada_gss, '\n"
                    " 'module.blocks.6.sa.scale_mul_1H11, module.blocks.6.sa.q_bias, module.blocks.6.sa.v_bias, module.blocks.6.sa.proj.bias, module.blocks.6.ca.v_bias, module.blocks.6.ca.mat_q.bias, '\n"
                    " 'module.blocks.6.ca.proj.bias, module.blocks.6.ffn.fc1.bias, module.blocks.6.ffn.fc2.bias, module.blocks.6.ca_norm.bias, module.blocks.7.ada_gss, module.blocks.7.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.7.sa.q_bias, module.blocks.7.sa.v_bias, module.blocks.7.sa.proj.bias, module.blocks.7.ca.v_bias, module.blocks.7.ca.mat_q.bias, module.blocks.7.ca.proj.bias, '\n"
                    " 'module.blocks.7.ffn.fc1.bias, module.blocks.7.ffn.fc2.bias, module.blocks.7.ca_norm.bias, module.blocks.8.ada_gss, module.blocks.8.sa.scale_mul_1H11, module.blocks.8.sa.q_bias, '\n"
                    " 'module.blocks.8.sa.v_bias, module.blocks.8.sa.proj.bias, module.blocks.8.ca.v_bias, module.blocks.8.ca.mat_q.bias, module.blocks.8.ca.proj.bias, module.blocks.8.ffn.fc1.bias, '\n"
                    " 'module.blocks.8.ffn.fc2.bias, module.blocks.8.ca_norm.bias, module.blocks.9.ada_gss, module.blocks.9.sa.scale_mul_1H11, module.blocks.9.sa.q_bias, module.blocks.9.sa.v_bias, '\n"
                    " 'module.blocks.9.sa.proj.bias, module.blocks.9.ca.v_bias, module.blocks.9.ca.mat_q.bias, module.blocks.9.ca.proj.bias, module.blocks.9.ffn.fc1.bias, module.blocks.9.ffn.fc2.bias, '\n"
                    " 'module.blocks.9.ca_norm.bias, module.blocks.10.ada_gss, module.blocks.10.sa.scale_mul_1H11, module.blocks.10.sa.q_bias, module.blocks.10.sa.v_bias, module.blocks.10.sa.proj.bias, '\n"
                    " 'module.blocks.10.ca.v_bias, module.blocks.10.ca.mat_q.bias, module.blocks.10.ca.proj.bias, module.blocks.10.ffn.fc1.bias, module.blocks.10.ffn.fc2.bias, module.blocks.10.ca_norm.bias, '\n"
                    " 'module.blocks.11.ada_gss, module.blocks.11.sa.scale_mul_1H11, module.blocks.11.sa.q_bias, module.blocks.11.sa.v_bias, module.blocks.11.sa.proj.bias, module.blocks.11.ca.v_bias, '\n"
                    " 'module.blocks.11.ca.mat_q.bias, module.blocks.11.ca.proj.bias, module.blocks.11.ffn.fc1.bias, module.blocks.11.ffn.fc2.bias, module.blocks.11.ca_norm.bias, module.blocks.12.ada_gss, '\n"
                    " 'module.blocks.12.sa.scale_mul_1H11, module.blocks.12.sa.q_bias, module.blocks.12.sa.v_bias, module.blocks.12.sa.proj.bias, module.blocks.12.ca.v_bias, module.blocks.12.ca.mat_q.bias, '\n"
                    " 'module.blocks.12.ca.proj.bias, module.blocks.12.ffn.fc1.bias, module.blocks.12.ffn.fc2.bias, module.blocks.12.ca_norm.bias, module.blocks.13.ada_gss, module.blocks.13.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.13.sa.q_bias, module.blocks.13.sa.v_bias, module.blocks.13.sa.proj.bias, module.blocks.13.ca.v_bias, module.blocks.13.ca.mat_q.bias, module.blocks.13.ca.proj.bias, '\n"
                    " 'module.blocks.13.ffn.fc1.bias, module.blocks.13.ffn.fc2.bias, module.blocks.13.ca_norm.bias, module.blocks.14.ada_gss, module.blocks.14.sa.scale_mul_1H11, module.blocks.14.sa.q_bias, '\n"
                    " 'module.blocks.14.sa.v_bias, module.blocks.14.sa.proj.bias, module.blocks.14.ca.v_bias, module.blocks.14.ca.mat_q.bias, module.blocks.14.ca.proj.bias, module.blocks.14.ffn.fc1.bias, '\n"
                    " 'module.blocks.14.ffn.fc2.bias, module.blocks.14.ca_norm.bias, module.blocks.15.ada_gss, module.blocks.15.sa.scale_mul_1H11, module.blocks.15.sa.q_bias, module.blocks.15.sa.v_bias, '\n"
                    " 'module.blocks.15.sa.proj.bias, module.blocks.15.ca.v_bias, module.blocks.15.ca.mat_q.bias, module.blocks.15.ca.proj.bias, module.blocks.15.ffn.fc1.bias, module.blocks.15.ffn.fc2.bias, '\n"
                    " 'module.blocks.15.ca_norm.bias, module.blocks.16.ada_gss, module.blocks.16.sa.scale_mul_1H11, module.blocks.16.sa.q_bias, module.blocks.16.sa.v_bias, module.blocks.16.sa.proj.bias, '\n"
                    " 'module.blocks.16.ca.v_bias, module.blocks.16.ca.mat_q.bias, module.blocks.16.ca.proj.bias, module.blocks.16.ffn.fc1.bias, module.blocks.16.ffn.fc2.bias, module.blocks.16.ca_norm.bias, '\n"
                    " 'module.blocks.17.ada_gss, module.blocks.17.sa.scale_mul_1H11, module.blocks.17.sa.q_bias, module.blocks.17.sa.v_bias, module.blocks.17.sa.proj.bias, module.blocks.17.ca.v_bias, '\n"
                    " 'module.blocks.17.ca.mat_q.bias, module.blocks.17.ca.proj.bias, module.blocks.17.ffn.fc1.bias, module.blocks.17.ffn.fc2.bias, module.blocks.17.ca_norm.bias, module.blocks.18.ada_gss, '\n"
                    " 'module.blocks.18.sa.scale_mul_1H11, module.blocks.18.sa.q_bias, module.blocks.18.sa.v_bias, module.blocks.18.sa.proj.bias, module.blocks.18.ca.v_bias, module.blocks.18.ca.mat_q.bias, '\n"
                    " 'module.blocks.18.ca.proj.bias, module.blocks.18.ffn.fc1.bias, module.blocks.18.ffn.fc2.bias, module.blocks.18.ca_norm.bias, module.blocks.19.ada_gss, module.blocks.19.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.19.sa.q_bias, module.blocks.19.sa.v_bias, module.blocks.19.sa.proj.bias, module.blocks.19.ca.v_bias, module.blocks.19.ca.mat_q.bias, module.blocks.19.ca.proj.bias, '\n"
                    " 'module.blocks.19.ffn.fc1.bias, module.blocks.19.ffn.fc2.bias, module.blocks.19.ca_norm.bias, module.blocks.20.ada_gss, module.blocks.20.sa.scale_mul_1H11, module.blocks.20.sa.q_bias, '\n"
                    " 'module.blocks.20.sa.v_bias, module.blocks.20.sa.proj.bias, module.blocks.20.ca.v_bias, module.blocks.20.ca.mat_q.bias, module.blocks.20.ca.proj.bias, module.blocks.20.ffn.fc1.bias, '\n"
                    " 'module.blocks.20.ffn.fc2.bias, module.blocks.20.ca_norm.bias, module.blocks.21.ada_gss, module.blocks.21.sa.scale_mul_1H11, module.blocks.21.sa.q_bias, module.blocks.21.sa.v_bias, '\n"
                    " 'module.blocks.21.sa.proj.bias, module.blocks.21.ca.v_bias, module.blocks.21.ca.mat_q.bias, module.blocks.21.ca.proj.bias, module.blocks.21.ffn.fc1.bias, module.blocks.21.ffn.fc2.bias, '\n"
                    " 'module.blocks.21.ca_norm.bias, module.blocks.22.ada_gss, module.blocks.22.sa.scale_mul_1H11, module.blocks.22.sa.q_bias, module.blocks.22.sa.v_bias, module.blocks.22.sa.proj.bias, '\n"
                    " 'module.blocks.22.ca.v_bias, module.blocks.22.ca.mat_q.bias, module.blocks.22.ca.proj.bias, module.blocks.22.ffn.fc1.bias, module.blocks.22.ffn.fc2.bias, module.blocks.22.ca_norm.bias, '\n"
                    " 'module.blocks.23.ada_gss, module.blocks.23.sa.scale_mul_1H11, module.blocks.23.sa.q_bias, module.blocks.23.sa.v_bias, module.blocks.23.sa.proj.bias, module.blocks.23.ca.v_bias, '\n"
                    " 'module.blocks.23.ca.mat_q.bias, module.blocks.23.ca.proj.bias, module.blocks.23.ffn.fc1.bias, module.blocks.23.ffn.fc2.bias, module.blocks.23.ca_norm.bias, module.blocks.24.ada_gss, '\n"
                    " 'module.blocks.24.sa.scale_mul_1H11, module.blocks.24.sa.q_bias, module.blocks.24.sa.v_bias, module.blocks.24.sa.proj.bias, module.blocks.24.ca.v_bias, module.blocks.24.ca.mat_q.bias, '\n"
                    " 'module.blocks.24.ca.proj.bias, module.blocks.24.ffn.fc1.bias, module.blocks.24.ffn.fc2.bias, module.blocks.24.ca_norm.bias, module.blocks.25.ada_gss, module.blocks.25.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.25.sa.q_bias, module.blocks.25.sa.v_bias, module.blocks.25.sa.proj.bias, module.blocks.25.ca.v_bias, module.blocks.25.ca.mat_q.bias, module.blocks.25.ca.proj.bias, '\n"
                    " 'module.blocks.25.ffn.fc1.bias, module.blocks.25.ffn.fc2.bias, module.blocks.25.ca_norm.bias, module.blocks.26.ada_gss, module.blocks.26.sa.scale_mul_1H11, module.blocks.26.sa.q_bias, '\n"
                    " 'module.blocks.26.sa.v_bias, module.blocks.26.sa.proj.bias, module.blocks.26.ca.v_bias, module.blocks.26.ca.mat_q.bias, module.blocks.26.ca.proj.bias, module.blocks.26.ffn.fc1.bias, '\n"
                    " 'module.blocks.26.ffn.fc2.bias, module.blocks.26.ca_norm.bias, module.blocks.27.ada_gss, module.blocks.27.sa.scale_mul_1H11, module.blocks.27.sa.q_bias, module.blocks.27.sa.v_bias, '\n"
                    " 'module.blocks.27.sa.proj.bias, module.blocks.27.ca.v_bias, module.blocks.27.ca.mat_q.bias, module.blocks.27.ca.proj.bias, module.blocks.27.ffn.fc1.bias, module.blocks.27.ffn.fc2.bias, '\n"
                    " 'module.blocks.27.ca_norm.bias, module.blocks.28.ada_gss, module.blocks.28.sa.scale_mul_1H11, module.blocks.28.sa.q_bias, module.blocks.28.sa.v_bias, module.blocks.28.sa.proj.bias, '\n"
                    " 'module.blocks.28.ca.v_bias, module.blocks.28.ca.mat_q.bias, module.blocks.28.ca.proj.bias, module.blocks.28.ffn.fc1.bias, module.blocks.28.ffn.fc2.bias, module.blocks.28.ca_norm.bias, '\n"
                    " 'module.blocks.29.ada_gss, module.blocks.29.sa.scale_mul_1H11, module.blocks.29.sa.q_bias, module.blocks.29.sa.v_bias, module.blocks.29.sa.proj.bias, module.blocks.29.ca.v_bias, '\n"
                    " 'module.blocks.29.ca.mat_q.bias, module.blocks.29.ca.proj.bias, module.blocks.29.ffn.fc1.bias, module.blocks.29.ffn.fc2.bias, module.blocks.29.ca_norm.bias, module.blocks.30.ada_gss, '\n"
                    " 'module.blocks.30.sa.scale_mul_1H11, module.blocks.30.sa.q_bias, module.blocks.30.sa.v_bias, module.blocks.30.sa.proj.bias, module.blocks.30.ca.v_bias, module.blocks.30.ca.mat_q.bias, '\n"
                    " 'module.blocks.30.ca.proj.bias, module.blocks.30.ffn.fc1.bias, module.blocks.30.ffn.fc2.bias, module.blocks.30.ca_norm.bias, module.blocks.31.ada_gss, module.blocks.31.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.31.sa.q_bias, module.blocks.31.sa.v_bias, module.blocks.31.sa.proj.bias, module.blocks.31.ca.v_bias, module.blocks.31.ca.mat_q.bias, module.blocks.31.ca.proj.bias, '\n"
                    " 'module.blocks.31.ffn.fc1.bias, module.blocks.31.ffn.fc2.bias, module.blocks.31.ca_norm.bias, module.blocks.32.ada_gss, module.blocks.32.sa.scale_mul_1H11, module.blocks.32.sa.q_bias, '\n"
                    " 'module.blocks.32.sa.v_bias, module.blocks.32.sa.proj.bias, module.blocks.32.ca.v_bias, module.blocks.32.ca.mat_q.bias, module.blocks.32.ca.proj.bias, module.blocks.32.ffn.fc1.bias, '\n"
                    " 'module.blocks.32.ffn.fc2.bias, module.blocks.32.ca_norm.bias, module.blocks.33.ada_gss, module.blocks.33.sa.scale_mul_1H11, module.blocks.33.sa.q_bias, module.blocks.33.sa.v_bias, '\n"
                    " 'module.blocks.33.sa.proj.bias, module.blocks.33.ca.v_bias, module.blocks.33.ca.mat_q.bias, module.blocks.33.ca.proj.bias, module.blocks.33.ffn.fc1.bias, module.blocks.33.ffn.fc2.bias, '\n"
                    " 'module.blocks.33.ca_norm.bias, module.blocks.34.ada_gss, module.blocks.34.sa.scale_mul_1H11, module.blocks.34.sa.q_bias, module.blocks.34.sa.v_bias, module.blocks.34.sa.proj.bias, '\n"
                    " 'module.blocks.34.ca.v_bias, module.blocks.34.ca.mat_q.bias, module.blocks.34.ca.proj.bias, module.blocks.34.ffn.fc1.bias, module.blocks.34.ffn.fc2.bias, module.blocks.34.ca_norm.bias, '\n"
                    " 'module.blocks.35.ada_gss, module.blocks.35.sa.scale_mul_1H11, module.blocks.35.sa.q_bias, module.blocks.35.sa.v_bias, module.blocks.35.sa.proj.bias, module.blocks.35.ca.v_bias, '\n"
                    " 'module.blocks.35.ca.mat_q.bias, module.blocks.35.ca.proj.bias, module.blocks.35.ffn.fc1.bias, module.blocks.35.ffn.fc2.bias, module.blocks.35.ca_norm.bias, module.blocks.36.ada_gss, '\n"
                    " 'module.blocks.36.sa.scale_mul_1H11, module.blocks.36.sa.q_bias, module.blocks.36.sa.v_bias, module.blocks.36.sa.proj.bias, module.blocks.36.ca.v_bias, module.blocks.36.ca.mat_q.bias, '\n"
                    " 'module.blocks.36.ca.proj.bias, module.blocks.36.ffn.fc1.bias, module.blocks.36.ffn.fc2.bias, module.blocks.36.ca_norm.bias, module.blocks.37.ada_gss, module.blocks.37.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.37.sa.q_bias, module.blocks.37.sa.v_bias, module.blocks.37.sa.proj.bias, module.blocks.37.ca.v_bias, module.blocks.37.ca.mat_q.bias, module.blocks.37.ca.proj.bias, '\n"
                    " 'module.blocks.37.ffn.fc1.bias, module.blocks.37.ffn.fc2.bias, module.blocks.37.ca_norm.bias, module.blocks.38.ada_gss, module.blocks.38.sa.scale_mul_1H11, module.blocks.38.sa.q_bias, '\n"
                    " 'module.blocks.38.sa.v_bias, module.blocks.38.sa.proj.bias, module.blocks.38.ca.v_bias, module.blocks.38.ca.mat_q.bias, module.blocks.38.ca.proj.bias, module.blocks.38.ffn.fc1.bias, '\n"
                    " 'module.blocks.38.ffn.fc2.bias, module.blocks.38.ca_norm.bias, module.blocks.39.ada_gss, module.blocks.39.sa.scale_mul_1H11, module.blocks.39.sa.q_bias, module.blocks.39.sa.v_bias, '\n"
                    " 'module.blocks.39.sa.proj.bias, module.blocks.39.ca.v_bias, module.blocks.39.ca.mat_q.bias, module.blocks.39.ca.proj.bias, module.blocks.39.ffn.fc1.bias, module.blocks.39.ffn.fc2.bias, '\n"
                    " 'module.blocks.39.ca_norm.bias, module.blocks.40.ada_gss, module.blocks.40.sa.scale_mul_1H11, module.blocks.40.sa.q_bias, module.blocks.40.sa.v_bias, module.blocks.40.sa.proj.bias, '\n"
                    " 'module.blocks.40.ca.v_bias, module.blocks.40.ca.mat_q.bias, module.blocks.40.ca.proj.bias, module.blocks.40.ffn.fc1.bias, module.blocks.40.ffn.fc2.bias, module.blocks.40.ca_norm.bias, '\n"
                    " 'module.blocks.41.ada_gss, module.blocks.41.sa.scale_mul_1H11, module.blocks.41.sa.q_bias, module.blocks.41.sa.v_bias, module.blocks.41.sa.proj.bias, module.blocks.41.ca.v_bias, '\n"
                    " 'module.blocks.41.ca.mat_q.bias, module.blocks.41.ca.proj.bias, module.blocks.41.ffn.fc1.bias, module.blocks.41.ffn.fc2.bias, module.blocks.41.ca_norm.bias, module.blocks.42.ada_gss, '\n"
                    " 'module.blocks.42.sa.scale_mul_1H11, module.blocks.42.sa.q_bias, module.blocks.42.sa.v_bias, module.blocks.42.sa.proj.bias, module.blocks.42.ca.v_bias, module.blocks.42.ca.mat_q.bias, '\n"
                    " 'module.blocks.42.ca.proj.bias, module.blocks.42.ffn.fc1.bias, module.blocks.42.ffn.fc2.bias, module.blocks.42.ca_norm.bias, module.blocks.43.ada_gss, module.blocks.43.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.43.sa.q_bias, module.blocks.43.sa.v_bias, module.blocks.43.sa.proj.bias, module.blocks.43.ca.v_bias, module.blocks.43.ca.mat_q.bias, module.blocks.43.ca.proj.bias, '\n"
                    " 'module.blocks.43.ffn.fc1.bias, module.blocks.43.ffn.fc2.bias, module.blocks.43.ca_norm.bias, module.blocks.44.ada_gss, module.blocks.44.sa.scale_mul_1H11, module.blocks.44.sa.q_bias, '\n"
                    " 'module.blocks.44.sa.v_bias, module.blocks.44.sa.proj.bias, module.blocks.44.ca.v_bias, module.blocks.44.ca.mat_q.bias, module.blocks.44.ca.proj.bias, module.blocks.44.ffn.fc1.bias, '\n"
                    " 'module.blocks.44.ffn.fc2.bias, module.blocks.44.ca_norm.bias, module.blocks.45.ada_gss, module.blocks.45.sa.scale_mul_1H11, module.blocks.45.sa.q_bias, module.blocks.45.sa.v_bias, '\n"
                    " 'module.blocks.45.sa.proj.bias, module.blocks.45.ca.v_bias, module.blocks.45.ca.mat_q.bias, module.blocks.45.ca.proj.bias, module.blocks.45.ffn.fc1.bias, module.blocks.45.ffn.fc2.bias, '\n"
                    " 'module.blocks.45.ca_norm.bias, module.blocks.46.ada_gss, module.blocks.46.sa.scale_mul_1H11, module.blocks.46.sa.q_bias, module.blocks.46.sa.v_bias, module.blocks.46.sa.proj.bias, '\n"
                    " 'module.blocks.46.ca.v_bias, module.blocks.46.ca.mat_q.bias, module.blocks.46.ca.proj.bias, module.blocks.46.ffn.fc1.bias, module.blocks.46.ffn.fc2.bias, module.blocks.46.ca_norm.bias, '\n"
                    " 'module.blocks.47.ada_gss, module.blocks.47.sa.scale_mul_1H11, module.blocks.47.sa.q_bias, module.blocks.47.sa.v_bias, module.blocks.47.sa.proj.bias, module.blocks.47.ca.v_bias, '\n"
                    " 'module.blocks.47.ca.mat_q.bias, module.blocks.47.ca.proj.bias, module.blocks.47.ffn.fc1.bias, module.blocks.47.ffn.fc2.bias, module.blocks.47.ca_norm.bias, module.blocks.48.ada_gss, '\n"
                    " 'module.blocks.48.sa.scale_mul_1H11, module.blocks.48.sa.q_bias, module.blocks.48.sa.v_bias, module.blocks.48.sa.proj.bias, module.blocks.48.ca.v_bias, module.blocks.48.ca.mat_q.bias, '\n"
                    " 'module.blocks.48.ca.proj.bias, module.blocks.48.ffn.fc1.bias, module.blocks.48.ffn.fc2.bias, module.blocks.48.ca_norm.bias, module.blocks.49.ada_gss, module.blocks.49.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.49.sa.q_bias, module.blocks.49.sa.v_bias, module.blocks.49.sa.proj.bias, module.blocks.49.ca.v_bias, module.blocks.49.ca.mat_q.bias, module.blocks.49.ca.proj.bias, '\n"
                    " 'module.blocks.49.ffn.fc1.bias, module.blocks.49.ffn.fc2.bias, module.blocks.49.ca_norm.bias, module.blocks.50.ada_gss, module.blocks.50.sa.scale_mul_1H11, module.blocks.50.sa.q_bias, '\n"
                    " 'module.blocks.50.sa.v_bias, module.blocks.50.sa.proj.bias, module.blocks.50.ca.v_bias, module.blocks.50.ca.mat_q.bias, module.blocks.50.ca.proj.bias, module.blocks.50.ffn.fc1.bias, '\n"
                    " 'module.blocks.50.ffn.fc2.bias, module.blocks.50.ca_norm.bias, module.blocks.51.ada_gss, module.blocks.51.sa.scale_mul_1H11, module.blocks.51.sa.q_bias, module.blocks.51.sa.v_bias, '\n"
                    " 'module.blocks.51.sa.proj.bias, module.blocks.51.ca.v_bias, module.blocks.51.ca.mat_q.bias, module.blocks.51.ca.proj.bias, module.blocks.51.ffn.fc1.bias, module.blocks.51.ffn.fc2.bias, '\n"
                    " 'module.blocks.51.ca_norm.bias, module.blocks.52.ada_gss, module.blocks.52.sa.scale_mul_1H11, module.blocks.52.sa.q_bias, module.blocks.52.sa.v_bias, module.blocks.52.sa.proj.bias, '\n"
                    " 'module.blocks.52.ca.v_bias, module.blocks.52.ca.mat_q.bias, module.blocks.52.ca.proj.bias, module.blocks.52.ffn.fc1.bias, module.blocks.52.ffn.fc2.bias, module.blocks.52.ca_norm.bias, '\n"
                    " 'module.blocks.53.ada_gss, module.blocks.53.sa.scale_mul_1H11, module.blocks.53.sa.q_bias, module.blocks.53.sa.v_bias, module.blocks.53.sa.proj.bias, module.blocks.53.ca.v_bias, '\n"
                    " 'module.blocks.53.ca.mat_q.bias, module.blocks.53.ca.proj.bias, module.blocks.53.ffn.fc1.bias, module.blocks.53.ffn.fc2.bias, module.blocks.53.ca_norm.bias, module.blocks.54.ada_gss, '\n"
                    " 'module.blocks.54.sa.scale_mul_1H11, module.blocks.54.sa.q_bias, module.blocks.54.sa.v_bias, module.blocks.54.sa.proj.bias, module.blocks.54.ca.v_bias, module.blocks.54.ca.mat_q.bias, '\n"
                    " 'module.blocks.54.ca.proj.bias, module.blocks.54.ffn.fc1.bias, module.blocks.54.ffn.fc2.bias, module.blocks.54.ca_norm.bias, module.blocks.55.ada_gss, module.blocks.55.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.55.sa.q_bias, module.blocks.55.sa.v_bias, module.blocks.55.sa.proj.bias, module.blocks.55.ca.v_bias, module.blocks.55.ca.mat_q.bias, module.blocks.55.ca.proj.bias, '\n"
                    " 'module.blocks.55.ffn.fc1.bias, module.blocks.55.ffn.fc2.bias, module.blocks.55.ca_norm.bias')",
          'wd_sc': 0.0}}

[09-01 23:25:17] (nity/utils/lr_control.py, line 116)=> [get_param_groups][rank0] type(model).__name__='NullDDP' count=1085, numel=8573892016
[09-01 23:25:17] (nity/utils/lr_control.py, line 117)=> 
[09-01 23:25:17] (inity_circuit_breaker.py, line 343)=> [vgpt] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.97), fused=True), opt_kw={'lr': 9.375e-05, 'weight_decay': 0}

[09-01 23:25:17] (inity_circuit_breaker.py, line 348)=> Loading T5 from google/flan-t5-xl...
[09-01 23:25:18] (_original/src/trainer.py, line  65)=> self.reweight_loss_by_scale: 1
[09-01 23:25:18] (inity_circuit_breaker.py, line 171)=> global bs=4, local bs=4
[09-01 23:25:18] (inity_circuit_breaker.py, line 172)=> initial args:
{
  local_out_path      : ./local_output
  data_path           : ./data
  bed                 : ./checkpoints
  vae_ckpt            : /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
  exp_name            : experiment
  ds                  : oi
  model               : infinity_8b
  short_cap_prob      : 0.2
  project_name        : infinity_circuit_breaker
  tf32                : True
  auto_resume         : True
  rush_resume         : 
  nowd                : 1
  enable_hybrid_shard : False
  inner_shard_degree  : 1
  zero                : 2
  buck                : chunk
  fsdp_orig           : True
  enable_checkpointing: full-block
  pad_to_multiplier   : 128
  log_every_iter      : False
  checkpoint_type     : torch
  seed                : None
  rand                : True
  task_id             : 123
  trial_id            : 123
  robust_run_id       : 0
  ckpt_trials         : []
  real_trial_id       : 123
  chunk_nodes         : 0
  is_master_node      : None
  log_txt_path        : ./local_output/log.txt
  t5_path             : google/flan-t5-xl
  online_t5           : True
  sdpa_mem            : True
  tfast               : 0
  model_alias         : 8b
  rms                 : False
  aln                 : 0.001
  alng                : 5e-06
  saln                : True
  haln                : True
  nm0                 : False
  tau                 : 1
  cos                 : True
  swi                 : False
  dp                  : -1
  drop                : 0.0
  hd                  : 0
  ca_gamma            : -1
  diva                : 1
  hd0                 : 0.02
  dec                 : 1
  cum                 : 3
  rwe                 : False
  tp                  : 0.0
  tk                  : 0.0
  tini                : 0.010416666666666666
  cfg                 : 0.1
  rand_uncond         : False
  ema                 : 0.9999
  tema                : 0
  fp16                : 2
  fuse                : False
  fused_norm          : False
  flash               : False
  xen                 : False
  use_flex_attn       : True
  stable              : False
  gblr                : 0.0001
  dblr                : 0.0001
  tblr                : 0.006
  glr                 : 1.5625e-06
  dlr                 : 1.5625e-06
  tlr                 : 9.375e-05
  gwd                 : 0.005
  dwd                 : 0.0005
  twd                 : 0.005
  gwde                : 0.005
  dwde                : 0.0005
  twde                : 0.005
  ls                  : 0.0
  lz                  : 0.0
  eq                  : 0
  ep                  : 100
  wp                  : 1e-08
  wp0                 : 0.005
  wpe                 : 1.0
  sche                : lin0
  log_freq            : 50
  gclip               : 6.0
  dclip               : 6.0
  tclip               : 5.0
  cdec                : False
  opt                 : adamw
  ada                 : 0.9_0.97
  dada                : 0.9_0.97
  oeps                : 0
  afuse               : True
  pn                  : 0.06M
  scale_schedule      : None
  patch_size          : None
  resos               : None
  data_load_reso      : None
  workers             : 4
  lbs                 : 4
  bs                  : 4
  batch_size          : 4
  glb_batch_size      : 4
  ac                  : 1
  r_accu              : 1.0
  norm_eps            : 1e-06
  tlen                : 512
  Ct5                 : 2048
  use_bit_label       : 1
  bitloss_type        : mean
  dynamic_resolution_across_gpus: 1
  enable_dynamic_length_prompt: 1
  use_streaming_dataset: 1
  iterable_data_buffersize: 30000
  save_model_iters_freq: 100
  noise_apply_layers  : 13
  noise_apply_strength: 0.3
  noise_apply_requant : 1
  rope2d_each_sa_layer: 1
  rope2d_normalized_by_hw: 2
  use_fsdp_model_ema  : 0
  add_lvl_embeding_only_first_block: 1
  reweight_loss_by_scale: 1
  always_training_scales: 100
  vae_type            : 14
  fake_vae_input      : False
  model_init_device   : cuda
  prefetch_factor     : 2
  apply_spatial_patchify: 1
  debug_bsc           : 0
  task_type           : t2i
  target_layers       : [10, 12, 14, 16, 18, 20]
  transform_layers    : [10, 12, 14, 16, 18, 20]
  lorra_alpha         : 5.0
  trainsets           : ['AlpacaSupervisedDataset', 'HarmfulDataset']
  valsets             : ['AlpacaSupervisedDataset', 'HarmfulDataset']
  adv_string          : Sure here's
  full_layers         : False
  harmful_prompts_path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
  sanitized_prompts_path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
  validation_ratio    : 0.1
  category            : hate
  lora_r              : 8
  lora_alpha          : 16
  lora_dropout        : 0.05
  lora_target_modules : ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
  lora_weight_path    : 
  lora_bias           : none
  q_lora              : False
  model_name_or_path  : meta-llama/Llama-2-7b-chat-hf
  adapter_name_or_path: 
  use_lora            : True
  cache_dir           : 
  optim               : adamw_torch
  model_max_length    : 512
  grouped_to_max_length: False
  use_refusal_retain  : True
  sc_train_subset     : ['']
  log_every           : 10
  sc_train_seq_type   : all_text
  coeff_schedule      : linear_converge
  sc_loss_type        : orig_act_dotprod
  branch              : master
  commit_id           : 
  commit_msg          : 
  cmd                 : --bed ./checkpoints --local_out_path ./local_output --project_name infinity_circuit_breaker --exp_name experiment --ep 100 --opt adamw --cum 3 --sche lin0 --fp16 2 --ada 0.9_0.97 --tini -1 --tclip 5 --flash 0 --alng 5e-06 --saln 1 --cos 1 --enable_checkpointing full-block --tblr 6e-3 --pn 0.06M --lbs 4 --workers 4 --short_cap_prob 0.2 --online_t5 1 --use_streaming_dataset 1 --iterable_data_buffersize 30000 --Ct5 2048 --t5_path google/flan-t5-xl --vae_type 14 --wp 0.00000001 --wpe 1 --dynamic_resolution_across_gpus 1 --enable_dynamic_length_prompt 1 --reweight_loss_by_scale 1 --add_lvl_embeding_only_first_block 1 --rope2d_each_sa_layer 1 --rope2d_normalized_by_hw 2 --use_fsdp_model_ema 0 --always_training_scales 100 --use_bit_label 1 --zero 2 --save_model_iters_freq 100 --log_freq 50 --checkpoint_type torch --prefetch_factor 2 --noise_apply_strength 0.3 --noise_apply_layers 13 --apply_spatial_patchify 1 --use_flex_attn True --pad 128 --data_path ./data --data_load_reso 512 --tlen 512 --workers 4 --prefetch_factor 2 --harmful_prompts_path /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts --sanitized_prompts_path /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts --validation_ratio 0.1 --category hate --target_layers 10,12,14,16,18,20 --transform_layers 10,12,14,16,18,20 --lorra_alpha 5.0 --trainsets AlpacaSupervisedDataset#HarmfulDataset --valsets AlpacaSupervisedDataset#HarmfulDataset --adv_string Sure here's --full_layers False --lora_r 8 --lora_alpha 16 --lora_dropout 0.05 --lora_target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj --lora_weight_path  --lora_bias none --q_lora False --model_name_or_path meta-llama/Llama-2-7b-chat-hf --adapter_name_or_path  --use_lora True --cache_dir  --optim adamw_torch --model_max_length 512 --grouped_to_max_length False --use_refusal_retain True --sc_train_subset  --log_every 10 --sc_train_seq_type all_text --coeff_schedule linear_converge --sc_loss_type orig_act_dotprod
  tag                 : UK
  acc_all             : None
  acc_real            : None
  acc_fake            : None
  last_Lnll           : None
  last_L1             : None
  last_Ld             : None
  last_wei_g          : None
  grad_boom           : None
  diff                : None
  diffs               : 
  diffs_ema           : None
  ca_performance      : 
  cur_phase           : 
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  iter_speed          : None
  img_per_day         : None
  max_nvidia_smi      : 0
  max_memory_allocated: None
  max_memory_reserved : None
  num_alloc_retries   : None
  MFU                 : None
  HFU                 : None
  dbg_modified        : False
  dbg_ks              : False
  dbg_ks_last         : None
  dbg                 : False
  ks                  : False
  nodata              : False
  nodata_tlen         : 320
  nova                : False
  prof                : 0
  prof_freq           : 50
  tos_profiler_file_prefix: vgpt_default/
  profall             : 0
  v_seed              : 0
  g_seed              : 0
}

[09-01 23:25:18] (inity_circuit_breaker.py, line 182)=> start_it=0, iters_train=100
[09-01 23:25:19] (inity_circuit_breaker.py, line 466)=> [PT info]  from ep0 it0, acc_str: [no acc str], diffs: ,    =======>  bed: ./checkpoints  <=======








=======================================================   RESTART [09-01 23:53:57]   =======================================================
[09-01 23:53:57] (inity_circuit_breaker.py, line 718)=> ============================================================
[09-01 23:53:57] (inity_circuit_breaker.py, line 719)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[09-01 23:53:57] (inity_circuit_breaker.py, line 720)=> Circuit Breaker Alpha: 0.1
[09-01 23:53:57] (inity_circuit_breaker.py, line 721)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[09-01 23:53:57] (inity_circuit_breaker.py, line 722)=> Circuit Breaker Enabled: True
[09-01 23:53:57] (inity_circuit_breaker.py, line 723)=> Number of Examples: 1000
[09-01 23:53:57] (inity_circuit_breaker.py, line 724)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[09-01 23:53:57] (inity_circuit_breaker.py, line 725)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[09-01 23:53:57] (inity_circuit_breaker.py, line 726)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[09-01 23:53:57] (inity_circuit_breaker.py, line 727)=> Validation Ratio: 0.1
[09-01 23:53:57] (inity_circuit_breaker.py, line 728)=> Category: hate
[09-01 23:53:57] (inity_circuit_breaker.py, line 729)=> Batch Size: 4
[09-01 23:53:57] (inity_circuit_breaker.py, line 730)=> Workers: 4
[09-01 23:53:57] (inity_circuit_breaker.py, line 731)=> Device: cuda:0
[09-01 23:53:57] (inity_circuit_breaker.py, line 732)=> Model: infinity_8b
[09-01 23:53:57] (inity_circuit_breaker.py, line 733)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-01 23:53:57] (inity_circuit_breaker.py, line 734)=> Rush Resume: 
[09-01 23:53:57] (inity_circuit_breaker.py, line 735)=> Selective Layers: 0,1,2,3,4,5
[09-01 23:53:57] (inity_circuit_breaker.py, line 736)=> ============================================================
[09-01 23:53:57] (inity_circuit_breaker.py, line 377)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[09-01 23:53:57] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-01 23:53:57] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-01 23:53:57] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[09-01 23:53:57] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-01 23:53:57] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-01 23:53:57] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[09-01 23:53:57] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[09-01 23:53:57] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[09-01 23:53:57] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[09-01 23:53:57] (inity_circuit_breaker.py, line 395)=> args.batch_size=4, vbs=6
[09-01 23:53:57] (inity_circuit_breaker.py, line 418)=> len(dataloader): 100, len(dataset): 100
[09-01 23:53:57] (inity_circuit_breaker.py, line 420)=> total_samples: 200
[09-01 23:53:57] (inity_circuit_breaker.py, line 421)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[09-01 23:53:57] (inity_circuit_breaker.py, line 140)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[09-01 23:53:57] (inity_circuit_breaker.py, line 142)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[09-01 23:53:57] (inity_circuit_breaker.py, line 146)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth







=======================================================   RESTART [09-01 23:54:40]   =======================================================
[09-01 23:54:40] (inity_circuit_breaker.py, line 718)=> ============================================================
[09-01 23:54:40] (inity_circuit_breaker.py, line 719)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[09-01 23:54:40] (inity_circuit_breaker.py, line 720)=> Circuit Breaker Alpha: 0.1
[09-01 23:54:40] (inity_circuit_breaker.py, line 721)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[09-01 23:54:40] (inity_circuit_breaker.py, line 722)=> Circuit Breaker Enabled: True
[09-01 23:54:40] (inity_circuit_breaker.py, line 723)=> Number of Examples: 1000
[09-01 23:54:40] (inity_circuit_breaker.py, line 724)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[09-01 23:54:40] (inity_circuit_breaker.py, line 725)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[09-01 23:54:40] (inity_circuit_breaker.py, line 726)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[09-01 23:54:40] (inity_circuit_breaker.py, line 727)=> Validation Ratio: 0.1
[09-01 23:54:40] (inity_circuit_breaker.py, line 728)=> Category: hate
[09-01 23:54:40] (inity_circuit_breaker.py, line 729)=> Batch Size: 4
[09-01 23:54:40] (inity_circuit_breaker.py, line 730)=> Workers: 4
[09-01 23:54:40] (inity_circuit_breaker.py, line 731)=> Device: cuda:0
[09-01 23:54:40] (inity_circuit_breaker.py, line 732)=> Model: infinity_8b
[09-01 23:54:40] (inity_circuit_breaker.py, line 733)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-01 23:54:40] (inity_circuit_breaker.py, line 734)=> Rush Resume: 
[09-01 23:54:40] (inity_circuit_breaker.py, line 735)=> Selective Layers: 0,1,2,3,4,5
[09-01 23:54:40] (inity_circuit_breaker.py, line 736)=> ============================================================
[09-01 23:54:40] (inity_circuit_breaker.py, line 377)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[09-01 23:54:40] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-01 23:54:40] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-01 23:54:40] (rain_dataset_infinity.py, line 193)=> Split 0 prompts: 0 retain, 0 validation
[09-01 23:54:40] (rain_dataset_infinity.py, line 224)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-01 23:54:40] (rain_dataset_infinity.py, line 233)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-01 23:54:41] (rain_dataset_infinity.py, line  84)=> Loaded 0 retain examples
[09-01 23:54:41] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[09-01 23:54:41] (rain_dataset_infinity.py, line  86)=> Loaded 0 validation examples
[09-01 23:54:41] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[09-01 23:54:41] (inity_circuit_breaker.py, line 395)=> args.batch_size=4, vbs=6
[09-01 23:54:41] (inity_circuit_breaker.py, line 418)=> len(dataloader): 100, len(dataset): 100
[09-01 23:54:41] (inity_circuit_breaker.py, line 420)=> total_samples: 200
[09-01 23:54:41] (inity_circuit_breaker.py, line 421)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[09-01 23:54:41] (inity_circuit_breaker.py, line 140)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[09-01 23:54:41] (inity_circuit_breaker.py, line 142)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[09-01 23:54:41] (inity_circuit_breaker.py, line 146)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-01 23:54:43] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[09-01 23:54:43] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[09-01 23:54:43] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[09-01 23:54:43] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[09-01 23:54:46] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[09-01 23:54:47] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[09-01 23:54:47] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[09-01 23:54:47] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[09-01 23:54:47] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[09-01 23:54:47] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

[09-01 23:54:59] (ity/models/init_param.py, line  15)=> [init_weights] Infinity with std=0.02
[09-01 23:54:59] (inity_circuit_breaker.py, line 246)=> [PT] GPT model = Infinity(
  drop_path_rate=0.2
  (norm0_cond): Identity()
  (text_norm): FastRMSNorm(C=2048, eps=1e-06, elementwise_affine=True)
  (text_proj_for_sos): TextAttentivePool(
    (ca): CrossAttention(
      Cq=3072, Ckv=2048, cos_attn=False
      (mat_kv): Linear(in_features=2048, out_features=6144, bias=False)
      (proj): Linear(in_features=3072, out_features=3072, bias=True)
      (proj_drop): Identity()
    )
  )
  (text_proj_for_ca): Sequential(
    (0): Linear(in_features=2048, out_features=3072, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=3072, out_features=3072, bias=True)
  )
  (lvl_embed): Embedding(15, 3072)
  (norm0_ve): Identity()
  (word_embed): Linear(in_features=56, out_features=3072, bias=True)
  (shared_ada_lin): Sequential(
    (0): SiLU()
    (1): SharedAdaLin(in_features=3072, out_features=18432, bias=True)
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=3072, out_features=6144, bias=True)
    )
  )
  (head): Linear(in_features=3072, out_features=112, bias=True)
  (blocks): ModuleList(
    (0): CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): Identity()
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
    (1-55): 55 x CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): DropPath(...)
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
  )
)


[09-01 23:54:59] (inity_circuit_breaker.py, line 248)=> [PT][#para] VAE=81.62, VAE.quant=0.00
[09-01 23:54:59] (inity_circuit_breaker.py, line 251)=> [PT][#para] GPT=8573.89


[09-01 23:55:00] (nity/utils/lr_control.py, line  71)=> [get_param_groups][lr decay] with_lr_scale=False, lr_scale=0.0
[09-01 23:55:00] (nity/utils/lr_control.py, line 111)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': '[no scale]',
         'params': "('module.text_norm.weight, module.text_proj_for_sos.ca.mat_kv.weight, module.text_proj_for_sos.ca.proj.weight, module.text_proj_for_ca.0.weight, module.text_proj_for_ca.2.weight, '\n"
                   " 'module.word_embed.weight, module.shared_ada_lin.1.weight, module.head_nm.ada_lin.1.weight, module.head.weight, module.blocks.0.sa.mat_qkv.weight, module.blocks.0.sa.proj.weight, '\n"
                   " 'module.blocks.0.ca.mat_q.weight, module.blocks.0.ca.mat_kv.weight, module.blocks.0.ca.proj.weight, module.blocks.0.ffn.fc1.weight, module.blocks.0.ffn.fc2.weight, module.blocks.0.ca_norm.weight, '\n"
                   " 'module.blocks.1.sa.mat_qkv.weight, module.blocks.1.sa.proj.weight, module.blocks.1.ca.mat_q.weight, module.blocks.1.ca.mat_kv.weight, module.blocks.1.ca.proj.weight, '\n"
                   " 'module.blocks.1.ffn.fc1.weight, module.blocks.1.ffn.fc2.weight, module.blocks.1.ca_norm.weight, module.blocks.2.sa.mat_qkv.weight, module.blocks.2.sa.proj.weight, module.blocks.2.ca.mat_q.weight, '\n"
                   " 'module.blocks.2.ca.mat_kv.weight, module.blocks.2.ca.proj.weight, module.blocks.2.ffn.fc1.weight, module.blocks.2.ffn.fc2.weight, module.blocks.2.ca_norm.weight, module.blocks.3.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.3.sa.proj.weight, module.blocks.3.ca.mat_q.weight, module.blocks.3.ca.mat_kv.weight, module.blocks.3.ca.proj.weight, module.blocks.3.ffn.fc1.weight, module.blocks.3.ffn.fc2.weight, '\n"
                   " 'module.blocks.3.ca_norm.weight, module.blocks.4.sa.mat_qkv.weight, module.blocks.4.sa.proj.weight, module.blocks.4.ca.mat_q.weight, module.blocks.4.ca.mat_kv.weight, '\n"
                   " 'module.blocks.4.ca.proj.weight, module.blocks.4.ffn.fc1.weight, module.blocks.4.ffn.fc2.weight, module.blocks.4.ca_norm.weight, module.blocks.5.sa.mat_qkv.weight, module.blocks.5.sa.proj.weight, '\n"
                   " 'module.blocks.5.ca.mat_q.weight, module.blocks.5.ca.mat_kv.weight, module.blocks.5.ca.proj.weight, module.blocks.5.ffn.fc1.weight, module.blocks.5.ffn.fc2.weight, module.blocks.5.ca_norm.weight, '\n"
                   " 'module.blocks.6.sa.mat_qkv.weight, module.blocks.6.sa.proj.weight, module.blocks.6.ca.mat_q.weight, module.blocks.6.ca.mat_kv.weight, module.blocks.6.ca.proj.weight, '\n"
                   " 'module.blocks.6.ffn.fc1.weight, module.blocks.6.ffn.fc2.weight, module.blocks.6.ca_norm.weight, module.blocks.7.sa.mat_qkv.weight, module.blocks.7.sa.proj.weight, module.blocks.7.ca.mat_q.weight, '\n"
                   " 'module.blocks.7.ca.mat_kv.weight, module.blocks.7.ca.proj.weight, module.blocks.7.ffn.fc1.weight, module.blocks.7.ffn.fc2.weight, module.blocks.7.ca_norm.weight, module.blocks.8.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.8.sa.proj.weight, module.blocks.8.ca.mat_q.weight, module.blocks.8.ca.mat_kv.weight, module.blocks.8.ca.proj.weight, module.blocks.8.ffn.fc1.weight, module.blocks.8.ffn.fc2.weight, '\n"
                   " 'module.blocks.8.ca_norm.weight, module.blocks.9.sa.mat_qkv.weight, module.blocks.9.sa.proj.weight, module.blocks.9.ca.mat_q.weight, module.blocks.9.ca.mat_kv.weight, '\n"
                   " 'module.blocks.9.ca.proj.weight, module.blocks.9.ffn.fc1.weight, module.blocks.9.ffn.fc2.weight, module.blocks.9.ca_norm.weight, module.blocks.10.sa.mat_qkv.weight, module.blocks.10.sa.proj.weight, '\n"
                   " 'module.blocks.10.ca.mat_q.weight, module.blocks.10.ca.mat_kv.weight, module.blocks.10.ca.proj.weight, module.blocks.10.ffn.fc1.weight, module.blocks.10.ffn.fc2.weight, '\n"
                   " 'module.blocks.10.ca_norm.weight, module.blocks.11.sa.mat_qkv.weight, module.blocks.11.sa.proj.weight, module.blocks.11.ca.mat_q.weight, module.blocks.11.ca.mat_kv.weight, '\n"
                   " 'module.blocks.11.ca.proj.weight, module.blocks.11.ffn.fc1.weight, module.blocks.11.ffn.fc2.weight, module.blocks.11.ca_norm.weight, module.blocks.12.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.12.sa.proj.weight, module.blocks.12.ca.mat_q.weight, module.blocks.12.ca.mat_kv.weight, module.blocks.12.ca.proj.weight, module.blocks.12.ffn.fc1.weight, '\n"
                   " 'module.blocks.12.ffn.fc2.weight, module.blocks.12.ca_norm.weight, module.blocks.13.sa.mat_qkv.weight, module.blocks.13.sa.proj.weight, module.blocks.13.ca.mat_q.weight, '\n"
                   " 'module.blocks.13.ca.mat_kv.weight, module.blocks.13.ca.proj.weight, module.blocks.13.ffn.fc1.weight, module.blocks.13.ffn.fc2.weight, module.blocks.13.ca_norm.weight, '\n"
                   " 'module.blocks.14.sa.mat_qkv.weight, module.blocks.14.sa.proj.weight, module.blocks.14.ca.mat_q.weight, module.blocks.14.ca.mat_kv.weight, module.blocks.14.ca.proj.weight, '\n"
                   " 'module.blocks.14.ffn.fc1.weight, module.blocks.14.ffn.fc2.weight, module.blocks.14.ca_norm.weight, module.blocks.15.sa.mat_qkv.weight, module.blocks.15.sa.proj.weight, '\n"
                   " 'module.blocks.15.ca.mat_q.weight, module.blocks.15.ca.mat_kv.weight, module.blocks.15.ca.proj.weight, module.blocks.15.ffn.fc1.weight, module.blocks.15.ffn.fc2.weight, '\n"
                   " 'module.blocks.15.ca_norm.weight, module.blocks.16.sa.mat_qkv.weight, module.blocks.16.sa.proj.weight, module.blocks.16.ca.mat_q.weight, module.blocks.16.ca.mat_kv.weight, '\n"
                   " 'module.blocks.16.ca.proj.weight, module.blocks.16.ffn.fc1.weight, module.blocks.16.ffn.fc2.weight, module.blocks.16.ca_norm.weight, module.blocks.17.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.17.sa.proj.weight, module.blocks.17.ca.mat_q.weight, module.blocks.17.ca.mat_kv.weight, module.blocks.17.ca.proj.weight, module.blocks.17.ffn.fc1.weight, '\n"
                   " 'module.blocks.17.ffn.fc2.weight, module.blocks.17.ca_norm.weight, module.blocks.18.sa.mat_qkv.weight, module.blocks.18.sa.proj.weight, module.blocks.18.ca.mat_q.weight, '\n"
                   " 'module.blocks.18.ca.mat_kv.weight, module.blocks.18.ca.proj.weight, module.blocks.18.ffn.fc1.weight, module.blocks.18.ffn.fc2.weight, module.blocks.18.ca_norm.weight, '\n"
                   " 'module.blocks.19.sa.mat_qkv.weight, module.blocks.19.sa.proj.weight, module.blocks.19.ca.mat_q.weight, module.blocks.19.ca.mat_kv.weight, module.blocks.19.ca.proj.weight, '\n"
                   " 'module.blocks.19.ffn.fc1.weight, module.blocks.19.ffn.fc2.weight, module.blocks.19.ca_norm.weight, module.blocks.20.sa.mat_qkv.weight, module.blocks.20.sa.proj.weight, '\n"
                   " 'module.blocks.20.ca.mat_q.weight, module.blocks.20.ca.mat_kv.weight, module.blocks.20.ca.proj.weight, module.blocks.20.ffn.fc1.weight, module.blocks.20.ffn.fc2.weight, '\n"
                   " 'module.blocks.20.ca_norm.weight, module.blocks.21.sa.mat_qkv.weight, module.blocks.21.sa.proj.weight, module.blocks.21.ca.mat_q.weight, module.blocks.21.ca.mat_kv.weight, '\n"
                   " 'module.blocks.21.ca.proj.weight, module.blocks.21.ffn.fc1.weight, module.blocks.21.ffn.fc2.weight, module.blocks.21.ca_norm.weight, module.blocks.22.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.22.sa.proj.weight, module.blocks.22.ca.mat_q.weight, module.blocks.22.ca.mat_kv.weight, module.blocks.22.ca.proj.weight, module.blocks.22.ffn.fc1.weight, '\n"
                   " 'module.blocks.22.ffn.fc2.weight, module.blocks.22.ca_norm.weight, module.blocks.23.sa.mat_qkv.weight, module.blocks.23.sa.proj.weight, module.blocks.23.ca.mat_q.weight, '\n"
                   " 'module.blocks.23.ca.mat_kv.weight, module.blocks.23.ca.proj.weight, module.blocks.23.ffn.fc1.weight, module.blocks.23.ffn.fc2.weight, module.blocks.23.ca_norm.weight, '\n"
                   " 'module.blocks.24.sa.mat_qkv.weight, module.blocks.24.sa.proj.weight, module.blocks.24.ca.mat_q.weight, module.blocks.24.ca.mat_kv.weight, module.blocks.24.ca.proj.weight, '\n"
                   " 'module.blocks.24.ffn.fc1.weight, module.blocks.24.ffn.fc2.weight, module.blocks.24.ca_norm.weight, module.blocks.25.sa.mat_qkv.weight, module.blocks.25.sa.proj.weight, '\n"
                   " 'module.blocks.25.ca.mat_q.weight, module.blocks.25.ca.mat_kv.weight, module.blocks.25.ca.proj.weight, module.blocks.25.ffn.fc1.weight, module.blocks.25.ffn.fc2.weight, '\n"
                   " 'module.blocks.25.ca_norm.weight, module.blocks.26.sa.mat_qkv.weight, module.blocks.26.sa.proj.weight, module.blocks.26.ca.mat_q.weight, module.blocks.26.ca.mat_kv.weight, '\n"
                   " 'module.blocks.26.ca.proj.weight, module.blocks.26.ffn.fc1.weight, module.blocks.26.ffn.fc2.weight, module.blocks.26.ca_norm.weight, module.blocks.27.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.27.sa.proj.weight, module.blocks.27.ca.mat_q.weight, module.blocks.27.ca.mat_kv.weight, module.blocks.27.ca.proj.weight, module.blocks.27.ffn.fc1.weight, '\n"
                   " 'module.blocks.27.ffn.fc2.weight, module.blocks.27.ca_norm.weight, module.blocks.28.sa.mat_qkv.weight, module.blocks.28.sa.proj.weight, module.blocks.28.ca.mat_q.weight, '\n"
                   " 'module.blocks.28.ca.mat_kv.weight, module.blocks.28.ca.proj.weight, module.blocks.28.ffn.fc1.weight, module.blocks.28.ffn.fc2.weight, module.blocks.28.ca_norm.weight, '\n"
                   " 'module.blocks.29.sa.mat_qkv.weight, module.blocks.29.sa.proj.weight, module.blocks.29.ca.mat_q.weight, module.blocks.29.ca.mat_kv.weight, module.blocks.29.ca.proj.weight, '\n"
                   " 'module.blocks.29.ffn.fc1.weight, module.blocks.29.ffn.fc2.weight, module.blocks.29.ca_norm.weight, module.blocks.30.sa.mat_qkv.weight, module.blocks.30.sa.proj.weight, '\n"
                   " 'module.blocks.30.ca.mat_q.weight, module.blocks.30.ca.mat_kv.weight, module.blocks.30.ca.proj.weight, module.blocks.30.ffn.fc1.weight, module.blocks.30.ffn.fc2.weight, '\n"
                   " 'module.blocks.30.ca_norm.weight, module.blocks.31.sa.mat_qkv.weight, module.blocks.31.sa.proj.weight, module.blocks.31.ca.mat_q.weight, module.blocks.31.ca.mat_kv.weight, '\n"
                   " 'module.blocks.31.ca.proj.weight, module.blocks.31.ffn.fc1.weight, module.blocks.31.ffn.fc2.weight, module.blocks.31.ca_norm.weight, module.blocks.32.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.32.sa.proj.weight, module.blocks.32.ca.mat_q.weight, module.blocks.32.ca.mat_kv.weight, module.blocks.32.ca.proj.weight, module.blocks.32.ffn.fc1.weight, '\n"
                   " 'module.blocks.32.ffn.fc2.weight, module.blocks.32.ca_norm.weight, module.blocks.33.sa.mat_qkv.weight, module.blocks.33.sa.proj.weight, module.blocks.33.ca.mat_q.weight, '\n"
                   " 'module.blocks.33.ca.mat_kv.weight, module.blocks.33.ca.proj.weight, module.blocks.33.ffn.fc1.weight, module.blocks.33.ffn.fc2.weight, module.blocks.33.ca_norm.weight, '\n"
                   " 'module.blocks.34.sa.mat_qkv.weight, module.blocks.34.sa.proj.weight, module.blocks.34.ca.mat_q.weight, module.blocks.34.ca.mat_kv.weight, module.blocks.34.ca.proj.weight, '\n"
                   " 'module.blocks.34.ffn.fc1.weight, module.blocks.34.ffn.fc2.weight, module.blocks.34.ca_norm.weight, module.blocks.35.sa.mat_qkv.weight, module.blocks.35.sa.proj.weight, '\n"
                   " 'module.blocks.35.ca.mat_q.weight, module.blocks.35.ca.mat_kv.weight, module.blocks.35.ca.proj.weight, module.blocks.35.ffn.fc1.weight, module.blocks.35.ffn.fc2.weight, '\n"
                   " 'module.blocks.35.ca_norm.weight, module.blocks.36.sa.mat_qkv.weight, module.blocks.36.sa.proj.weight, module.blocks.36.ca.mat_q.weight, module.blocks.36.ca.mat_kv.weight, '\n"
                   " 'module.blocks.36.ca.proj.weight, module.blocks.36.ffn.fc1.weight, module.blocks.36.ffn.fc2.weight, module.blocks.36.ca_norm.weight, module.blocks.37.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.37.sa.proj.weight, module.blocks.37.ca.mat_q.weight, module.blocks.37.ca.mat_kv.weight, module.blocks.37.ca.proj.weight, module.blocks.37.ffn.fc1.weight, '\n"
                   " 'module.blocks.37.ffn.fc2.weight, module.blocks.37.ca_norm.weight, module.blocks.38.sa.mat_qkv.weight, module.blocks.38.sa.proj.weight, module.blocks.38.ca.mat_q.weight, '\n"
                   " 'module.blocks.38.ca.mat_kv.weight, module.blocks.38.ca.proj.weight, module.blocks.38.ffn.fc1.weight, module.blocks.38.ffn.fc2.weight, module.blocks.38.ca_norm.weight, '\n"
                   " 'module.blocks.39.sa.mat_qkv.weight, module.blocks.39.sa.proj.weight, module.blocks.39.ca.mat_q.weight, module.blocks.39.ca.mat_kv.weight, module.blocks.39.ca.proj.weight, '\n"
                   " 'module.blocks.39.ffn.fc1.weight, module.blocks.39.ffn.fc2.weight, module.blocks.39.ca_norm.weight, module.blocks.40.sa.mat_qkv.weight, module.blocks.40.sa.proj.weight, '\n"
                   " 'module.blocks.40.ca.mat_q.weight, module.blocks.40.ca.mat_kv.weight, module.blocks.40.ca.proj.weight, module.blocks.40.ffn.fc1.weight, module.blocks.40.ffn.fc2.weight, '\n"
                   " 'module.blocks.40.ca_norm.weight, module.blocks.41.sa.mat_qkv.weight, module.blocks.41.sa.proj.weight, module.blocks.41.ca.mat_q.weight, module.blocks.41.ca.mat_kv.weight, '\n"
                   " 'module.blocks.41.ca.proj.weight, module.blocks.41.ffn.fc1.weight, module.blocks.41.ffn.fc2.weight, module.blocks.41.ca_norm.weight, module.blocks.42.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.42.sa.proj.weight, module.blocks.42.ca.mat_q.weight, module.blocks.42.ca.mat_kv.weight, module.blocks.42.ca.proj.weight, module.blocks.42.ffn.fc1.weight, '\n"
                   " 'module.blocks.42.ffn.fc2.weight, module.blocks.42.ca_norm.weight, module.blocks.43.sa.mat_qkv.weight, module.blocks.43.sa.proj.weight, module.blocks.43.ca.mat_q.weight, '\n"
                   " 'module.blocks.43.ca.mat_kv.weight, module.blocks.43.ca.proj.weight, module.blocks.43.ffn.fc1.weight, module.blocks.43.ffn.fc2.weight, module.blocks.43.ca_norm.weight, '\n"
                   " 'module.blocks.44.sa.mat_qkv.weight, module.blocks.44.sa.proj.weight, module.blocks.44.ca.mat_q.weight, module.blocks.44.ca.mat_kv.weight, module.blocks.44.ca.proj.weight, '\n"
                   " 'module.blocks.44.ffn.fc1.weight, module.blocks.44.ffn.fc2.weight, module.blocks.44.ca_norm.weight, module.blocks.45.sa.mat_qkv.weight, module.blocks.45.sa.proj.weight, '\n"
                   " 'module.blocks.45.ca.mat_q.weight, module.blocks.45.ca.mat_kv.weight, module.blocks.45.ca.proj.weight, module.blocks.45.ffn.fc1.weight, module.blocks.45.ffn.fc2.weight, '\n"
                   " 'module.blocks.45.ca_norm.weight, module.blocks.46.sa.mat_qkv.weight, module.blocks.46.sa.proj.weight, module.blocks.46.ca.mat_q.weight, module.blocks.46.ca.mat_kv.weight, '\n"
                   " 'module.blocks.46.ca.proj.weight, module.blocks.46.ffn.fc1.weight, module.blocks.46.ffn.fc2.weight, module.blocks.46.ca_norm.weight, module.blocks.47.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.47.sa.proj.weight, module.blocks.47.ca.mat_q.weight, module.blocks.47.ca.mat_kv.weight, module.blocks.47.ca.proj.weight, module.blocks.47.ffn.fc1.weight, '\n"
                   " 'module.blocks.47.ffn.fc2.weight, module.blocks.47.ca_norm.weight, module.blocks.48.sa.mat_qkv.weight, module.blocks.48.sa.proj.weight, module.blocks.48.ca.mat_q.weight, '\n"
                   " 'module.blocks.48.ca.mat_kv.weight, module.blocks.48.ca.proj.weight, module.blocks.48.ffn.fc1.weight, module.blocks.48.ffn.fc2.weight, module.blocks.48.ca_norm.weight, '\n"
                   " 'module.blocks.49.sa.mat_qkv.weight, module.blocks.49.sa.proj.weight, module.blocks.49.ca.mat_q.weight, module.blocks.49.ca.mat_kv.weight, module.blocks.49.ca.proj.weight, '\n"
                   " 'module.blocks.49.ffn.fc1.weight, module.blocks.49.ffn.fc2.weight, module.blocks.49.ca_norm.weight, module.blocks.50.sa.mat_qkv.weight, module.blocks.50.sa.proj.weight, '\n"
                   " 'module.blocks.50.ca.mat_q.weight, module.blocks.50.ca.mat_kv.weight, module.blocks.50.ca.proj.weight, module.blocks.50.ffn.fc1.weight, module.blocks.50.ffn.fc2.weight, '\n"
                   " 'module.blocks.50.ca_norm.weight, module.blocks.51.sa.mat_qkv.weight, module.blocks.51.sa.proj.weight, module.blocks.51.ca.mat_q.weight, module.blocks.51.ca.mat_kv.weight, '\n"
                   " 'module.blocks.51.ca.proj.weight, module.blocks.51.ffn.fc1.weight, module.blocks.51.ffn.fc2.weight, module.blocks.51.ca_norm.weight, module.blocks.52.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.52.sa.proj.weight, module.blocks.52.ca.mat_q.weight, module.blocks.52.ca.mat_kv.weight, module.blocks.52.ca.proj.weight, module.blocks.52.ffn.fc1.weight, '\n"
                   " 'module.blocks.52.ffn.fc2.weight, module.blocks.52.ca_norm.weight, module.blocks.53.sa.mat_qkv.weight, module.blocks.53.sa.proj.weight, module.blocks.53.ca.mat_q.weight, '\n"
                   " 'module.blocks.53.ca.mat_kv.weight, module.blocks.53.ca.proj.weight, module.blocks.53.ffn.fc1.weight, module.blocks.53.ffn.fc2.weight, module.blocks.53.ca_norm.weight, '\n"
                   " 'module.blocks.54.sa.mat_qkv.weight, module.blocks.54.sa.proj.weight, module.blocks.54.ca.mat_q.weight, module.blocks.54.ca.mat_kv.weight, module.blocks.54.ca.proj.weight, '\n"
                   " 'module.blocks.54.ffn.fc1.weight, module.blocks.54.ffn.fc2.weight, module.blocks.54.ca_norm.weight, module.blocks.55.sa.mat_qkv.weight, module.blocks.55.sa.proj.weight, '\n"
                   " 'module.blocks.55.ca.mat_q.weight, module.blocks.55.ca.mat_kv.weight, module.blocks.55.ca.proj.weight, module.blocks.55.ffn.fc1.weight, module.blocks.55.ffn.fc2.weight, '\n"
                   " 'module.blocks.55.ca_norm.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': '[no scale]',
          'params': "('module.cfg_uncond, module.pos_start, module.text_proj_for_sos.ca.mat_q, module.text_proj_for_sos.ca.v_bias, module.text_proj_for_sos.ca.proj.bias, module.text_proj_for_ca.0.bias, '\n"
                    " 'module.text_proj_for_ca.2.bias, module.lvl_embed.weight, module.word_embed.bias, module.shared_ada_lin.1.bias, module.head_nm.ada_lin.1.bias, module.head.bias, module.blocks.0.ada_gss, '\n"
                    " 'module.blocks.0.sa.scale_mul_1H11, module.blocks.0.sa.q_bias, module.blocks.0.sa.v_bias, module.blocks.0.sa.proj.bias, module.blocks.0.ca.v_bias, module.blocks.0.ca.mat_q.bias, '\n"
                    " 'module.blocks.0.ca.proj.bias, module.blocks.0.ffn.fc1.bias, module.blocks.0.ffn.fc2.bias, module.blocks.0.ca_norm.bias, module.blocks.1.ada_gss, module.blocks.1.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.1.sa.q_bias, module.blocks.1.sa.v_bias, module.blocks.1.sa.proj.bias, module.blocks.1.ca.v_bias, module.blocks.1.ca.mat_q.bias, module.blocks.1.ca.proj.bias, '\n"
                    " 'module.blocks.1.ffn.fc1.bias, module.blocks.1.ffn.fc2.bias, module.blocks.1.ca_norm.bias, module.blocks.2.ada_gss, module.blocks.2.sa.scale_mul_1H11, module.blocks.2.sa.q_bias, '\n"
                    " 'module.blocks.2.sa.v_bias, module.blocks.2.sa.proj.bias, module.blocks.2.ca.v_bias, module.blocks.2.ca.mat_q.bias, module.blocks.2.ca.proj.bias, module.blocks.2.ffn.fc1.bias, '\n"
                    " 'module.blocks.2.ffn.fc2.bias, module.blocks.2.ca_norm.bias, module.blocks.3.ada_gss, module.blocks.3.sa.scale_mul_1H11, module.blocks.3.sa.q_bias, module.blocks.3.sa.v_bias, '\n"
                    " 'module.blocks.3.sa.proj.bias, module.blocks.3.ca.v_bias, module.blocks.3.ca.mat_q.bias, module.blocks.3.ca.proj.bias, module.blocks.3.ffn.fc1.bias, module.blocks.3.ffn.fc2.bias, '\n"
                    " 'module.blocks.3.ca_norm.bias, module.blocks.4.ada_gss, module.blocks.4.sa.scale_mul_1H11, module.blocks.4.sa.q_bias, module.blocks.4.sa.v_bias, module.blocks.4.sa.proj.bias, '\n"
                    " 'module.blocks.4.ca.v_bias, module.blocks.4.ca.mat_q.bias, module.blocks.4.ca.proj.bias, module.blocks.4.ffn.fc1.bias, module.blocks.4.ffn.fc2.bias, module.blocks.4.ca_norm.bias, '\n"
                    " 'module.blocks.5.ada_gss, module.blocks.5.sa.scale_mul_1H11, module.blocks.5.sa.q_bias, module.blocks.5.sa.v_bias, module.blocks.5.sa.proj.bias, module.blocks.5.ca.v_bias, '\n"
                    " 'module.blocks.5.ca.mat_q.bias, module.blocks.5.ca.proj.bias, module.blocks.5.ffn.fc1.bias, module.blocks.5.ffn.fc2.bias, module.blocks.5.ca_norm.bias, module.blocks.6.ada_gss, '\n"
                    " 'module.blocks.6.sa.scale_mul_1H11, module.blocks.6.sa.q_bias, module.blocks.6.sa.v_bias, module.blocks.6.sa.proj.bias, module.blocks.6.ca.v_bias, module.blocks.6.ca.mat_q.bias, '\n"
                    " 'module.blocks.6.ca.proj.bias, module.blocks.6.ffn.fc1.bias, module.blocks.6.ffn.fc2.bias, module.blocks.6.ca_norm.bias, module.blocks.7.ada_gss, module.blocks.7.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.7.sa.q_bias, module.blocks.7.sa.v_bias, module.blocks.7.sa.proj.bias, module.blocks.7.ca.v_bias, module.blocks.7.ca.mat_q.bias, module.blocks.7.ca.proj.bias, '\n"
                    " 'module.blocks.7.ffn.fc1.bias, module.blocks.7.ffn.fc2.bias, module.blocks.7.ca_norm.bias, module.blocks.8.ada_gss, module.blocks.8.sa.scale_mul_1H11, module.blocks.8.sa.q_bias, '\n"
                    " 'module.blocks.8.sa.v_bias, module.blocks.8.sa.proj.bias, module.blocks.8.ca.v_bias, module.blocks.8.ca.mat_q.bias, module.blocks.8.ca.proj.bias, module.blocks.8.ffn.fc1.bias, '\n"
                    " 'module.blocks.8.ffn.fc2.bias, module.blocks.8.ca_norm.bias, module.blocks.9.ada_gss, module.blocks.9.sa.scale_mul_1H11, module.blocks.9.sa.q_bias, module.blocks.9.sa.v_bias, '\n"
                    " 'module.blocks.9.sa.proj.bias, module.blocks.9.ca.v_bias, module.blocks.9.ca.mat_q.bias, module.blocks.9.ca.proj.bias, module.blocks.9.ffn.fc1.bias, module.blocks.9.ffn.fc2.bias, '\n"
                    " 'module.blocks.9.ca_norm.bias, module.blocks.10.ada_gss, module.blocks.10.sa.scale_mul_1H11, module.blocks.10.sa.q_bias, module.blocks.10.sa.v_bias, module.blocks.10.sa.proj.bias, '\n"
                    " 'module.blocks.10.ca.v_bias, module.blocks.10.ca.mat_q.bias, module.blocks.10.ca.proj.bias, module.blocks.10.ffn.fc1.bias, module.blocks.10.ffn.fc2.bias, module.blocks.10.ca_norm.bias, '\n"
                    " 'module.blocks.11.ada_gss, module.blocks.11.sa.scale_mul_1H11, module.blocks.11.sa.q_bias, module.blocks.11.sa.v_bias, module.blocks.11.sa.proj.bias, module.blocks.11.ca.v_bias, '\n"
                    " 'module.blocks.11.ca.mat_q.bias, module.blocks.11.ca.proj.bias, module.blocks.11.ffn.fc1.bias, module.blocks.11.ffn.fc2.bias, module.blocks.11.ca_norm.bias, module.blocks.12.ada_gss, '\n"
                    " 'module.blocks.12.sa.scale_mul_1H11, module.blocks.12.sa.q_bias, module.blocks.12.sa.v_bias, module.blocks.12.sa.proj.bias, module.blocks.12.ca.v_bias, module.blocks.12.ca.mat_q.bias, '\n"
                    " 'module.blocks.12.ca.proj.bias, module.blocks.12.ffn.fc1.bias, module.blocks.12.ffn.fc2.bias, module.blocks.12.ca_norm.bias, module.blocks.13.ada_gss, module.blocks.13.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.13.sa.q_bias, module.blocks.13.sa.v_bias, module.blocks.13.sa.proj.bias, module.blocks.13.ca.v_bias, module.blocks.13.ca.mat_q.bias, module.blocks.13.ca.proj.bias, '\n"
                    " 'module.blocks.13.ffn.fc1.bias, module.blocks.13.ffn.fc2.bias, module.blocks.13.ca_norm.bias, module.blocks.14.ada_gss, module.blocks.14.sa.scale_mul_1H11, module.blocks.14.sa.q_bias, '\n"
                    " 'module.blocks.14.sa.v_bias, module.blocks.14.sa.proj.bias, module.blocks.14.ca.v_bias, module.blocks.14.ca.mat_q.bias, module.blocks.14.ca.proj.bias, module.blocks.14.ffn.fc1.bias, '\n"
                    " 'module.blocks.14.ffn.fc2.bias, module.blocks.14.ca_norm.bias, module.blocks.15.ada_gss, module.blocks.15.sa.scale_mul_1H11, module.blocks.15.sa.q_bias, module.blocks.15.sa.v_bias, '\n"
                    " 'module.blocks.15.sa.proj.bias, module.blocks.15.ca.v_bias, module.blocks.15.ca.mat_q.bias, module.blocks.15.ca.proj.bias, module.blocks.15.ffn.fc1.bias, module.blocks.15.ffn.fc2.bias, '\n"
                    " 'module.blocks.15.ca_norm.bias, module.blocks.16.ada_gss, module.blocks.16.sa.scale_mul_1H11, module.blocks.16.sa.q_bias, module.blocks.16.sa.v_bias, module.blocks.16.sa.proj.bias, '\n"
                    " 'module.blocks.16.ca.v_bias, module.blocks.16.ca.mat_q.bias, module.blocks.16.ca.proj.bias, module.blocks.16.ffn.fc1.bias, module.blocks.16.ffn.fc2.bias, module.blocks.16.ca_norm.bias, '\n"
                    " 'module.blocks.17.ada_gss, module.blocks.17.sa.scale_mul_1H11, module.blocks.17.sa.q_bias, module.blocks.17.sa.v_bias, module.blocks.17.sa.proj.bias, module.blocks.17.ca.v_bias, '\n"
                    " 'module.blocks.17.ca.mat_q.bias, module.blocks.17.ca.proj.bias, module.blocks.17.ffn.fc1.bias, module.blocks.17.ffn.fc2.bias, module.blocks.17.ca_norm.bias, module.blocks.18.ada_gss, '\n"
                    " 'module.blocks.18.sa.scale_mul_1H11, module.blocks.18.sa.q_bias, module.blocks.18.sa.v_bias, module.blocks.18.sa.proj.bias, module.blocks.18.ca.v_bias, module.blocks.18.ca.mat_q.bias, '\n"
                    " 'module.blocks.18.ca.proj.bias, module.blocks.18.ffn.fc1.bias, module.blocks.18.ffn.fc2.bias, module.blocks.18.ca_norm.bias, module.blocks.19.ada_gss, module.blocks.19.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.19.sa.q_bias, module.blocks.19.sa.v_bias, module.blocks.19.sa.proj.bias, module.blocks.19.ca.v_bias, module.blocks.19.ca.mat_q.bias, module.blocks.19.ca.proj.bias, '\n"
                    " 'module.blocks.19.ffn.fc1.bias, module.blocks.19.ffn.fc2.bias, module.blocks.19.ca_norm.bias, module.blocks.20.ada_gss, module.blocks.20.sa.scale_mul_1H11, module.blocks.20.sa.q_bias, '\n"
                    " 'module.blocks.20.sa.v_bias, module.blocks.20.sa.proj.bias, module.blocks.20.ca.v_bias, module.blocks.20.ca.mat_q.bias, module.blocks.20.ca.proj.bias, module.blocks.20.ffn.fc1.bias, '\n"
                    " 'module.blocks.20.ffn.fc2.bias, module.blocks.20.ca_norm.bias, module.blocks.21.ada_gss, module.blocks.21.sa.scale_mul_1H11, module.blocks.21.sa.q_bias, module.blocks.21.sa.v_bias, '\n"
                    " 'module.blocks.21.sa.proj.bias, module.blocks.21.ca.v_bias, module.blocks.21.ca.mat_q.bias, module.blocks.21.ca.proj.bias, module.blocks.21.ffn.fc1.bias, module.blocks.21.ffn.fc2.bias, '\n"
                    " 'module.blocks.21.ca_norm.bias, module.blocks.22.ada_gss, module.blocks.22.sa.scale_mul_1H11, module.blocks.22.sa.q_bias, module.blocks.22.sa.v_bias, module.blocks.22.sa.proj.bias, '\n"
                    " 'module.blocks.22.ca.v_bias, module.blocks.22.ca.mat_q.bias, module.blocks.22.ca.proj.bias, module.blocks.22.ffn.fc1.bias, module.blocks.22.ffn.fc2.bias, module.blocks.22.ca_norm.bias, '\n"
                    " 'module.blocks.23.ada_gss, module.blocks.23.sa.scale_mul_1H11, module.blocks.23.sa.q_bias, module.blocks.23.sa.v_bias, module.blocks.23.sa.proj.bias, module.blocks.23.ca.v_bias, '\n"
                    " 'module.blocks.23.ca.mat_q.bias, module.blocks.23.ca.proj.bias, module.blocks.23.ffn.fc1.bias, module.blocks.23.ffn.fc2.bias, module.blocks.23.ca_norm.bias, module.blocks.24.ada_gss, '\n"
                    " 'module.blocks.24.sa.scale_mul_1H11, module.blocks.24.sa.q_bias, module.blocks.24.sa.v_bias, module.blocks.24.sa.proj.bias, module.blocks.24.ca.v_bias, module.blocks.24.ca.mat_q.bias, '\n"
                    " 'module.blocks.24.ca.proj.bias, module.blocks.24.ffn.fc1.bias, module.blocks.24.ffn.fc2.bias, module.blocks.24.ca_norm.bias, module.blocks.25.ada_gss, module.blocks.25.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.25.sa.q_bias, module.blocks.25.sa.v_bias, module.blocks.25.sa.proj.bias, module.blocks.25.ca.v_bias, module.blocks.25.ca.mat_q.bias, module.blocks.25.ca.proj.bias, '\n"
                    " 'module.blocks.25.ffn.fc1.bias, module.blocks.25.ffn.fc2.bias, module.blocks.25.ca_norm.bias, module.blocks.26.ada_gss, module.blocks.26.sa.scale_mul_1H11, module.blocks.26.sa.q_bias, '\n"
                    " 'module.blocks.26.sa.v_bias, module.blocks.26.sa.proj.bias, module.blocks.26.ca.v_bias, module.blocks.26.ca.mat_q.bias, module.blocks.26.ca.proj.bias, module.blocks.26.ffn.fc1.bias, '\n"
                    " 'module.blocks.26.ffn.fc2.bias, module.blocks.26.ca_norm.bias, module.blocks.27.ada_gss, module.blocks.27.sa.scale_mul_1H11, module.blocks.27.sa.q_bias, module.blocks.27.sa.v_bias, '\n"
                    " 'module.blocks.27.sa.proj.bias, module.blocks.27.ca.v_bias, module.blocks.27.ca.mat_q.bias, module.blocks.27.ca.proj.bias, module.blocks.27.ffn.fc1.bias, module.blocks.27.ffn.fc2.bias, '\n"
                    " 'module.blocks.27.ca_norm.bias, module.blocks.28.ada_gss, module.blocks.28.sa.scale_mul_1H11, module.blocks.28.sa.q_bias, module.blocks.28.sa.v_bias, module.blocks.28.sa.proj.bias, '\n"
                    " 'module.blocks.28.ca.v_bias, module.blocks.28.ca.mat_q.bias, module.blocks.28.ca.proj.bias, module.blocks.28.ffn.fc1.bias, module.blocks.28.ffn.fc2.bias, module.blocks.28.ca_norm.bias, '\n"
                    " 'module.blocks.29.ada_gss, module.blocks.29.sa.scale_mul_1H11, module.blocks.29.sa.q_bias, module.blocks.29.sa.v_bias, module.blocks.29.sa.proj.bias, module.blocks.29.ca.v_bias, '\n"
                    " 'module.blocks.29.ca.mat_q.bias, module.blocks.29.ca.proj.bias, module.blocks.29.ffn.fc1.bias, module.blocks.29.ffn.fc2.bias, module.blocks.29.ca_norm.bias, module.blocks.30.ada_gss, '\n"
                    " 'module.blocks.30.sa.scale_mul_1H11, module.blocks.30.sa.q_bias, module.blocks.30.sa.v_bias, module.blocks.30.sa.proj.bias, module.blocks.30.ca.v_bias, module.blocks.30.ca.mat_q.bias, '\n"
                    " 'module.blocks.30.ca.proj.bias, module.blocks.30.ffn.fc1.bias, module.blocks.30.ffn.fc2.bias, module.blocks.30.ca_norm.bias, module.blocks.31.ada_gss, module.blocks.31.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.31.sa.q_bias, module.blocks.31.sa.v_bias, module.blocks.31.sa.proj.bias, module.blocks.31.ca.v_bias, module.blocks.31.ca.mat_q.bias, module.blocks.31.ca.proj.bias, '\n"
                    " 'module.blocks.31.ffn.fc1.bias, module.blocks.31.ffn.fc2.bias, module.blocks.31.ca_norm.bias, module.blocks.32.ada_gss, module.blocks.32.sa.scale_mul_1H11, module.blocks.32.sa.q_bias, '\n"
                    " 'module.blocks.32.sa.v_bias, module.blocks.32.sa.proj.bias, module.blocks.32.ca.v_bias, module.blocks.32.ca.mat_q.bias, module.blocks.32.ca.proj.bias, module.blocks.32.ffn.fc1.bias, '\n"
                    " 'module.blocks.32.ffn.fc2.bias, module.blocks.32.ca_norm.bias, module.blocks.33.ada_gss, module.blocks.33.sa.scale_mul_1H11, module.blocks.33.sa.q_bias, module.blocks.33.sa.v_bias, '\n"
                    " 'module.blocks.33.sa.proj.bias, module.blocks.33.ca.v_bias, module.blocks.33.ca.mat_q.bias, module.blocks.33.ca.proj.bias, module.blocks.33.ffn.fc1.bias, module.blocks.33.ffn.fc2.bias, '\n"
                    " 'module.blocks.33.ca_norm.bias, module.blocks.34.ada_gss, module.blocks.34.sa.scale_mul_1H11, module.blocks.34.sa.q_bias, module.blocks.34.sa.v_bias, module.blocks.34.sa.proj.bias, '\n"
                    " 'module.blocks.34.ca.v_bias, module.blocks.34.ca.mat_q.bias, module.blocks.34.ca.proj.bias, module.blocks.34.ffn.fc1.bias, module.blocks.34.ffn.fc2.bias, module.blocks.34.ca_norm.bias, '\n"
                    " 'module.blocks.35.ada_gss, module.blocks.35.sa.scale_mul_1H11, module.blocks.35.sa.q_bias, module.blocks.35.sa.v_bias, module.blocks.35.sa.proj.bias, module.blocks.35.ca.v_bias, '\n"
                    " 'module.blocks.35.ca.mat_q.bias, module.blocks.35.ca.proj.bias, module.blocks.35.ffn.fc1.bias, module.blocks.35.ffn.fc2.bias, module.blocks.35.ca_norm.bias, module.blocks.36.ada_gss, '\n"
                    " 'module.blocks.36.sa.scale_mul_1H11, module.blocks.36.sa.q_bias, module.blocks.36.sa.v_bias, module.blocks.36.sa.proj.bias, module.blocks.36.ca.v_bias, module.blocks.36.ca.mat_q.bias, '\n"
                    " 'module.blocks.36.ca.proj.bias, module.blocks.36.ffn.fc1.bias, module.blocks.36.ffn.fc2.bias, module.blocks.36.ca_norm.bias, module.blocks.37.ada_gss, module.blocks.37.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.37.sa.q_bias, module.blocks.37.sa.v_bias, module.blocks.37.sa.proj.bias, module.blocks.37.ca.v_bias, module.blocks.37.ca.mat_q.bias, module.blocks.37.ca.proj.bias, '\n"
                    " 'module.blocks.37.ffn.fc1.bias, module.blocks.37.ffn.fc2.bias, module.blocks.37.ca_norm.bias, module.blocks.38.ada_gss, module.blocks.38.sa.scale_mul_1H11, module.blocks.38.sa.q_bias, '\n"
                    " 'module.blocks.38.sa.v_bias, module.blocks.38.sa.proj.bias, module.blocks.38.ca.v_bias, module.blocks.38.ca.mat_q.bias, module.blocks.38.ca.proj.bias, module.blocks.38.ffn.fc1.bias, '\n"
                    " 'module.blocks.38.ffn.fc2.bias, module.blocks.38.ca_norm.bias, module.blocks.39.ada_gss, module.blocks.39.sa.scale_mul_1H11, module.blocks.39.sa.q_bias, module.blocks.39.sa.v_bias, '\n"
                    " 'module.blocks.39.sa.proj.bias, module.blocks.39.ca.v_bias, module.blocks.39.ca.mat_q.bias, module.blocks.39.ca.proj.bias, module.blocks.39.ffn.fc1.bias, module.blocks.39.ffn.fc2.bias, '\n"
                    " 'module.blocks.39.ca_norm.bias, module.blocks.40.ada_gss, module.blocks.40.sa.scale_mul_1H11, module.blocks.40.sa.q_bias, module.blocks.40.sa.v_bias, module.blocks.40.sa.proj.bias, '\n"
                    " 'module.blocks.40.ca.v_bias, module.blocks.40.ca.mat_q.bias, module.blocks.40.ca.proj.bias, module.blocks.40.ffn.fc1.bias, module.blocks.40.ffn.fc2.bias, module.blocks.40.ca_norm.bias, '\n"
                    " 'module.blocks.41.ada_gss, module.blocks.41.sa.scale_mul_1H11, module.blocks.41.sa.q_bias, module.blocks.41.sa.v_bias, module.blocks.41.sa.proj.bias, module.blocks.41.ca.v_bias, '\n"
                    " 'module.blocks.41.ca.mat_q.bias, module.blocks.41.ca.proj.bias, module.blocks.41.ffn.fc1.bias, module.blocks.41.ffn.fc2.bias, module.blocks.41.ca_norm.bias, module.blocks.42.ada_gss, '\n"
                    " 'module.blocks.42.sa.scale_mul_1H11, module.blocks.42.sa.q_bias, module.blocks.42.sa.v_bias, module.blocks.42.sa.proj.bias, module.blocks.42.ca.v_bias, module.blocks.42.ca.mat_q.bias, '\n"
                    " 'module.blocks.42.ca.proj.bias, module.blocks.42.ffn.fc1.bias, module.blocks.42.ffn.fc2.bias, module.blocks.42.ca_norm.bias, module.blocks.43.ada_gss, module.blocks.43.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.43.sa.q_bias, module.blocks.43.sa.v_bias, module.blocks.43.sa.proj.bias, module.blocks.43.ca.v_bias, module.blocks.43.ca.mat_q.bias, module.blocks.43.ca.proj.bias, '\n"
                    " 'module.blocks.43.ffn.fc1.bias, module.blocks.43.ffn.fc2.bias, module.blocks.43.ca_norm.bias, module.blocks.44.ada_gss, module.blocks.44.sa.scale_mul_1H11, module.blocks.44.sa.q_bias, '\n"
                    " 'module.blocks.44.sa.v_bias, module.blocks.44.sa.proj.bias, module.blocks.44.ca.v_bias, module.blocks.44.ca.mat_q.bias, module.blocks.44.ca.proj.bias, module.blocks.44.ffn.fc1.bias, '\n"
                    " 'module.blocks.44.ffn.fc2.bias, module.blocks.44.ca_norm.bias, module.blocks.45.ada_gss, module.blocks.45.sa.scale_mul_1H11, module.blocks.45.sa.q_bias, module.blocks.45.sa.v_bias, '\n"
                    " 'module.blocks.45.sa.proj.bias, module.blocks.45.ca.v_bias, module.blocks.45.ca.mat_q.bias, module.blocks.45.ca.proj.bias, module.blocks.45.ffn.fc1.bias, module.blocks.45.ffn.fc2.bias, '\n"
                    " 'module.blocks.45.ca_norm.bias, module.blocks.46.ada_gss, module.blocks.46.sa.scale_mul_1H11, module.blocks.46.sa.q_bias, module.blocks.46.sa.v_bias, module.blocks.46.sa.proj.bias, '\n"
                    " 'module.blocks.46.ca.v_bias, module.blocks.46.ca.mat_q.bias, module.blocks.46.ca.proj.bias, module.blocks.46.ffn.fc1.bias, module.blocks.46.ffn.fc2.bias, module.blocks.46.ca_norm.bias, '\n"
                    " 'module.blocks.47.ada_gss, module.blocks.47.sa.scale_mul_1H11, module.blocks.47.sa.q_bias, module.blocks.47.sa.v_bias, module.blocks.47.sa.proj.bias, module.blocks.47.ca.v_bias, '\n"
                    " 'module.blocks.47.ca.mat_q.bias, module.blocks.47.ca.proj.bias, module.blocks.47.ffn.fc1.bias, module.blocks.47.ffn.fc2.bias, module.blocks.47.ca_norm.bias, module.blocks.48.ada_gss, '\n"
                    " 'module.blocks.48.sa.scale_mul_1H11, module.blocks.48.sa.q_bias, module.blocks.48.sa.v_bias, module.blocks.48.sa.proj.bias, module.blocks.48.ca.v_bias, module.blocks.48.ca.mat_q.bias, '\n"
                    " 'module.blocks.48.ca.proj.bias, module.blocks.48.ffn.fc1.bias, module.blocks.48.ffn.fc2.bias, module.blocks.48.ca_norm.bias, module.blocks.49.ada_gss, module.blocks.49.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.49.sa.q_bias, module.blocks.49.sa.v_bias, module.blocks.49.sa.proj.bias, module.blocks.49.ca.v_bias, module.blocks.49.ca.mat_q.bias, module.blocks.49.ca.proj.bias, '\n"
                    " 'module.blocks.49.ffn.fc1.bias, module.blocks.49.ffn.fc2.bias, module.blocks.49.ca_norm.bias, module.blocks.50.ada_gss, module.blocks.50.sa.scale_mul_1H11, module.blocks.50.sa.q_bias, '\n"
                    " 'module.blocks.50.sa.v_bias, module.blocks.50.sa.proj.bias, module.blocks.50.ca.v_bias, module.blocks.50.ca.mat_q.bias, module.blocks.50.ca.proj.bias, module.blocks.50.ffn.fc1.bias, '\n"
                    " 'module.blocks.50.ffn.fc2.bias, module.blocks.50.ca_norm.bias, module.blocks.51.ada_gss, module.blocks.51.sa.scale_mul_1H11, module.blocks.51.sa.q_bias, module.blocks.51.sa.v_bias, '\n"
                    " 'module.blocks.51.sa.proj.bias, module.blocks.51.ca.v_bias, module.blocks.51.ca.mat_q.bias, module.blocks.51.ca.proj.bias, module.blocks.51.ffn.fc1.bias, module.blocks.51.ffn.fc2.bias, '\n"
                    " 'module.blocks.51.ca_norm.bias, module.blocks.52.ada_gss, module.blocks.52.sa.scale_mul_1H11, module.blocks.52.sa.q_bias, module.blocks.52.sa.v_bias, module.blocks.52.sa.proj.bias, '\n"
                    " 'module.blocks.52.ca.v_bias, module.blocks.52.ca.mat_q.bias, module.blocks.52.ca.proj.bias, module.blocks.52.ffn.fc1.bias, module.blocks.52.ffn.fc2.bias, module.blocks.52.ca_norm.bias, '\n"
                    " 'module.blocks.53.ada_gss, module.blocks.53.sa.scale_mul_1H11, module.blocks.53.sa.q_bias, module.blocks.53.sa.v_bias, module.blocks.53.sa.proj.bias, module.blocks.53.ca.v_bias, '\n"
                    " 'module.blocks.53.ca.mat_q.bias, module.blocks.53.ca.proj.bias, module.blocks.53.ffn.fc1.bias, module.blocks.53.ffn.fc2.bias, module.blocks.53.ca_norm.bias, module.blocks.54.ada_gss, '\n"
                    " 'module.blocks.54.sa.scale_mul_1H11, module.blocks.54.sa.q_bias, module.blocks.54.sa.v_bias, module.blocks.54.sa.proj.bias, module.blocks.54.ca.v_bias, module.blocks.54.ca.mat_q.bias, '\n"
                    " 'module.blocks.54.ca.proj.bias, module.blocks.54.ffn.fc1.bias, module.blocks.54.ffn.fc2.bias, module.blocks.54.ca_norm.bias, module.blocks.55.ada_gss, module.blocks.55.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.55.sa.q_bias, module.blocks.55.sa.v_bias, module.blocks.55.sa.proj.bias, module.blocks.55.ca.v_bias, module.blocks.55.ca.mat_q.bias, module.blocks.55.ca.proj.bias, '\n"
                    " 'module.blocks.55.ffn.fc1.bias, module.blocks.55.ffn.fc2.bias, module.blocks.55.ca_norm.bias')",
          'wd_sc': 0.0}}

[09-01 23:55:00] (nity/utils/lr_control.py, line 116)=> [get_param_groups][rank0] type(model).__name__='NullDDP' count=1085, numel=8573892016
[09-01 23:55:00] (nity/utils/lr_control.py, line 117)=> 
[09-01 23:55:00] (inity_circuit_breaker.py, line 343)=> [vgpt] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.97), fused=True), opt_kw={'lr': 9.375e-05, 'weight_decay': 0}

[09-01 23:55:00] (inity_circuit_breaker.py, line 348)=> Loading T5 from google/flan-t5-xl...







=======================================================   RESTART [09-02 00:00:22]   =======================================================
[09-02 00:00:22] (inity_circuit_breaker.py, line 718)=> ============================================================
[09-02 00:00:22] (inity_circuit_breaker.py, line 719)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[09-02 00:00:22] (inity_circuit_breaker.py, line 720)=> Circuit Breaker Alpha: 0.1
[09-02 00:00:22] (inity_circuit_breaker.py, line 721)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[09-02 00:00:22] (inity_circuit_breaker.py, line 722)=> Circuit Breaker Enabled: True
[09-02 00:00:22] (inity_circuit_breaker.py, line 723)=> Number of Examples: 1000
[09-02 00:00:22] (inity_circuit_breaker.py, line 724)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[09-02 00:00:22] (inity_circuit_breaker.py, line 725)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[09-02 00:00:22] (inity_circuit_breaker.py, line 726)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[09-02 00:00:22] (inity_circuit_breaker.py, line 727)=> Validation Ratio: 0.1
[09-02 00:00:22] (inity_circuit_breaker.py, line 728)=> Category: hate
[09-02 00:00:22] (inity_circuit_breaker.py, line 729)=> Batch Size: 4
[09-02 00:00:22] (inity_circuit_breaker.py, line 730)=> Workers: 4
[09-02 00:00:22] (inity_circuit_breaker.py, line 731)=> Device: cuda:0
[09-02 00:00:22] (inity_circuit_breaker.py, line 732)=> Model: infinity_8b
[09-02 00:00:22] (inity_circuit_breaker.py, line 733)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-02 00:00:22] (inity_circuit_breaker.py, line 734)=> Rush Resume: 
[09-02 00:00:22] (inity_circuit_breaker.py, line 735)=> Selective Layers: 0,1,2,3,4,5
[09-02 00:00:22] (inity_circuit_breaker.py, line 736)=> ============================================================
[09-02 00:00:22] (inity_circuit_breaker.py, line 377)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[09-02 00:00:22] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-02 00:00:22] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-02 00:00:22] (rain_dataset_infinity.py, line 205)=> Split 1000 prompts: 900 retain, 100 validation
[09-02 00:00:22] (rain_dataset_infinity.py, line 236)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-02 00:00:22] (rain_dataset_infinity.py, line 245)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-02 00:00:22] (rain_dataset_infinity.py, line  84)=> Loaded 900 retain examples
[09-02 00:00:22] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[09-02 00:00:22] (rain_dataset_infinity.py, line  86)=> Loaded 100 validation examples
[09-02 00:00:22] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[09-02 00:00:22] (inity_circuit_breaker.py, line 395)=> args.batch_size=4, vbs=6
[09-02 00:00:22] (inity_circuit_breaker.py, line 418)=> len(dataloader): 100, len(dataset): 100
[09-02 00:00:22] (inity_circuit_breaker.py, line 420)=> total_samples: 200
[09-02 00:00:22] (inity_circuit_breaker.py, line 421)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[09-02 00:00:22] (inity_circuit_breaker.py, line 140)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[09-02 00:00:22] (inity_circuit_breaker.py, line 142)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[09-02 00:00:22] (inity_circuit_breaker.py, line 146)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-02 00:00:25] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[09-02 00:00:25] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[09-02 00:00:25] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[09-02 00:00:25] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[09-02 00:00:27] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[09-02 00:00:29] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[09-02 00:00:29] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[09-02 00:00:29] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[09-02 00:00:29] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[09-02 00:00:29] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

[09-02 00:00:41] (ity/models/init_param.py, line  15)=> [init_weights] Infinity with std=0.02
[09-02 00:00:41] (inity_circuit_breaker.py, line 246)=> [PT] GPT model = Infinity(
  drop_path_rate=0.2
  (norm0_cond): Identity()
  (text_norm): FastRMSNorm(C=2048, eps=1e-06, elementwise_affine=True)
  (text_proj_for_sos): TextAttentivePool(
    (ca): CrossAttention(
      Cq=3072, Ckv=2048, cos_attn=False
      (mat_kv): Linear(in_features=2048, out_features=6144, bias=False)
      (proj): Linear(in_features=3072, out_features=3072, bias=True)
      (proj_drop): Identity()
    )
  )
  (text_proj_for_ca): Sequential(
    (0): Linear(in_features=2048, out_features=3072, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=3072, out_features=3072, bias=True)
  )
  (lvl_embed): Embedding(15, 3072)
  (norm0_ve): Identity()
  (word_embed): Linear(in_features=56, out_features=3072, bias=True)
  (shared_ada_lin): Sequential(
    (0): SiLU()
    (1): SharedAdaLin(in_features=3072, out_features=18432, bias=True)
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=3072, out_features=6144, bias=True)
    )
  )
  (head): Linear(in_features=3072, out_features=112, bias=True)
  (blocks): ModuleList(
    (0): CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): Identity()
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
    (1-55): 55 x CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): DropPath(...)
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
  )
)


[09-02 00:00:41] (inity_circuit_breaker.py, line 248)=> [PT][#para] VAE=81.62, VAE.quant=0.00
[09-02 00:00:41] (inity_circuit_breaker.py, line 251)=> [PT][#para] GPT=8573.89


[09-02 00:00:41] (nity/utils/lr_control.py, line  71)=> [get_param_groups][lr decay] with_lr_scale=False, lr_scale=0.0
[09-02 00:00:41] (nity/utils/lr_control.py, line 111)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': '[no scale]',
         'params': "('module.text_norm.weight, module.text_proj_for_sos.ca.mat_kv.weight, module.text_proj_for_sos.ca.proj.weight, module.text_proj_for_ca.0.weight, module.text_proj_for_ca.2.weight, '\n"
                   " 'module.word_embed.weight, module.shared_ada_lin.1.weight, module.head_nm.ada_lin.1.weight, module.head.weight, module.blocks.0.sa.mat_qkv.weight, module.blocks.0.sa.proj.weight, '\n"
                   " 'module.blocks.0.ca.mat_q.weight, module.blocks.0.ca.mat_kv.weight, module.blocks.0.ca.proj.weight, module.blocks.0.ffn.fc1.weight, module.blocks.0.ffn.fc2.weight, module.blocks.0.ca_norm.weight, '\n"
                   " 'module.blocks.1.sa.mat_qkv.weight, module.blocks.1.sa.proj.weight, module.blocks.1.ca.mat_q.weight, module.blocks.1.ca.mat_kv.weight, module.blocks.1.ca.proj.weight, '\n"
                   " 'module.blocks.1.ffn.fc1.weight, module.blocks.1.ffn.fc2.weight, module.blocks.1.ca_norm.weight, module.blocks.2.sa.mat_qkv.weight, module.blocks.2.sa.proj.weight, module.blocks.2.ca.mat_q.weight, '\n"
                   " 'module.blocks.2.ca.mat_kv.weight, module.blocks.2.ca.proj.weight, module.blocks.2.ffn.fc1.weight, module.blocks.2.ffn.fc2.weight, module.blocks.2.ca_norm.weight, module.blocks.3.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.3.sa.proj.weight, module.blocks.3.ca.mat_q.weight, module.blocks.3.ca.mat_kv.weight, module.blocks.3.ca.proj.weight, module.blocks.3.ffn.fc1.weight, module.blocks.3.ffn.fc2.weight, '\n"
                   " 'module.blocks.3.ca_norm.weight, module.blocks.4.sa.mat_qkv.weight, module.blocks.4.sa.proj.weight, module.blocks.4.ca.mat_q.weight, module.blocks.4.ca.mat_kv.weight, '\n"
                   " 'module.blocks.4.ca.proj.weight, module.blocks.4.ffn.fc1.weight, module.blocks.4.ffn.fc2.weight, module.blocks.4.ca_norm.weight, module.blocks.5.sa.mat_qkv.weight, module.blocks.5.sa.proj.weight, '\n"
                   " 'module.blocks.5.ca.mat_q.weight, module.blocks.5.ca.mat_kv.weight, module.blocks.5.ca.proj.weight, module.blocks.5.ffn.fc1.weight, module.blocks.5.ffn.fc2.weight, module.blocks.5.ca_norm.weight, '\n"
                   " 'module.blocks.6.sa.mat_qkv.weight, module.blocks.6.sa.proj.weight, module.blocks.6.ca.mat_q.weight, module.blocks.6.ca.mat_kv.weight, module.blocks.6.ca.proj.weight, '\n"
                   " 'module.blocks.6.ffn.fc1.weight, module.blocks.6.ffn.fc2.weight, module.blocks.6.ca_norm.weight, module.blocks.7.sa.mat_qkv.weight, module.blocks.7.sa.proj.weight, module.blocks.7.ca.mat_q.weight, '\n"
                   " 'module.blocks.7.ca.mat_kv.weight, module.blocks.7.ca.proj.weight, module.blocks.7.ffn.fc1.weight, module.blocks.7.ffn.fc2.weight, module.blocks.7.ca_norm.weight, module.blocks.8.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.8.sa.proj.weight, module.blocks.8.ca.mat_q.weight, module.blocks.8.ca.mat_kv.weight, module.blocks.8.ca.proj.weight, module.blocks.8.ffn.fc1.weight, module.blocks.8.ffn.fc2.weight, '\n"
                   " 'module.blocks.8.ca_norm.weight, module.blocks.9.sa.mat_qkv.weight, module.blocks.9.sa.proj.weight, module.blocks.9.ca.mat_q.weight, module.blocks.9.ca.mat_kv.weight, '\n"
                   " 'module.blocks.9.ca.proj.weight, module.blocks.9.ffn.fc1.weight, module.blocks.9.ffn.fc2.weight, module.blocks.9.ca_norm.weight, module.blocks.10.sa.mat_qkv.weight, module.blocks.10.sa.proj.weight, '\n"
                   " 'module.blocks.10.ca.mat_q.weight, module.blocks.10.ca.mat_kv.weight, module.blocks.10.ca.proj.weight, module.blocks.10.ffn.fc1.weight, module.blocks.10.ffn.fc2.weight, '\n"
                   " 'module.blocks.10.ca_norm.weight, module.blocks.11.sa.mat_qkv.weight, module.blocks.11.sa.proj.weight, module.blocks.11.ca.mat_q.weight, module.blocks.11.ca.mat_kv.weight, '\n"
                   " 'module.blocks.11.ca.proj.weight, module.blocks.11.ffn.fc1.weight, module.blocks.11.ffn.fc2.weight, module.blocks.11.ca_norm.weight, module.blocks.12.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.12.sa.proj.weight, module.blocks.12.ca.mat_q.weight, module.blocks.12.ca.mat_kv.weight, module.blocks.12.ca.proj.weight, module.blocks.12.ffn.fc1.weight, '\n"
                   " 'module.blocks.12.ffn.fc2.weight, module.blocks.12.ca_norm.weight, module.blocks.13.sa.mat_qkv.weight, module.blocks.13.sa.proj.weight, module.blocks.13.ca.mat_q.weight, '\n"
                   " 'module.blocks.13.ca.mat_kv.weight, module.blocks.13.ca.proj.weight, module.blocks.13.ffn.fc1.weight, module.blocks.13.ffn.fc2.weight, module.blocks.13.ca_norm.weight, '\n"
                   " 'module.blocks.14.sa.mat_qkv.weight, module.blocks.14.sa.proj.weight, module.blocks.14.ca.mat_q.weight, module.blocks.14.ca.mat_kv.weight, module.blocks.14.ca.proj.weight, '\n"
                   " 'module.blocks.14.ffn.fc1.weight, module.blocks.14.ffn.fc2.weight, module.blocks.14.ca_norm.weight, module.blocks.15.sa.mat_qkv.weight, module.blocks.15.sa.proj.weight, '\n"
                   " 'module.blocks.15.ca.mat_q.weight, module.blocks.15.ca.mat_kv.weight, module.blocks.15.ca.proj.weight, module.blocks.15.ffn.fc1.weight, module.blocks.15.ffn.fc2.weight, '\n"
                   " 'module.blocks.15.ca_norm.weight, module.blocks.16.sa.mat_qkv.weight, module.blocks.16.sa.proj.weight, module.blocks.16.ca.mat_q.weight, module.blocks.16.ca.mat_kv.weight, '\n"
                   " 'module.blocks.16.ca.proj.weight, module.blocks.16.ffn.fc1.weight, module.blocks.16.ffn.fc2.weight, module.blocks.16.ca_norm.weight, module.blocks.17.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.17.sa.proj.weight, module.blocks.17.ca.mat_q.weight, module.blocks.17.ca.mat_kv.weight, module.blocks.17.ca.proj.weight, module.blocks.17.ffn.fc1.weight, '\n"
                   " 'module.blocks.17.ffn.fc2.weight, module.blocks.17.ca_norm.weight, module.blocks.18.sa.mat_qkv.weight, module.blocks.18.sa.proj.weight, module.blocks.18.ca.mat_q.weight, '\n"
                   " 'module.blocks.18.ca.mat_kv.weight, module.blocks.18.ca.proj.weight, module.blocks.18.ffn.fc1.weight, module.blocks.18.ffn.fc2.weight, module.blocks.18.ca_norm.weight, '\n"
                   " 'module.blocks.19.sa.mat_qkv.weight, module.blocks.19.sa.proj.weight, module.blocks.19.ca.mat_q.weight, module.blocks.19.ca.mat_kv.weight, module.blocks.19.ca.proj.weight, '\n"
                   " 'module.blocks.19.ffn.fc1.weight, module.blocks.19.ffn.fc2.weight, module.blocks.19.ca_norm.weight, module.blocks.20.sa.mat_qkv.weight, module.blocks.20.sa.proj.weight, '\n"
                   " 'module.blocks.20.ca.mat_q.weight, module.blocks.20.ca.mat_kv.weight, module.blocks.20.ca.proj.weight, module.blocks.20.ffn.fc1.weight, module.blocks.20.ffn.fc2.weight, '\n"
                   " 'module.blocks.20.ca_norm.weight, module.blocks.21.sa.mat_qkv.weight, module.blocks.21.sa.proj.weight, module.blocks.21.ca.mat_q.weight, module.blocks.21.ca.mat_kv.weight, '\n"
                   " 'module.blocks.21.ca.proj.weight, module.blocks.21.ffn.fc1.weight, module.blocks.21.ffn.fc2.weight, module.blocks.21.ca_norm.weight, module.blocks.22.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.22.sa.proj.weight, module.blocks.22.ca.mat_q.weight, module.blocks.22.ca.mat_kv.weight, module.blocks.22.ca.proj.weight, module.blocks.22.ffn.fc1.weight, '\n"
                   " 'module.blocks.22.ffn.fc2.weight, module.blocks.22.ca_norm.weight, module.blocks.23.sa.mat_qkv.weight, module.blocks.23.sa.proj.weight, module.blocks.23.ca.mat_q.weight, '\n"
                   " 'module.blocks.23.ca.mat_kv.weight, module.blocks.23.ca.proj.weight, module.blocks.23.ffn.fc1.weight, module.blocks.23.ffn.fc2.weight, module.blocks.23.ca_norm.weight, '\n"
                   " 'module.blocks.24.sa.mat_qkv.weight, module.blocks.24.sa.proj.weight, module.blocks.24.ca.mat_q.weight, module.blocks.24.ca.mat_kv.weight, module.blocks.24.ca.proj.weight, '\n"
                   " 'module.blocks.24.ffn.fc1.weight, module.blocks.24.ffn.fc2.weight, module.blocks.24.ca_norm.weight, module.blocks.25.sa.mat_qkv.weight, module.blocks.25.sa.proj.weight, '\n"
                   " 'module.blocks.25.ca.mat_q.weight, module.blocks.25.ca.mat_kv.weight, module.blocks.25.ca.proj.weight, module.blocks.25.ffn.fc1.weight, module.blocks.25.ffn.fc2.weight, '\n"
                   " 'module.blocks.25.ca_norm.weight, module.blocks.26.sa.mat_qkv.weight, module.blocks.26.sa.proj.weight, module.blocks.26.ca.mat_q.weight, module.blocks.26.ca.mat_kv.weight, '\n"
                   " 'module.blocks.26.ca.proj.weight, module.blocks.26.ffn.fc1.weight, module.blocks.26.ffn.fc2.weight, module.blocks.26.ca_norm.weight, module.blocks.27.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.27.sa.proj.weight, module.blocks.27.ca.mat_q.weight, module.blocks.27.ca.mat_kv.weight, module.blocks.27.ca.proj.weight, module.blocks.27.ffn.fc1.weight, '\n"
                   " 'module.blocks.27.ffn.fc2.weight, module.blocks.27.ca_norm.weight, module.blocks.28.sa.mat_qkv.weight, module.blocks.28.sa.proj.weight, module.blocks.28.ca.mat_q.weight, '\n"
                   " 'module.blocks.28.ca.mat_kv.weight, module.blocks.28.ca.proj.weight, module.blocks.28.ffn.fc1.weight, module.blocks.28.ffn.fc2.weight, module.blocks.28.ca_norm.weight, '\n"
                   " 'module.blocks.29.sa.mat_qkv.weight, module.blocks.29.sa.proj.weight, module.blocks.29.ca.mat_q.weight, module.blocks.29.ca.mat_kv.weight, module.blocks.29.ca.proj.weight, '\n"
                   " 'module.blocks.29.ffn.fc1.weight, module.blocks.29.ffn.fc2.weight, module.blocks.29.ca_norm.weight, module.blocks.30.sa.mat_qkv.weight, module.blocks.30.sa.proj.weight, '\n"
                   " 'module.blocks.30.ca.mat_q.weight, module.blocks.30.ca.mat_kv.weight, module.blocks.30.ca.proj.weight, module.blocks.30.ffn.fc1.weight, module.blocks.30.ffn.fc2.weight, '\n"
                   " 'module.blocks.30.ca_norm.weight, module.blocks.31.sa.mat_qkv.weight, module.blocks.31.sa.proj.weight, module.blocks.31.ca.mat_q.weight, module.blocks.31.ca.mat_kv.weight, '\n"
                   " 'module.blocks.31.ca.proj.weight, module.blocks.31.ffn.fc1.weight, module.blocks.31.ffn.fc2.weight, module.blocks.31.ca_norm.weight, module.blocks.32.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.32.sa.proj.weight, module.blocks.32.ca.mat_q.weight, module.blocks.32.ca.mat_kv.weight, module.blocks.32.ca.proj.weight, module.blocks.32.ffn.fc1.weight, '\n"
                   " 'module.blocks.32.ffn.fc2.weight, module.blocks.32.ca_norm.weight, module.blocks.33.sa.mat_qkv.weight, module.blocks.33.sa.proj.weight, module.blocks.33.ca.mat_q.weight, '\n"
                   " 'module.blocks.33.ca.mat_kv.weight, module.blocks.33.ca.proj.weight, module.blocks.33.ffn.fc1.weight, module.blocks.33.ffn.fc2.weight, module.blocks.33.ca_norm.weight, '\n"
                   " 'module.blocks.34.sa.mat_qkv.weight, module.blocks.34.sa.proj.weight, module.blocks.34.ca.mat_q.weight, module.blocks.34.ca.mat_kv.weight, module.blocks.34.ca.proj.weight, '\n"
                   " 'module.blocks.34.ffn.fc1.weight, module.blocks.34.ffn.fc2.weight, module.blocks.34.ca_norm.weight, module.blocks.35.sa.mat_qkv.weight, module.blocks.35.sa.proj.weight, '\n"
                   " 'module.blocks.35.ca.mat_q.weight, module.blocks.35.ca.mat_kv.weight, module.blocks.35.ca.proj.weight, module.blocks.35.ffn.fc1.weight, module.blocks.35.ffn.fc2.weight, '\n"
                   " 'module.blocks.35.ca_norm.weight, module.blocks.36.sa.mat_qkv.weight, module.blocks.36.sa.proj.weight, module.blocks.36.ca.mat_q.weight, module.blocks.36.ca.mat_kv.weight, '\n"
                   " 'module.blocks.36.ca.proj.weight, module.blocks.36.ffn.fc1.weight, module.blocks.36.ffn.fc2.weight, module.blocks.36.ca_norm.weight, module.blocks.37.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.37.sa.proj.weight, module.blocks.37.ca.mat_q.weight, module.blocks.37.ca.mat_kv.weight, module.blocks.37.ca.proj.weight, module.blocks.37.ffn.fc1.weight, '\n"
                   " 'module.blocks.37.ffn.fc2.weight, module.blocks.37.ca_norm.weight, module.blocks.38.sa.mat_qkv.weight, module.blocks.38.sa.proj.weight, module.blocks.38.ca.mat_q.weight, '\n"
                   " 'module.blocks.38.ca.mat_kv.weight, module.blocks.38.ca.proj.weight, module.blocks.38.ffn.fc1.weight, module.blocks.38.ffn.fc2.weight, module.blocks.38.ca_norm.weight, '\n"
                   " 'module.blocks.39.sa.mat_qkv.weight, module.blocks.39.sa.proj.weight, module.blocks.39.ca.mat_q.weight, module.blocks.39.ca.mat_kv.weight, module.blocks.39.ca.proj.weight, '\n"
                   " 'module.blocks.39.ffn.fc1.weight, module.blocks.39.ffn.fc2.weight, module.blocks.39.ca_norm.weight, module.blocks.40.sa.mat_qkv.weight, module.blocks.40.sa.proj.weight, '\n"
                   " 'module.blocks.40.ca.mat_q.weight, module.blocks.40.ca.mat_kv.weight, module.blocks.40.ca.proj.weight, module.blocks.40.ffn.fc1.weight, module.blocks.40.ffn.fc2.weight, '\n"
                   " 'module.blocks.40.ca_norm.weight, module.blocks.41.sa.mat_qkv.weight, module.blocks.41.sa.proj.weight, module.blocks.41.ca.mat_q.weight, module.blocks.41.ca.mat_kv.weight, '\n"
                   " 'module.blocks.41.ca.proj.weight, module.blocks.41.ffn.fc1.weight, module.blocks.41.ffn.fc2.weight, module.blocks.41.ca_norm.weight, module.blocks.42.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.42.sa.proj.weight, module.blocks.42.ca.mat_q.weight, module.blocks.42.ca.mat_kv.weight, module.blocks.42.ca.proj.weight, module.blocks.42.ffn.fc1.weight, '\n"
                   " 'module.blocks.42.ffn.fc2.weight, module.blocks.42.ca_norm.weight, module.blocks.43.sa.mat_qkv.weight, module.blocks.43.sa.proj.weight, module.blocks.43.ca.mat_q.weight, '\n"
                   " 'module.blocks.43.ca.mat_kv.weight, module.blocks.43.ca.proj.weight, module.blocks.43.ffn.fc1.weight, module.blocks.43.ffn.fc2.weight, module.blocks.43.ca_norm.weight, '\n"
                   " 'module.blocks.44.sa.mat_qkv.weight, module.blocks.44.sa.proj.weight, module.blocks.44.ca.mat_q.weight, module.blocks.44.ca.mat_kv.weight, module.blocks.44.ca.proj.weight, '\n"
                   " 'module.blocks.44.ffn.fc1.weight, module.blocks.44.ffn.fc2.weight, module.blocks.44.ca_norm.weight, module.blocks.45.sa.mat_qkv.weight, module.blocks.45.sa.proj.weight, '\n"
                   " 'module.blocks.45.ca.mat_q.weight, module.blocks.45.ca.mat_kv.weight, module.blocks.45.ca.proj.weight, module.blocks.45.ffn.fc1.weight, module.blocks.45.ffn.fc2.weight, '\n"
                   " 'module.blocks.45.ca_norm.weight, module.blocks.46.sa.mat_qkv.weight, module.blocks.46.sa.proj.weight, module.blocks.46.ca.mat_q.weight, module.blocks.46.ca.mat_kv.weight, '\n"
                   " 'module.blocks.46.ca.proj.weight, module.blocks.46.ffn.fc1.weight, module.blocks.46.ffn.fc2.weight, module.blocks.46.ca_norm.weight, module.blocks.47.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.47.sa.proj.weight, module.blocks.47.ca.mat_q.weight, module.blocks.47.ca.mat_kv.weight, module.blocks.47.ca.proj.weight, module.blocks.47.ffn.fc1.weight, '\n"
                   " 'module.blocks.47.ffn.fc2.weight, module.blocks.47.ca_norm.weight, module.blocks.48.sa.mat_qkv.weight, module.blocks.48.sa.proj.weight, module.blocks.48.ca.mat_q.weight, '\n"
                   " 'module.blocks.48.ca.mat_kv.weight, module.blocks.48.ca.proj.weight, module.blocks.48.ffn.fc1.weight, module.blocks.48.ffn.fc2.weight, module.blocks.48.ca_norm.weight, '\n"
                   " 'module.blocks.49.sa.mat_qkv.weight, module.blocks.49.sa.proj.weight, module.blocks.49.ca.mat_q.weight, module.blocks.49.ca.mat_kv.weight, module.blocks.49.ca.proj.weight, '\n"
                   " 'module.blocks.49.ffn.fc1.weight, module.blocks.49.ffn.fc2.weight, module.blocks.49.ca_norm.weight, module.blocks.50.sa.mat_qkv.weight, module.blocks.50.sa.proj.weight, '\n"
                   " 'module.blocks.50.ca.mat_q.weight, module.blocks.50.ca.mat_kv.weight, module.blocks.50.ca.proj.weight, module.blocks.50.ffn.fc1.weight, module.blocks.50.ffn.fc2.weight, '\n"
                   " 'module.blocks.50.ca_norm.weight, module.blocks.51.sa.mat_qkv.weight, module.blocks.51.sa.proj.weight, module.blocks.51.ca.mat_q.weight, module.blocks.51.ca.mat_kv.weight, '\n"
                   " 'module.blocks.51.ca.proj.weight, module.blocks.51.ffn.fc1.weight, module.blocks.51.ffn.fc2.weight, module.blocks.51.ca_norm.weight, module.blocks.52.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.52.sa.proj.weight, module.blocks.52.ca.mat_q.weight, module.blocks.52.ca.mat_kv.weight, module.blocks.52.ca.proj.weight, module.blocks.52.ffn.fc1.weight, '\n"
                   " 'module.blocks.52.ffn.fc2.weight, module.blocks.52.ca_norm.weight, module.blocks.53.sa.mat_qkv.weight, module.blocks.53.sa.proj.weight, module.blocks.53.ca.mat_q.weight, '\n"
                   " 'module.blocks.53.ca.mat_kv.weight, module.blocks.53.ca.proj.weight, module.blocks.53.ffn.fc1.weight, module.blocks.53.ffn.fc2.weight, module.blocks.53.ca_norm.weight, '\n"
                   " 'module.blocks.54.sa.mat_qkv.weight, module.blocks.54.sa.proj.weight, module.blocks.54.ca.mat_q.weight, module.blocks.54.ca.mat_kv.weight, module.blocks.54.ca.proj.weight, '\n"
                   " 'module.blocks.54.ffn.fc1.weight, module.blocks.54.ffn.fc2.weight, module.blocks.54.ca_norm.weight, module.blocks.55.sa.mat_qkv.weight, module.blocks.55.sa.proj.weight, '\n"
                   " 'module.blocks.55.ca.mat_q.weight, module.blocks.55.ca.mat_kv.weight, module.blocks.55.ca.proj.weight, module.blocks.55.ffn.fc1.weight, module.blocks.55.ffn.fc2.weight, '\n"
                   " 'module.blocks.55.ca_norm.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': '[no scale]',
          'params': "('module.cfg_uncond, module.pos_start, module.text_proj_for_sos.ca.mat_q, module.text_proj_for_sos.ca.v_bias, module.text_proj_for_sos.ca.proj.bias, module.text_proj_for_ca.0.bias, '\n"
                    " 'module.text_proj_for_ca.2.bias, module.lvl_embed.weight, module.word_embed.bias, module.shared_ada_lin.1.bias, module.head_nm.ada_lin.1.bias, module.head.bias, module.blocks.0.ada_gss, '\n"
                    " 'module.blocks.0.sa.scale_mul_1H11, module.blocks.0.sa.q_bias, module.blocks.0.sa.v_bias, module.blocks.0.sa.proj.bias, module.blocks.0.ca.v_bias, module.blocks.0.ca.mat_q.bias, '\n"
                    " 'module.blocks.0.ca.proj.bias, module.blocks.0.ffn.fc1.bias, module.blocks.0.ffn.fc2.bias, module.blocks.0.ca_norm.bias, module.blocks.1.ada_gss, module.blocks.1.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.1.sa.q_bias, module.blocks.1.sa.v_bias, module.blocks.1.sa.proj.bias, module.blocks.1.ca.v_bias, module.blocks.1.ca.mat_q.bias, module.blocks.1.ca.proj.bias, '\n"
                    " 'module.blocks.1.ffn.fc1.bias, module.blocks.1.ffn.fc2.bias, module.blocks.1.ca_norm.bias, module.blocks.2.ada_gss, module.blocks.2.sa.scale_mul_1H11, module.blocks.2.sa.q_bias, '\n"
                    " 'module.blocks.2.sa.v_bias, module.blocks.2.sa.proj.bias, module.blocks.2.ca.v_bias, module.blocks.2.ca.mat_q.bias, module.blocks.2.ca.proj.bias, module.blocks.2.ffn.fc1.bias, '\n"
                    " 'module.blocks.2.ffn.fc2.bias, module.blocks.2.ca_norm.bias, module.blocks.3.ada_gss, module.blocks.3.sa.scale_mul_1H11, module.blocks.3.sa.q_bias, module.blocks.3.sa.v_bias, '\n"
                    " 'module.blocks.3.sa.proj.bias, module.blocks.3.ca.v_bias, module.blocks.3.ca.mat_q.bias, module.blocks.3.ca.proj.bias, module.blocks.3.ffn.fc1.bias, module.blocks.3.ffn.fc2.bias, '\n"
                    " 'module.blocks.3.ca_norm.bias, module.blocks.4.ada_gss, module.blocks.4.sa.scale_mul_1H11, module.blocks.4.sa.q_bias, module.blocks.4.sa.v_bias, module.blocks.4.sa.proj.bias, '\n"
                    " 'module.blocks.4.ca.v_bias, module.blocks.4.ca.mat_q.bias, module.blocks.4.ca.proj.bias, module.blocks.4.ffn.fc1.bias, module.blocks.4.ffn.fc2.bias, module.blocks.4.ca_norm.bias, '\n"
                    " 'module.blocks.5.ada_gss, module.blocks.5.sa.scale_mul_1H11, module.blocks.5.sa.q_bias, module.blocks.5.sa.v_bias, module.blocks.5.sa.proj.bias, module.blocks.5.ca.v_bias, '\n"
                    " 'module.blocks.5.ca.mat_q.bias, module.blocks.5.ca.proj.bias, module.blocks.5.ffn.fc1.bias, module.blocks.5.ffn.fc2.bias, module.blocks.5.ca_norm.bias, module.blocks.6.ada_gss, '\n"
                    " 'module.blocks.6.sa.scale_mul_1H11, module.blocks.6.sa.q_bias, module.blocks.6.sa.v_bias, module.blocks.6.sa.proj.bias, module.blocks.6.ca.v_bias, module.blocks.6.ca.mat_q.bias, '\n"
                    " 'module.blocks.6.ca.proj.bias, module.blocks.6.ffn.fc1.bias, module.blocks.6.ffn.fc2.bias, module.blocks.6.ca_norm.bias, module.blocks.7.ada_gss, module.blocks.7.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.7.sa.q_bias, module.blocks.7.sa.v_bias, module.blocks.7.sa.proj.bias, module.blocks.7.ca.v_bias, module.blocks.7.ca.mat_q.bias, module.blocks.7.ca.proj.bias, '\n"
                    " 'module.blocks.7.ffn.fc1.bias, module.blocks.7.ffn.fc2.bias, module.blocks.7.ca_norm.bias, module.blocks.8.ada_gss, module.blocks.8.sa.scale_mul_1H11, module.blocks.8.sa.q_bias, '\n"
                    " 'module.blocks.8.sa.v_bias, module.blocks.8.sa.proj.bias, module.blocks.8.ca.v_bias, module.blocks.8.ca.mat_q.bias, module.blocks.8.ca.proj.bias, module.blocks.8.ffn.fc1.bias, '\n"
                    " 'module.blocks.8.ffn.fc2.bias, module.blocks.8.ca_norm.bias, module.blocks.9.ada_gss, module.blocks.9.sa.scale_mul_1H11, module.blocks.9.sa.q_bias, module.blocks.9.sa.v_bias, '\n"
                    " 'module.blocks.9.sa.proj.bias, module.blocks.9.ca.v_bias, module.blocks.9.ca.mat_q.bias, module.blocks.9.ca.proj.bias, module.blocks.9.ffn.fc1.bias, module.blocks.9.ffn.fc2.bias, '\n"
                    " 'module.blocks.9.ca_norm.bias, module.blocks.10.ada_gss, module.blocks.10.sa.scale_mul_1H11, module.blocks.10.sa.q_bias, module.blocks.10.sa.v_bias, module.blocks.10.sa.proj.bias, '\n"
                    " 'module.blocks.10.ca.v_bias, module.blocks.10.ca.mat_q.bias, module.blocks.10.ca.proj.bias, module.blocks.10.ffn.fc1.bias, module.blocks.10.ffn.fc2.bias, module.blocks.10.ca_norm.bias, '\n"
                    " 'module.blocks.11.ada_gss, module.blocks.11.sa.scale_mul_1H11, module.blocks.11.sa.q_bias, module.blocks.11.sa.v_bias, module.blocks.11.sa.proj.bias, module.blocks.11.ca.v_bias, '\n"
                    " 'module.blocks.11.ca.mat_q.bias, module.blocks.11.ca.proj.bias, module.blocks.11.ffn.fc1.bias, module.blocks.11.ffn.fc2.bias, module.blocks.11.ca_norm.bias, module.blocks.12.ada_gss, '\n"
                    " 'module.blocks.12.sa.scale_mul_1H11, module.blocks.12.sa.q_bias, module.blocks.12.sa.v_bias, module.blocks.12.sa.proj.bias, module.blocks.12.ca.v_bias, module.blocks.12.ca.mat_q.bias, '\n"
                    " 'module.blocks.12.ca.proj.bias, module.blocks.12.ffn.fc1.bias, module.blocks.12.ffn.fc2.bias, module.blocks.12.ca_norm.bias, module.blocks.13.ada_gss, module.blocks.13.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.13.sa.q_bias, module.blocks.13.sa.v_bias, module.blocks.13.sa.proj.bias, module.blocks.13.ca.v_bias, module.blocks.13.ca.mat_q.bias, module.blocks.13.ca.proj.bias, '\n"
                    " 'module.blocks.13.ffn.fc1.bias, module.blocks.13.ffn.fc2.bias, module.blocks.13.ca_norm.bias, module.blocks.14.ada_gss, module.blocks.14.sa.scale_mul_1H11, module.blocks.14.sa.q_bias, '\n"
                    " 'module.blocks.14.sa.v_bias, module.blocks.14.sa.proj.bias, module.blocks.14.ca.v_bias, module.blocks.14.ca.mat_q.bias, module.blocks.14.ca.proj.bias, module.blocks.14.ffn.fc1.bias, '\n"
                    " 'module.blocks.14.ffn.fc2.bias, module.blocks.14.ca_norm.bias, module.blocks.15.ada_gss, module.blocks.15.sa.scale_mul_1H11, module.blocks.15.sa.q_bias, module.blocks.15.sa.v_bias, '\n"
                    " 'module.blocks.15.sa.proj.bias, module.blocks.15.ca.v_bias, module.blocks.15.ca.mat_q.bias, module.blocks.15.ca.proj.bias, module.blocks.15.ffn.fc1.bias, module.blocks.15.ffn.fc2.bias, '\n"
                    " 'module.blocks.15.ca_norm.bias, module.blocks.16.ada_gss, module.blocks.16.sa.scale_mul_1H11, module.blocks.16.sa.q_bias, module.blocks.16.sa.v_bias, module.blocks.16.sa.proj.bias, '\n"
                    " 'module.blocks.16.ca.v_bias, module.blocks.16.ca.mat_q.bias, module.blocks.16.ca.proj.bias, module.blocks.16.ffn.fc1.bias, module.blocks.16.ffn.fc2.bias, module.blocks.16.ca_norm.bias, '\n"
                    " 'module.blocks.17.ada_gss, module.blocks.17.sa.scale_mul_1H11, module.blocks.17.sa.q_bias, module.blocks.17.sa.v_bias, module.blocks.17.sa.proj.bias, module.blocks.17.ca.v_bias, '\n"
                    " 'module.blocks.17.ca.mat_q.bias, module.blocks.17.ca.proj.bias, module.blocks.17.ffn.fc1.bias, module.blocks.17.ffn.fc2.bias, module.blocks.17.ca_norm.bias, module.blocks.18.ada_gss, '\n"
                    " 'module.blocks.18.sa.scale_mul_1H11, module.blocks.18.sa.q_bias, module.blocks.18.sa.v_bias, module.blocks.18.sa.proj.bias, module.blocks.18.ca.v_bias, module.blocks.18.ca.mat_q.bias, '\n"
                    " 'module.blocks.18.ca.proj.bias, module.blocks.18.ffn.fc1.bias, module.blocks.18.ffn.fc2.bias, module.blocks.18.ca_norm.bias, module.blocks.19.ada_gss, module.blocks.19.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.19.sa.q_bias, module.blocks.19.sa.v_bias, module.blocks.19.sa.proj.bias, module.blocks.19.ca.v_bias, module.blocks.19.ca.mat_q.bias, module.blocks.19.ca.proj.bias, '\n"
                    " 'module.blocks.19.ffn.fc1.bias, module.blocks.19.ffn.fc2.bias, module.blocks.19.ca_norm.bias, module.blocks.20.ada_gss, module.blocks.20.sa.scale_mul_1H11, module.blocks.20.sa.q_bias, '\n"
                    " 'module.blocks.20.sa.v_bias, module.blocks.20.sa.proj.bias, module.blocks.20.ca.v_bias, module.blocks.20.ca.mat_q.bias, module.blocks.20.ca.proj.bias, module.blocks.20.ffn.fc1.bias, '\n"
                    " 'module.blocks.20.ffn.fc2.bias, module.blocks.20.ca_norm.bias, module.blocks.21.ada_gss, module.blocks.21.sa.scale_mul_1H11, module.blocks.21.sa.q_bias, module.blocks.21.sa.v_bias, '\n"
                    " 'module.blocks.21.sa.proj.bias, module.blocks.21.ca.v_bias, module.blocks.21.ca.mat_q.bias, module.blocks.21.ca.proj.bias, module.blocks.21.ffn.fc1.bias, module.blocks.21.ffn.fc2.bias, '\n"
                    " 'module.blocks.21.ca_norm.bias, module.blocks.22.ada_gss, module.blocks.22.sa.scale_mul_1H11, module.blocks.22.sa.q_bias, module.blocks.22.sa.v_bias, module.blocks.22.sa.proj.bias, '\n"
                    " 'module.blocks.22.ca.v_bias, module.blocks.22.ca.mat_q.bias, module.blocks.22.ca.proj.bias, module.blocks.22.ffn.fc1.bias, module.blocks.22.ffn.fc2.bias, module.blocks.22.ca_norm.bias, '\n"
                    " 'module.blocks.23.ada_gss, module.blocks.23.sa.scale_mul_1H11, module.blocks.23.sa.q_bias, module.blocks.23.sa.v_bias, module.blocks.23.sa.proj.bias, module.blocks.23.ca.v_bias, '\n"
                    " 'module.blocks.23.ca.mat_q.bias, module.blocks.23.ca.proj.bias, module.blocks.23.ffn.fc1.bias, module.blocks.23.ffn.fc2.bias, module.blocks.23.ca_norm.bias, module.blocks.24.ada_gss, '\n"
                    " 'module.blocks.24.sa.scale_mul_1H11, module.blocks.24.sa.q_bias, module.blocks.24.sa.v_bias, module.blocks.24.sa.proj.bias, module.blocks.24.ca.v_bias, module.blocks.24.ca.mat_q.bias, '\n"
                    " 'module.blocks.24.ca.proj.bias, module.blocks.24.ffn.fc1.bias, module.blocks.24.ffn.fc2.bias, module.blocks.24.ca_norm.bias, module.blocks.25.ada_gss, module.blocks.25.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.25.sa.q_bias, module.blocks.25.sa.v_bias, module.blocks.25.sa.proj.bias, module.blocks.25.ca.v_bias, module.blocks.25.ca.mat_q.bias, module.blocks.25.ca.proj.bias, '\n"
                    " 'module.blocks.25.ffn.fc1.bias, module.blocks.25.ffn.fc2.bias, module.blocks.25.ca_norm.bias, module.blocks.26.ada_gss, module.blocks.26.sa.scale_mul_1H11, module.blocks.26.sa.q_bias, '\n"
                    " 'module.blocks.26.sa.v_bias, module.blocks.26.sa.proj.bias, module.blocks.26.ca.v_bias, module.blocks.26.ca.mat_q.bias, module.blocks.26.ca.proj.bias, module.blocks.26.ffn.fc1.bias, '\n"
                    " 'module.blocks.26.ffn.fc2.bias, module.blocks.26.ca_norm.bias, module.blocks.27.ada_gss, module.blocks.27.sa.scale_mul_1H11, module.blocks.27.sa.q_bias, module.blocks.27.sa.v_bias, '\n"
                    " 'module.blocks.27.sa.proj.bias, module.blocks.27.ca.v_bias, module.blocks.27.ca.mat_q.bias, module.blocks.27.ca.proj.bias, module.blocks.27.ffn.fc1.bias, module.blocks.27.ffn.fc2.bias, '\n"
                    " 'module.blocks.27.ca_norm.bias, module.blocks.28.ada_gss, module.blocks.28.sa.scale_mul_1H11, module.blocks.28.sa.q_bias, module.blocks.28.sa.v_bias, module.blocks.28.sa.proj.bias, '\n"
                    " 'module.blocks.28.ca.v_bias, module.blocks.28.ca.mat_q.bias, module.blocks.28.ca.proj.bias, module.blocks.28.ffn.fc1.bias, module.blocks.28.ffn.fc2.bias, module.blocks.28.ca_norm.bias, '\n"
                    " 'module.blocks.29.ada_gss, module.blocks.29.sa.scale_mul_1H11, module.blocks.29.sa.q_bias, module.blocks.29.sa.v_bias, module.blocks.29.sa.proj.bias, module.blocks.29.ca.v_bias, '\n"
                    " 'module.blocks.29.ca.mat_q.bias, module.blocks.29.ca.proj.bias, module.blocks.29.ffn.fc1.bias, module.blocks.29.ffn.fc2.bias, module.blocks.29.ca_norm.bias, module.blocks.30.ada_gss, '\n"
                    " 'module.blocks.30.sa.scale_mul_1H11, module.blocks.30.sa.q_bias, module.blocks.30.sa.v_bias, module.blocks.30.sa.proj.bias, module.blocks.30.ca.v_bias, module.blocks.30.ca.mat_q.bias, '\n"
                    " 'module.blocks.30.ca.proj.bias, module.blocks.30.ffn.fc1.bias, module.blocks.30.ffn.fc2.bias, module.blocks.30.ca_norm.bias, module.blocks.31.ada_gss, module.blocks.31.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.31.sa.q_bias, module.blocks.31.sa.v_bias, module.blocks.31.sa.proj.bias, module.blocks.31.ca.v_bias, module.blocks.31.ca.mat_q.bias, module.blocks.31.ca.proj.bias, '\n"
                    " 'module.blocks.31.ffn.fc1.bias, module.blocks.31.ffn.fc2.bias, module.blocks.31.ca_norm.bias, module.blocks.32.ada_gss, module.blocks.32.sa.scale_mul_1H11, module.blocks.32.sa.q_bias, '\n"
                    " 'module.blocks.32.sa.v_bias, module.blocks.32.sa.proj.bias, module.blocks.32.ca.v_bias, module.blocks.32.ca.mat_q.bias, module.blocks.32.ca.proj.bias, module.blocks.32.ffn.fc1.bias, '\n"
                    " 'module.blocks.32.ffn.fc2.bias, module.blocks.32.ca_norm.bias, module.blocks.33.ada_gss, module.blocks.33.sa.scale_mul_1H11, module.blocks.33.sa.q_bias, module.blocks.33.sa.v_bias, '\n"
                    " 'module.blocks.33.sa.proj.bias, module.blocks.33.ca.v_bias, module.blocks.33.ca.mat_q.bias, module.blocks.33.ca.proj.bias, module.blocks.33.ffn.fc1.bias, module.blocks.33.ffn.fc2.bias, '\n"
                    " 'module.blocks.33.ca_norm.bias, module.blocks.34.ada_gss, module.blocks.34.sa.scale_mul_1H11, module.blocks.34.sa.q_bias, module.blocks.34.sa.v_bias, module.blocks.34.sa.proj.bias, '\n"
                    " 'module.blocks.34.ca.v_bias, module.blocks.34.ca.mat_q.bias, module.blocks.34.ca.proj.bias, module.blocks.34.ffn.fc1.bias, module.blocks.34.ffn.fc2.bias, module.blocks.34.ca_norm.bias, '\n"
                    " 'module.blocks.35.ada_gss, module.blocks.35.sa.scale_mul_1H11, module.blocks.35.sa.q_bias, module.blocks.35.sa.v_bias, module.blocks.35.sa.proj.bias, module.blocks.35.ca.v_bias, '\n"
                    " 'module.blocks.35.ca.mat_q.bias, module.blocks.35.ca.proj.bias, module.blocks.35.ffn.fc1.bias, module.blocks.35.ffn.fc2.bias, module.blocks.35.ca_norm.bias, module.blocks.36.ada_gss, '\n"
                    " 'module.blocks.36.sa.scale_mul_1H11, module.blocks.36.sa.q_bias, module.blocks.36.sa.v_bias, module.blocks.36.sa.proj.bias, module.blocks.36.ca.v_bias, module.blocks.36.ca.mat_q.bias, '\n"
                    " 'module.blocks.36.ca.proj.bias, module.blocks.36.ffn.fc1.bias, module.blocks.36.ffn.fc2.bias, module.blocks.36.ca_norm.bias, module.blocks.37.ada_gss, module.blocks.37.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.37.sa.q_bias, module.blocks.37.sa.v_bias, module.blocks.37.sa.proj.bias, module.blocks.37.ca.v_bias, module.blocks.37.ca.mat_q.bias, module.blocks.37.ca.proj.bias, '\n"
                    " 'module.blocks.37.ffn.fc1.bias, module.blocks.37.ffn.fc2.bias, module.blocks.37.ca_norm.bias, module.blocks.38.ada_gss, module.blocks.38.sa.scale_mul_1H11, module.blocks.38.sa.q_bias, '\n"
                    " 'module.blocks.38.sa.v_bias, module.blocks.38.sa.proj.bias, module.blocks.38.ca.v_bias, module.blocks.38.ca.mat_q.bias, module.blocks.38.ca.proj.bias, module.blocks.38.ffn.fc1.bias, '\n"
                    " 'module.blocks.38.ffn.fc2.bias, module.blocks.38.ca_norm.bias, module.blocks.39.ada_gss, module.blocks.39.sa.scale_mul_1H11, module.blocks.39.sa.q_bias, module.blocks.39.sa.v_bias, '\n"
                    " 'module.blocks.39.sa.proj.bias, module.blocks.39.ca.v_bias, module.blocks.39.ca.mat_q.bias, module.blocks.39.ca.proj.bias, module.blocks.39.ffn.fc1.bias, module.blocks.39.ffn.fc2.bias, '\n"
                    " 'module.blocks.39.ca_norm.bias, module.blocks.40.ada_gss, module.blocks.40.sa.scale_mul_1H11, module.blocks.40.sa.q_bias, module.blocks.40.sa.v_bias, module.blocks.40.sa.proj.bias, '\n"
                    " 'module.blocks.40.ca.v_bias, module.blocks.40.ca.mat_q.bias, module.blocks.40.ca.proj.bias, module.blocks.40.ffn.fc1.bias, module.blocks.40.ffn.fc2.bias, module.blocks.40.ca_norm.bias, '\n"
                    " 'module.blocks.41.ada_gss, module.blocks.41.sa.scale_mul_1H11, module.blocks.41.sa.q_bias, module.blocks.41.sa.v_bias, module.blocks.41.sa.proj.bias, module.blocks.41.ca.v_bias, '\n"
                    " 'module.blocks.41.ca.mat_q.bias, module.blocks.41.ca.proj.bias, module.blocks.41.ffn.fc1.bias, module.blocks.41.ffn.fc2.bias, module.blocks.41.ca_norm.bias, module.blocks.42.ada_gss, '\n"
                    " 'module.blocks.42.sa.scale_mul_1H11, module.blocks.42.sa.q_bias, module.blocks.42.sa.v_bias, module.blocks.42.sa.proj.bias, module.blocks.42.ca.v_bias, module.blocks.42.ca.mat_q.bias, '\n"
                    " 'module.blocks.42.ca.proj.bias, module.blocks.42.ffn.fc1.bias, module.blocks.42.ffn.fc2.bias, module.blocks.42.ca_norm.bias, module.blocks.43.ada_gss, module.blocks.43.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.43.sa.q_bias, module.blocks.43.sa.v_bias, module.blocks.43.sa.proj.bias, module.blocks.43.ca.v_bias, module.blocks.43.ca.mat_q.bias, module.blocks.43.ca.proj.bias, '\n"
                    " 'module.blocks.43.ffn.fc1.bias, module.blocks.43.ffn.fc2.bias, module.blocks.43.ca_norm.bias, module.blocks.44.ada_gss, module.blocks.44.sa.scale_mul_1H11, module.blocks.44.sa.q_bias, '\n"
                    " 'module.blocks.44.sa.v_bias, module.blocks.44.sa.proj.bias, module.blocks.44.ca.v_bias, module.blocks.44.ca.mat_q.bias, module.blocks.44.ca.proj.bias, module.blocks.44.ffn.fc1.bias, '\n"
                    " 'module.blocks.44.ffn.fc2.bias, module.blocks.44.ca_norm.bias, module.blocks.45.ada_gss, module.blocks.45.sa.scale_mul_1H11, module.blocks.45.sa.q_bias, module.blocks.45.sa.v_bias, '\n"
                    " 'module.blocks.45.sa.proj.bias, module.blocks.45.ca.v_bias, module.blocks.45.ca.mat_q.bias, module.blocks.45.ca.proj.bias, module.blocks.45.ffn.fc1.bias, module.blocks.45.ffn.fc2.bias, '\n"
                    " 'module.blocks.45.ca_norm.bias, module.blocks.46.ada_gss, module.blocks.46.sa.scale_mul_1H11, module.blocks.46.sa.q_bias, module.blocks.46.sa.v_bias, module.blocks.46.sa.proj.bias, '\n"
                    " 'module.blocks.46.ca.v_bias, module.blocks.46.ca.mat_q.bias, module.blocks.46.ca.proj.bias, module.blocks.46.ffn.fc1.bias, module.blocks.46.ffn.fc2.bias, module.blocks.46.ca_norm.bias, '\n"
                    " 'module.blocks.47.ada_gss, module.blocks.47.sa.scale_mul_1H11, module.blocks.47.sa.q_bias, module.blocks.47.sa.v_bias, module.blocks.47.sa.proj.bias, module.blocks.47.ca.v_bias, '\n"
                    " 'module.blocks.47.ca.mat_q.bias, module.blocks.47.ca.proj.bias, module.blocks.47.ffn.fc1.bias, module.blocks.47.ffn.fc2.bias, module.blocks.47.ca_norm.bias, module.blocks.48.ada_gss, '\n"
                    " 'module.blocks.48.sa.scale_mul_1H11, module.blocks.48.sa.q_bias, module.blocks.48.sa.v_bias, module.blocks.48.sa.proj.bias, module.blocks.48.ca.v_bias, module.blocks.48.ca.mat_q.bias, '\n"
                    " 'module.blocks.48.ca.proj.bias, module.blocks.48.ffn.fc1.bias, module.blocks.48.ffn.fc2.bias, module.blocks.48.ca_norm.bias, module.blocks.49.ada_gss, module.blocks.49.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.49.sa.q_bias, module.blocks.49.sa.v_bias, module.blocks.49.sa.proj.bias, module.blocks.49.ca.v_bias, module.blocks.49.ca.mat_q.bias, module.blocks.49.ca.proj.bias, '\n"
                    " 'module.blocks.49.ffn.fc1.bias, module.blocks.49.ffn.fc2.bias, module.blocks.49.ca_norm.bias, module.blocks.50.ada_gss, module.blocks.50.sa.scale_mul_1H11, module.blocks.50.sa.q_bias, '\n"
                    " 'module.blocks.50.sa.v_bias, module.blocks.50.sa.proj.bias, module.blocks.50.ca.v_bias, module.blocks.50.ca.mat_q.bias, module.blocks.50.ca.proj.bias, module.blocks.50.ffn.fc1.bias, '\n"
                    " 'module.blocks.50.ffn.fc2.bias, module.blocks.50.ca_norm.bias, module.blocks.51.ada_gss, module.blocks.51.sa.scale_mul_1H11, module.blocks.51.sa.q_bias, module.blocks.51.sa.v_bias, '\n"
                    " 'module.blocks.51.sa.proj.bias, module.blocks.51.ca.v_bias, module.blocks.51.ca.mat_q.bias, module.blocks.51.ca.proj.bias, module.blocks.51.ffn.fc1.bias, module.blocks.51.ffn.fc2.bias, '\n"
                    " 'module.blocks.51.ca_norm.bias, module.blocks.52.ada_gss, module.blocks.52.sa.scale_mul_1H11, module.blocks.52.sa.q_bias, module.blocks.52.sa.v_bias, module.blocks.52.sa.proj.bias, '\n"
                    " 'module.blocks.52.ca.v_bias, module.blocks.52.ca.mat_q.bias, module.blocks.52.ca.proj.bias, module.blocks.52.ffn.fc1.bias, module.blocks.52.ffn.fc2.bias, module.blocks.52.ca_norm.bias, '\n"
                    " 'module.blocks.53.ada_gss, module.blocks.53.sa.scale_mul_1H11, module.blocks.53.sa.q_bias, module.blocks.53.sa.v_bias, module.blocks.53.sa.proj.bias, module.blocks.53.ca.v_bias, '\n"
                    " 'module.blocks.53.ca.mat_q.bias, module.blocks.53.ca.proj.bias, module.blocks.53.ffn.fc1.bias, module.blocks.53.ffn.fc2.bias, module.blocks.53.ca_norm.bias, module.blocks.54.ada_gss, '\n"
                    " 'module.blocks.54.sa.scale_mul_1H11, module.blocks.54.sa.q_bias, module.blocks.54.sa.v_bias, module.blocks.54.sa.proj.bias, module.blocks.54.ca.v_bias, module.blocks.54.ca.mat_q.bias, '\n"
                    " 'module.blocks.54.ca.proj.bias, module.blocks.54.ffn.fc1.bias, module.blocks.54.ffn.fc2.bias, module.blocks.54.ca_norm.bias, module.blocks.55.ada_gss, module.blocks.55.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.55.sa.q_bias, module.blocks.55.sa.v_bias, module.blocks.55.sa.proj.bias, module.blocks.55.ca.v_bias, module.blocks.55.ca.mat_q.bias, module.blocks.55.ca.proj.bias, '\n"
                    " 'module.blocks.55.ffn.fc1.bias, module.blocks.55.ffn.fc2.bias, module.blocks.55.ca_norm.bias')",
          'wd_sc': 0.0}}

[09-02 00:00:41] (nity/utils/lr_control.py, line 116)=> [get_param_groups][rank0] type(model).__name__='NullDDP' count=1085, numel=8573892016
[09-02 00:00:41] (nity/utils/lr_control.py, line 117)=> 
[09-02 00:00:41] (inity_circuit_breaker.py, line 343)=> [vgpt] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.97), fused=True), opt_kw={'lr': 9.375e-05, 'weight_decay': 0}

[09-02 00:00:41] (inity_circuit_breaker.py, line 348)=> Loading T5 from google/flan-t5-xl...







=======================================================   RESTART [09-06 23:26:11]   =======================================================
[09-06 23:26:11] (inity_circuit_breaker.py, line 718)=> ============================================================
[09-06 23:26:11] (inity_circuit_breaker.py, line 719)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[09-06 23:26:11] (inity_circuit_breaker.py, line 720)=> Circuit Breaker Alpha: 0.1
[09-06 23:26:11] (inity_circuit_breaker.py, line 721)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[09-06 23:26:11] (inity_circuit_breaker.py, line 722)=> Circuit Breaker Enabled: True
[09-06 23:26:11] (inity_circuit_breaker.py, line 723)=> Number of Examples: 1000
[09-06 23:26:11] (inity_circuit_breaker.py, line 724)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[09-06 23:26:11] (inity_circuit_breaker.py, line 725)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[09-06 23:26:11] (inity_circuit_breaker.py, line 726)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[09-06 23:26:11] (inity_circuit_breaker.py, line 727)=> Validation Ratio: 0.1
[09-06 23:26:11] (inity_circuit_breaker.py, line 728)=> Category: hate
[09-06 23:26:11] (inity_circuit_breaker.py, line 729)=> Batch Size: 4
[09-06 23:26:11] (inity_circuit_breaker.py, line 730)=> Workers: 4
[09-06 23:26:11] (inity_circuit_breaker.py, line 731)=> Device: cuda:0
[09-06 23:26:11] (inity_circuit_breaker.py, line 732)=> Model: infinity_8b
[09-06 23:26:11] (inity_circuit_breaker.py, line 733)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-06 23:26:11] (inity_circuit_breaker.py, line 734)=> Rush Resume: 
[09-06 23:26:11] (inity_circuit_breaker.py, line 735)=> Selective Layers: 0,1,2,3,4,5
[09-06 23:26:11] (inity_circuit_breaker.py, line 736)=> ============================================================
[09-06 23:26:11] (inity_circuit_breaker.py, line 377)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[09-06 23:26:11] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-06 23:26:11] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-06 23:26:11] (rain_dataset_infinity.py, line 205)=> Split 1000 prompts: 900 retain, 100 validation
[09-06 23:26:11] (rain_dataset_infinity.py, line 236)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-06 23:26:11] (rain_dataset_infinity.py, line 245)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-06 23:26:12] (rain_dataset_infinity.py, line  84)=> Loaded 900 retain examples
[09-06 23:26:12] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[09-06 23:26:12] (rain_dataset_infinity.py, line  86)=> Loaded 100 validation examples
[09-06 23:26:12] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[09-06 23:26:12] (inity_circuit_breaker.py, line 395)=> args.batch_size=4, vbs=6
[09-06 23:26:12] (inity_circuit_breaker.py, line 418)=> len(dataloader): 100, len(dataset): 100
[09-06 23:26:12] (inity_circuit_breaker.py, line 420)=> total_samples: 200
[09-06 23:26:12] (inity_circuit_breaker.py, line 421)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[09-06 23:26:12] (inity_circuit_breaker.py, line 140)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[09-06 23:26:12] (inity_circuit_breaker.py, line 142)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[09-06 23:26:12] (inity_circuit_breaker.py, line 146)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-06 23:26:14] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[09-06 23:26:14] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[09-06 23:26:14] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[09-06 23:26:15] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[09-06 23:26:17] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[09-06 23:26:19] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[09-06 23:26:19] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[09-06 23:26:19] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[09-06 23:26:19] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[09-06 23:26:19] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

[09-06 23:26:28] (ity/models/init_param.py, line  15)=> [init_weights] Infinity with std=0.02
[09-06 23:26:28] (inity_circuit_breaker.py, line 246)=> [PT] GPT model = Infinity(
  drop_path_rate=0.2
  (norm0_cond): Identity()
  (text_norm): FastRMSNorm(C=2048, eps=1e-06, elementwise_affine=True)
  (text_proj_for_sos): TextAttentivePool(
    (ca): CrossAttention(
      Cq=3072, Ckv=2048, cos_attn=False
      (mat_kv): Linear(in_features=2048, out_features=6144, bias=False)
      (proj): Linear(in_features=3072, out_features=3072, bias=True)
      (proj_drop): Identity()
    )
  )
  (text_proj_for_ca): Sequential(
    (0): Linear(in_features=2048, out_features=3072, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=3072, out_features=3072, bias=True)
  )
  (lvl_embed): Embedding(15, 3072)
  (norm0_ve): Identity()
  (word_embed): Linear(in_features=56, out_features=3072, bias=True)
  (shared_ada_lin): Sequential(
    (0): SiLU()
    (1): SharedAdaLin(in_features=3072, out_features=18432, bias=True)
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=3072, out_features=6144, bias=True)
    )
  )
  (head): Linear(in_features=3072, out_features=112, bias=True)
  (blocks): ModuleList(
    (0): CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): Identity()
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
    (1-55): 55 x CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): DropPath(...)
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
  )
)


[09-06 23:26:28] (inity_circuit_breaker.py, line 248)=> [PT][#para] VAE=81.62, VAE.quant=0.00
[09-06 23:26:28] (inity_circuit_breaker.py, line 251)=> [PT][#para] GPT=8573.89


[09-06 23:26:28] (nity/utils/lr_control.py, line  71)=> [get_param_groups][lr decay] with_lr_scale=False, lr_scale=0.0
[09-06 23:26:28] (nity/utils/lr_control.py, line 111)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': '[no scale]',
         'params': "('module.text_norm.weight, module.text_proj_for_sos.ca.mat_kv.weight, module.text_proj_for_sos.ca.proj.weight, module.text_proj_for_ca.0.weight, module.text_proj_for_ca.2.weight, '\n"
                   " 'module.word_embed.weight, module.shared_ada_lin.1.weight, module.head_nm.ada_lin.1.weight, module.head.weight, module.blocks.0.sa.mat_qkv.weight, module.blocks.0.sa.proj.weight, '\n"
                   " 'module.blocks.0.ca.mat_q.weight, module.blocks.0.ca.mat_kv.weight, module.blocks.0.ca.proj.weight, module.blocks.0.ffn.fc1.weight, module.blocks.0.ffn.fc2.weight, module.blocks.0.ca_norm.weight, '\n"
                   " 'module.blocks.1.sa.mat_qkv.weight, module.blocks.1.sa.proj.weight, module.blocks.1.ca.mat_q.weight, module.blocks.1.ca.mat_kv.weight, module.blocks.1.ca.proj.weight, '\n"
                   " 'module.blocks.1.ffn.fc1.weight, module.blocks.1.ffn.fc2.weight, module.blocks.1.ca_norm.weight, module.blocks.2.sa.mat_qkv.weight, module.blocks.2.sa.proj.weight, module.blocks.2.ca.mat_q.weight, '\n"
                   " 'module.blocks.2.ca.mat_kv.weight, module.blocks.2.ca.proj.weight, module.blocks.2.ffn.fc1.weight, module.blocks.2.ffn.fc2.weight, module.blocks.2.ca_norm.weight, module.blocks.3.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.3.sa.proj.weight, module.blocks.3.ca.mat_q.weight, module.blocks.3.ca.mat_kv.weight, module.blocks.3.ca.proj.weight, module.blocks.3.ffn.fc1.weight, module.blocks.3.ffn.fc2.weight, '\n"
                   " 'module.blocks.3.ca_norm.weight, module.blocks.4.sa.mat_qkv.weight, module.blocks.4.sa.proj.weight, module.blocks.4.ca.mat_q.weight, module.blocks.4.ca.mat_kv.weight, '\n"
                   " 'module.blocks.4.ca.proj.weight, module.blocks.4.ffn.fc1.weight, module.blocks.4.ffn.fc2.weight, module.blocks.4.ca_norm.weight, module.blocks.5.sa.mat_qkv.weight, module.blocks.5.sa.proj.weight, '\n"
                   " 'module.blocks.5.ca.mat_q.weight, module.blocks.5.ca.mat_kv.weight, module.blocks.5.ca.proj.weight, module.blocks.5.ffn.fc1.weight, module.blocks.5.ffn.fc2.weight, module.blocks.5.ca_norm.weight, '\n"
                   " 'module.blocks.6.sa.mat_qkv.weight, module.blocks.6.sa.proj.weight, module.blocks.6.ca.mat_q.weight, module.blocks.6.ca.mat_kv.weight, module.blocks.6.ca.proj.weight, '\n"
                   " 'module.blocks.6.ffn.fc1.weight, module.blocks.6.ffn.fc2.weight, module.blocks.6.ca_norm.weight, module.blocks.7.sa.mat_qkv.weight, module.blocks.7.sa.proj.weight, module.blocks.7.ca.mat_q.weight, '\n"
                   " 'module.blocks.7.ca.mat_kv.weight, module.blocks.7.ca.proj.weight, module.blocks.7.ffn.fc1.weight, module.blocks.7.ffn.fc2.weight, module.blocks.7.ca_norm.weight, module.blocks.8.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.8.sa.proj.weight, module.blocks.8.ca.mat_q.weight, module.blocks.8.ca.mat_kv.weight, module.blocks.8.ca.proj.weight, module.blocks.8.ffn.fc1.weight, module.blocks.8.ffn.fc2.weight, '\n"
                   " 'module.blocks.8.ca_norm.weight, module.blocks.9.sa.mat_qkv.weight, module.blocks.9.sa.proj.weight, module.blocks.9.ca.mat_q.weight, module.blocks.9.ca.mat_kv.weight, '\n"
                   " 'module.blocks.9.ca.proj.weight, module.blocks.9.ffn.fc1.weight, module.blocks.9.ffn.fc2.weight, module.blocks.9.ca_norm.weight, module.blocks.10.sa.mat_qkv.weight, module.blocks.10.sa.proj.weight, '\n"
                   " 'module.blocks.10.ca.mat_q.weight, module.blocks.10.ca.mat_kv.weight, module.blocks.10.ca.proj.weight, module.blocks.10.ffn.fc1.weight, module.blocks.10.ffn.fc2.weight, '\n"
                   " 'module.blocks.10.ca_norm.weight, module.blocks.11.sa.mat_qkv.weight, module.blocks.11.sa.proj.weight, module.blocks.11.ca.mat_q.weight, module.blocks.11.ca.mat_kv.weight, '\n"
                   " 'module.blocks.11.ca.proj.weight, module.blocks.11.ffn.fc1.weight, module.blocks.11.ffn.fc2.weight, module.blocks.11.ca_norm.weight, module.blocks.12.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.12.sa.proj.weight, module.blocks.12.ca.mat_q.weight, module.blocks.12.ca.mat_kv.weight, module.blocks.12.ca.proj.weight, module.blocks.12.ffn.fc1.weight, '\n"
                   " 'module.blocks.12.ffn.fc2.weight, module.blocks.12.ca_norm.weight, module.blocks.13.sa.mat_qkv.weight, module.blocks.13.sa.proj.weight, module.blocks.13.ca.mat_q.weight, '\n"
                   " 'module.blocks.13.ca.mat_kv.weight, module.blocks.13.ca.proj.weight, module.blocks.13.ffn.fc1.weight, module.blocks.13.ffn.fc2.weight, module.blocks.13.ca_norm.weight, '\n"
                   " 'module.blocks.14.sa.mat_qkv.weight, module.blocks.14.sa.proj.weight, module.blocks.14.ca.mat_q.weight, module.blocks.14.ca.mat_kv.weight, module.blocks.14.ca.proj.weight, '\n"
                   " 'module.blocks.14.ffn.fc1.weight, module.blocks.14.ffn.fc2.weight, module.blocks.14.ca_norm.weight, module.blocks.15.sa.mat_qkv.weight, module.blocks.15.sa.proj.weight, '\n"
                   " 'module.blocks.15.ca.mat_q.weight, module.blocks.15.ca.mat_kv.weight, module.blocks.15.ca.proj.weight, module.blocks.15.ffn.fc1.weight, module.blocks.15.ffn.fc2.weight, '\n"
                   " 'module.blocks.15.ca_norm.weight, module.blocks.16.sa.mat_qkv.weight, module.blocks.16.sa.proj.weight, module.blocks.16.ca.mat_q.weight, module.blocks.16.ca.mat_kv.weight, '\n"
                   " 'module.blocks.16.ca.proj.weight, module.blocks.16.ffn.fc1.weight, module.blocks.16.ffn.fc2.weight, module.blocks.16.ca_norm.weight, module.blocks.17.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.17.sa.proj.weight, module.blocks.17.ca.mat_q.weight, module.blocks.17.ca.mat_kv.weight, module.blocks.17.ca.proj.weight, module.blocks.17.ffn.fc1.weight, '\n"
                   " 'module.blocks.17.ffn.fc2.weight, module.blocks.17.ca_norm.weight, module.blocks.18.sa.mat_qkv.weight, module.blocks.18.sa.proj.weight, module.blocks.18.ca.mat_q.weight, '\n"
                   " 'module.blocks.18.ca.mat_kv.weight, module.blocks.18.ca.proj.weight, module.blocks.18.ffn.fc1.weight, module.blocks.18.ffn.fc2.weight, module.blocks.18.ca_norm.weight, '\n"
                   " 'module.blocks.19.sa.mat_qkv.weight, module.blocks.19.sa.proj.weight, module.blocks.19.ca.mat_q.weight, module.blocks.19.ca.mat_kv.weight, module.blocks.19.ca.proj.weight, '\n"
                   " 'module.blocks.19.ffn.fc1.weight, module.blocks.19.ffn.fc2.weight, module.blocks.19.ca_norm.weight, module.blocks.20.sa.mat_qkv.weight, module.blocks.20.sa.proj.weight, '\n"
                   " 'module.blocks.20.ca.mat_q.weight, module.blocks.20.ca.mat_kv.weight, module.blocks.20.ca.proj.weight, module.blocks.20.ffn.fc1.weight, module.blocks.20.ffn.fc2.weight, '\n"
                   " 'module.blocks.20.ca_norm.weight, module.blocks.21.sa.mat_qkv.weight, module.blocks.21.sa.proj.weight, module.blocks.21.ca.mat_q.weight, module.blocks.21.ca.mat_kv.weight, '\n"
                   " 'module.blocks.21.ca.proj.weight, module.blocks.21.ffn.fc1.weight, module.blocks.21.ffn.fc2.weight, module.blocks.21.ca_norm.weight, module.blocks.22.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.22.sa.proj.weight, module.blocks.22.ca.mat_q.weight, module.blocks.22.ca.mat_kv.weight, module.blocks.22.ca.proj.weight, module.blocks.22.ffn.fc1.weight, '\n"
                   " 'module.blocks.22.ffn.fc2.weight, module.blocks.22.ca_norm.weight, module.blocks.23.sa.mat_qkv.weight, module.blocks.23.sa.proj.weight, module.blocks.23.ca.mat_q.weight, '\n"
                   " 'module.blocks.23.ca.mat_kv.weight, module.blocks.23.ca.proj.weight, module.blocks.23.ffn.fc1.weight, module.blocks.23.ffn.fc2.weight, module.blocks.23.ca_norm.weight, '\n"
                   " 'module.blocks.24.sa.mat_qkv.weight, module.blocks.24.sa.proj.weight, module.blocks.24.ca.mat_q.weight, module.blocks.24.ca.mat_kv.weight, module.blocks.24.ca.proj.weight, '\n"
                   " 'module.blocks.24.ffn.fc1.weight, module.blocks.24.ffn.fc2.weight, module.blocks.24.ca_norm.weight, module.blocks.25.sa.mat_qkv.weight, module.blocks.25.sa.proj.weight, '\n"
                   " 'module.blocks.25.ca.mat_q.weight, module.blocks.25.ca.mat_kv.weight, module.blocks.25.ca.proj.weight, module.blocks.25.ffn.fc1.weight, module.blocks.25.ffn.fc2.weight, '\n"
                   " 'module.blocks.25.ca_norm.weight, module.blocks.26.sa.mat_qkv.weight, module.blocks.26.sa.proj.weight, module.blocks.26.ca.mat_q.weight, module.blocks.26.ca.mat_kv.weight, '\n"
                   " 'module.blocks.26.ca.proj.weight, module.blocks.26.ffn.fc1.weight, module.blocks.26.ffn.fc2.weight, module.blocks.26.ca_norm.weight, module.blocks.27.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.27.sa.proj.weight, module.blocks.27.ca.mat_q.weight, module.blocks.27.ca.mat_kv.weight, module.blocks.27.ca.proj.weight, module.blocks.27.ffn.fc1.weight, '\n"
                   " 'module.blocks.27.ffn.fc2.weight, module.blocks.27.ca_norm.weight, module.blocks.28.sa.mat_qkv.weight, module.blocks.28.sa.proj.weight, module.blocks.28.ca.mat_q.weight, '\n"
                   " 'module.blocks.28.ca.mat_kv.weight, module.blocks.28.ca.proj.weight, module.blocks.28.ffn.fc1.weight, module.blocks.28.ffn.fc2.weight, module.blocks.28.ca_norm.weight, '\n"
                   " 'module.blocks.29.sa.mat_qkv.weight, module.blocks.29.sa.proj.weight, module.blocks.29.ca.mat_q.weight, module.blocks.29.ca.mat_kv.weight, module.blocks.29.ca.proj.weight, '\n"
                   " 'module.blocks.29.ffn.fc1.weight, module.blocks.29.ffn.fc2.weight, module.blocks.29.ca_norm.weight, module.blocks.30.sa.mat_qkv.weight, module.blocks.30.sa.proj.weight, '\n"
                   " 'module.blocks.30.ca.mat_q.weight, module.blocks.30.ca.mat_kv.weight, module.blocks.30.ca.proj.weight, module.blocks.30.ffn.fc1.weight, module.blocks.30.ffn.fc2.weight, '\n"
                   " 'module.blocks.30.ca_norm.weight, module.blocks.31.sa.mat_qkv.weight, module.blocks.31.sa.proj.weight, module.blocks.31.ca.mat_q.weight, module.blocks.31.ca.mat_kv.weight, '\n"
                   " 'module.blocks.31.ca.proj.weight, module.blocks.31.ffn.fc1.weight, module.blocks.31.ffn.fc2.weight, module.blocks.31.ca_norm.weight, module.blocks.32.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.32.sa.proj.weight, module.blocks.32.ca.mat_q.weight, module.blocks.32.ca.mat_kv.weight, module.blocks.32.ca.proj.weight, module.blocks.32.ffn.fc1.weight, '\n"
                   " 'module.blocks.32.ffn.fc2.weight, module.blocks.32.ca_norm.weight, module.blocks.33.sa.mat_qkv.weight, module.blocks.33.sa.proj.weight, module.blocks.33.ca.mat_q.weight, '\n"
                   " 'module.blocks.33.ca.mat_kv.weight, module.blocks.33.ca.proj.weight, module.blocks.33.ffn.fc1.weight, module.blocks.33.ffn.fc2.weight, module.blocks.33.ca_norm.weight, '\n"
                   " 'module.blocks.34.sa.mat_qkv.weight, module.blocks.34.sa.proj.weight, module.blocks.34.ca.mat_q.weight, module.blocks.34.ca.mat_kv.weight, module.blocks.34.ca.proj.weight, '\n"
                   " 'module.blocks.34.ffn.fc1.weight, module.blocks.34.ffn.fc2.weight, module.blocks.34.ca_norm.weight, module.blocks.35.sa.mat_qkv.weight, module.blocks.35.sa.proj.weight, '\n"
                   " 'module.blocks.35.ca.mat_q.weight, module.blocks.35.ca.mat_kv.weight, module.blocks.35.ca.proj.weight, module.blocks.35.ffn.fc1.weight, module.blocks.35.ffn.fc2.weight, '\n"
                   " 'module.blocks.35.ca_norm.weight, module.blocks.36.sa.mat_qkv.weight, module.blocks.36.sa.proj.weight, module.blocks.36.ca.mat_q.weight, module.blocks.36.ca.mat_kv.weight, '\n"
                   " 'module.blocks.36.ca.proj.weight, module.blocks.36.ffn.fc1.weight, module.blocks.36.ffn.fc2.weight, module.blocks.36.ca_norm.weight, module.blocks.37.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.37.sa.proj.weight, module.blocks.37.ca.mat_q.weight, module.blocks.37.ca.mat_kv.weight, module.blocks.37.ca.proj.weight, module.blocks.37.ffn.fc1.weight, '\n"
                   " 'module.blocks.37.ffn.fc2.weight, module.blocks.37.ca_norm.weight, module.blocks.38.sa.mat_qkv.weight, module.blocks.38.sa.proj.weight, module.blocks.38.ca.mat_q.weight, '\n"
                   " 'module.blocks.38.ca.mat_kv.weight, module.blocks.38.ca.proj.weight, module.blocks.38.ffn.fc1.weight, module.blocks.38.ffn.fc2.weight, module.blocks.38.ca_norm.weight, '\n"
                   " 'module.blocks.39.sa.mat_qkv.weight, module.blocks.39.sa.proj.weight, module.blocks.39.ca.mat_q.weight, module.blocks.39.ca.mat_kv.weight, module.blocks.39.ca.proj.weight, '\n"
                   " 'module.blocks.39.ffn.fc1.weight, module.blocks.39.ffn.fc2.weight, module.blocks.39.ca_norm.weight, module.blocks.40.sa.mat_qkv.weight, module.blocks.40.sa.proj.weight, '\n"
                   " 'module.blocks.40.ca.mat_q.weight, module.blocks.40.ca.mat_kv.weight, module.blocks.40.ca.proj.weight, module.blocks.40.ffn.fc1.weight, module.blocks.40.ffn.fc2.weight, '\n"
                   " 'module.blocks.40.ca_norm.weight, module.blocks.41.sa.mat_qkv.weight, module.blocks.41.sa.proj.weight, module.blocks.41.ca.mat_q.weight, module.blocks.41.ca.mat_kv.weight, '\n"
                   " 'module.blocks.41.ca.proj.weight, module.blocks.41.ffn.fc1.weight, module.blocks.41.ffn.fc2.weight, module.blocks.41.ca_norm.weight, module.blocks.42.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.42.sa.proj.weight, module.blocks.42.ca.mat_q.weight, module.blocks.42.ca.mat_kv.weight, module.blocks.42.ca.proj.weight, module.blocks.42.ffn.fc1.weight, '\n"
                   " 'module.blocks.42.ffn.fc2.weight, module.blocks.42.ca_norm.weight, module.blocks.43.sa.mat_qkv.weight, module.blocks.43.sa.proj.weight, module.blocks.43.ca.mat_q.weight, '\n"
                   " 'module.blocks.43.ca.mat_kv.weight, module.blocks.43.ca.proj.weight, module.blocks.43.ffn.fc1.weight, module.blocks.43.ffn.fc2.weight, module.blocks.43.ca_norm.weight, '\n"
                   " 'module.blocks.44.sa.mat_qkv.weight, module.blocks.44.sa.proj.weight, module.blocks.44.ca.mat_q.weight, module.blocks.44.ca.mat_kv.weight, module.blocks.44.ca.proj.weight, '\n"
                   " 'module.blocks.44.ffn.fc1.weight, module.blocks.44.ffn.fc2.weight, module.blocks.44.ca_norm.weight, module.blocks.45.sa.mat_qkv.weight, module.blocks.45.sa.proj.weight, '\n"
                   " 'module.blocks.45.ca.mat_q.weight, module.blocks.45.ca.mat_kv.weight, module.blocks.45.ca.proj.weight, module.blocks.45.ffn.fc1.weight, module.blocks.45.ffn.fc2.weight, '\n"
                   " 'module.blocks.45.ca_norm.weight, module.blocks.46.sa.mat_qkv.weight, module.blocks.46.sa.proj.weight, module.blocks.46.ca.mat_q.weight, module.blocks.46.ca.mat_kv.weight, '\n"
                   " 'module.blocks.46.ca.proj.weight, module.blocks.46.ffn.fc1.weight, module.blocks.46.ffn.fc2.weight, module.blocks.46.ca_norm.weight, module.blocks.47.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.47.sa.proj.weight, module.blocks.47.ca.mat_q.weight, module.blocks.47.ca.mat_kv.weight, module.blocks.47.ca.proj.weight, module.blocks.47.ffn.fc1.weight, '\n"
                   " 'module.blocks.47.ffn.fc2.weight, module.blocks.47.ca_norm.weight, module.blocks.48.sa.mat_qkv.weight, module.blocks.48.sa.proj.weight, module.blocks.48.ca.mat_q.weight, '\n"
                   " 'module.blocks.48.ca.mat_kv.weight, module.blocks.48.ca.proj.weight, module.blocks.48.ffn.fc1.weight, module.blocks.48.ffn.fc2.weight, module.blocks.48.ca_norm.weight, '\n"
                   " 'module.blocks.49.sa.mat_qkv.weight, module.blocks.49.sa.proj.weight, module.blocks.49.ca.mat_q.weight, module.blocks.49.ca.mat_kv.weight, module.blocks.49.ca.proj.weight, '\n"
                   " 'module.blocks.49.ffn.fc1.weight, module.blocks.49.ffn.fc2.weight, module.blocks.49.ca_norm.weight, module.blocks.50.sa.mat_qkv.weight, module.blocks.50.sa.proj.weight, '\n"
                   " 'module.blocks.50.ca.mat_q.weight, module.blocks.50.ca.mat_kv.weight, module.blocks.50.ca.proj.weight, module.blocks.50.ffn.fc1.weight, module.blocks.50.ffn.fc2.weight, '\n"
                   " 'module.blocks.50.ca_norm.weight, module.blocks.51.sa.mat_qkv.weight, module.blocks.51.sa.proj.weight, module.blocks.51.ca.mat_q.weight, module.blocks.51.ca.mat_kv.weight, '\n"
                   " 'module.blocks.51.ca.proj.weight, module.blocks.51.ffn.fc1.weight, module.blocks.51.ffn.fc2.weight, module.blocks.51.ca_norm.weight, module.blocks.52.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.52.sa.proj.weight, module.blocks.52.ca.mat_q.weight, module.blocks.52.ca.mat_kv.weight, module.blocks.52.ca.proj.weight, module.blocks.52.ffn.fc1.weight, '\n"
                   " 'module.blocks.52.ffn.fc2.weight, module.blocks.52.ca_norm.weight, module.blocks.53.sa.mat_qkv.weight, module.blocks.53.sa.proj.weight, module.blocks.53.ca.mat_q.weight, '\n"
                   " 'module.blocks.53.ca.mat_kv.weight, module.blocks.53.ca.proj.weight, module.blocks.53.ffn.fc1.weight, module.blocks.53.ffn.fc2.weight, module.blocks.53.ca_norm.weight, '\n"
                   " 'module.blocks.54.sa.mat_qkv.weight, module.blocks.54.sa.proj.weight, module.blocks.54.ca.mat_q.weight, module.blocks.54.ca.mat_kv.weight, module.blocks.54.ca.proj.weight, '\n"
                   " 'module.blocks.54.ffn.fc1.weight, module.blocks.54.ffn.fc2.weight, module.blocks.54.ca_norm.weight, module.blocks.55.sa.mat_qkv.weight, module.blocks.55.sa.proj.weight, '\n"
                   " 'module.blocks.55.ca.mat_q.weight, module.blocks.55.ca.mat_kv.weight, module.blocks.55.ca.proj.weight, module.blocks.55.ffn.fc1.weight, module.blocks.55.ffn.fc2.weight, '\n"
                   " 'module.blocks.55.ca_norm.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': '[no scale]',
          'params': "('module.cfg_uncond, module.pos_start, module.text_proj_for_sos.ca.mat_q, module.text_proj_for_sos.ca.v_bias, module.text_proj_for_sos.ca.proj.bias, module.text_proj_for_ca.0.bias, '\n"
                    " 'module.text_proj_for_ca.2.bias, module.lvl_embed.weight, module.word_embed.bias, module.shared_ada_lin.1.bias, module.head_nm.ada_lin.1.bias, module.head.bias, module.blocks.0.ada_gss, '\n"
                    " 'module.blocks.0.sa.scale_mul_1H11, module.blocks.0.sa.q_bias, module.blocks.0.sa.v_bias, module.blocks.0.sa.proj.bias, module.blocks.0.ca.v_bias, module.blocks.0.ca.mat_q.bias, '\n"
                    " 'module.blocks.0.ca.proj.bias, module.blocks.0.ffn.fc1.bias, module.blocks.0.ffn.fc2.bias, module.blocks.0.ca_norm.bias, module.blocks.1.ada_gss, module.blocks.1.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.1.sa.q_bias, module.blocks.1.sa.v_bias, module.blocks.1.sa.proj.bias, module.blocks.1.ca.v_bias, module.blocks.1.ca.mat_q.bias, module.blocks.1.ca.proj.bias, '\n"
                    " 'module.blocks.1.ffn.fc1.bias, module.blocks.1.ffn.fc2.bias, module.blocks.1.ca_norm.bias, module.blocks.2.ada_gss, module.blocks.2.sa.scale_mul_1H11, module.blocks.2.sa.q_bias, '\n"
                    " 'module.blocks.2.sa.v_bias, module.blocks.2.sa.proj.bias, module.blocks.2.ca.v_bias, module.blocks.2.ca.mat_q.bias, module.blocks.2.ca.proj.bias, module.blocks.2.ffn.fc1.bias, '\n"
                    " 'module.blocks.2.ffn.fc2.bias, module.blocks.2.ca_norm.bias, module.blocks.3.ada_gss, module.blocks.3.sa.scale_mul_1H11, module.blocks.3.sa.q_bias, module.blocks.3.sa.v_bias, '\n"
                    " 'module.blocks.3.sa.proj.bias, module.blocks.3.ca.v_bias, module.blocks.3.ca.mat_q.bias, module.blocks.3.ca.proj.bias, module.blocks.3.ffn.fc1.bias, module.blocks.3.ffn.fc2.bias, '\n"
                    " 'module.blocks.3.ca_norm.bias, module.blocks.4.ada_gss, module.blocks.4.sa.scale_mul_1H11, module.blocks.4.sa.q_bias, module.blocks.4.sa.v_bias, module.blocks.4.sa.proj.bias, '\n"
                    " 'module.blocks.4.ca.v_bias, module.blocks.4.ca.mat_q.bias, module.blocks.4.ca.proj.bias, module.blocks.4.ffn.fc1.bias, module.blocks.4.ffn.fc2.bias, module.blocks.4.ca_norm.bias, '\n"
                    " 'module.blocks.5.ada_gss, module.blocks.5.sa.scale_mul_1H11, module.blocks.5.sa.q_bias, module.blocks.5.sa.v_bias, module.blocks.5.sa.proj.bias, module.blocks.5.ca.v_bias, '\n"
                    " 'module.blocks.5.ca.mat_q.bias, module.blocks.5.ca.proj.bias, module.blocks.5.ffn.fc1.bias, module.blocks.5.ffn.fc2.bias, module.blocks.5.ca_norm.bias, module.blocks.6.ada_gss, '\n"
                    " 'module.blocks.6.sa.scale_mul_1H11, module.blocks.6.sa.q_bias, module.blocks.6.sa.v_bias, module.blocks.6.sa.proj.bias, module.blocks.6.ca.v_bias, module.blocks.6.ca.mat_q.bias, '\n"
                    " 'module.blocks.6.ca.proj.bias, module.blocks.6.ffn.fc1.bias, module.blocks.6.ffn.fc2.bias, module.blocks.6.ca_norm.bias, module.blocks.7.ada_gss, module.blocks.7.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.7.sa.q_bias, module.blocks.7.sa.v_bias, module.blocks.7.sa.proj.bias, module.blocks.7.ca.v_bias, module.blocks.7.ca.mat_q.bias, module.blocks.7.ca.proj.bias, '\n"
                    " 'module.blocks.7.ffn.fc1.bias, module.blocks.7.ffn.fc2.bias, module.blocks.7.ca_norm.bias, module.blocks.8.ada_gss, module.blocks.8.sa.scale_mul_1H11, module.blocks.8.sa.q_bias, '\n"
                    " 'module.blocks.8.sa.v_bias, module.blocks.8.sa.proj.bias, module.blocks.8.ca.v_bias, module.blocks.8.ca.mat_q.bias, module.blocks.8.ca.proj.bias, module.blocks.8.ffn.fc1.bias, '\n"
                    " 'module.blocks.8.ffn.fc2.bias, module.blocks.8.ca_norm.bias, module.blocks.9.ada_gss, module.blocks.9.sa.scale_mul_1H11, module.blocks.9.sa.q_bias, module.blocks.9.sa.v_bias, '\n"
                    " 'module.blocks.9.sa.proj.bias, module.blocks.9.ca.v_bias, module.blocks.9.ca.mat_q.bias, module.blocks.9.ca.proj.bias, module.blocks.9.ffn.fc1.bias, module.blocks.9.ffn.fc2.bias, '\n"
                    " 'module.blocks.9.ca_norm.bias, module.blocks.10.ada_gss, module.blocks.10.sa.scale_mul_1H11, module.blocks.10.sa.q_bias, module.blocks.10.sa.v_bias, module.blocks.10.sa.proj.bias, '\n"
                    " 'module.blocks.10.ca.v_bias, module.blocks.10.ca.mat_q.bias, module.blocks.10.ca.proj.bias, module.blocks.10.ffn.fc1.bias, module.blocks.10.ffn.fc2.bias, module.blocks.10.ca_norm.bias, '\n"
                    " 'module.blocks.11.ada_gss, module.blocks.11.sa.scale_mul_1H11, module.blocks.11.sa.q_bias, module.blocks.11.sa.v_bias, module.blocks.11.sa.proj.bias, module.blocks.11.ca.v_bias, '\n"
                    " 'module.blocks.11.ca.mat_q.bias, module.blocks.11.ca.proj.bias, module.blocks.11.ffn.fc1.bias, module.blocks.11.ffn.fc2.bias, module.blocks.11.ca_norm.bias, module.blocks.12.ada_gss, '\n"
                    " 'module.blocks.12.sa.scale_mul_1H11, module.blocks.12.sa.q_bias, module.blocks.12.sa.v_bias, module.blocks.12.sa.proj.bias, module.blocks.12.ca.v_bias, module.blocks.12.ca.mat_q.bias, '\n"
                    " 'module.blocks.12.ca.proj.bias, module.blocks.12.ffn.fc1.bias, module.blocks.12.ffn.fc2.bias, module.blocks.12.ca_norm.bias, module.blocks.13.ada_gss, module.blocks.13.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.13.sa.q_bias, module.blocks.13.sa.v_bias, module.blocks.13.sa.proj.bias, module.blocks.13.ca.v_bias, module.blocks.13.ca.mat_q.bias, module.blocks.13.ca.proj.bias, '\n"
                    " 'module.blocks.13.ffn.fc1.bias, module.blocks.13.ffn.fc2.bias, module.blocks.13.ca_norm.bias, module.blocks.14.ada_gss, module.blocks.14.sa.scale_mul_1H11, module.blocks.14.sa.q_bias, '\n"
                    " 'module.blocks.14.sa.v_bias, module.blocks.14.sa.proj.bias, module.blocks.14.ca.v_bias, module.blocks.14.ca.mat_q.bias, module.blocks.14.ca.proj.bias, module.blocks.14.ffn.fc1.bias, '\n"
                    " 'module.blocks.14.ffn.fc2.bias, module.blocks.14.ca_norm.bias, module.blocks.15.ada_gss, module.blocks.15.sa.scale_mul_1H11, module.blocks.15.sa.q_bias, module.blocks.15.sa.v_bias, '\n"
                    " 'module.blocks.15.sa.proj.bias, module.blocks.15.ca.v_bias, module.blocks.15.ca.mat_q.bias, module.blocks.15.ca.proj.bias, module.blocks.15.ffn.fc1.bias, module.blocks.15.ffn.fc2.bias, '\n"
                    " 'module.blocks.15.ca_norm.bias, module.blocks.16.ada_gss, module.blocks.16.sa.scale_mul_1H11, module.blocks.16.sa.q_bias, module.blocks.16.sa.v_bias, module.blocks.16.sa.proj.bias, '\n"
                    " 'module.blocks.16.ca.v_bias, module.blocks.16.ca.mat_q.bias, module.blocks.16.ca.proj.bias, module.blocks.16.ffn.fc1.bias, module.blocks.16.ffn.fc2.bias, module.blocks.16.ca_norm.bias, '\n"
                    " 'module.blocks.17.ada_gss, module.blocks.17.sa.scale_mul_1H11, module.blocks.17.sa.q_bias, module.blocks.17.sa.v_bias, module.blocks.17.sa.proj.bias, module.blocks.17.ca.v_bias, '\n"
                    " 'module.blocks.17.ca.mat_q.bias, module.blocks.17.ca.proj.bias, module.blocks.17.ffn.fc1.bias, module.blocks.17.ffn.fc2.bias, module.blocks.17.ca_norm.bias, module.blocks.18.ada_gss, '\n"
                    " 'module.blocks.18.sa.scale_mul_1H11, module.blocks.18.sa.q_bias, module.blocks.18.sa.v_bias, module.blocks.18.sa.proj.bias, module.blocks.18.ca.v_bias, module.blocks.18.ca.mat_q.bias, '\n"
                    " 'module.blocks.18.ca.proj.bias, module.blocks.18.ffn.fc1.bias, module.blocks.18.ffn.fc2.bias, module.blocks.18.ca_norm.bias, module.blocks.19.ada_gss, module.blocks.19.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.19.sa.q_bias, module.blocks.19.sa.v_bias, module.blocks.19.sa.proj.bias, module.blocks.19.ca.v_bias, module.blocks.19.ca.mat_q.bias, module.blocks.19.ca.proj.bias, '\n"
                    " 'module.blocks.19.ffn.fc1.bias, module.blocks.19.ffn.fc2.bias, module.blocks.19.ca_norm.bias, module.blocks.20.ada_gss, module.blocks.20.sa.scale_mul_1H11, module.blocks.20.sa.q_bias, '\n"
                    " 'module.blocks.20.sa.v_bias, module.blocks.20.sa.proj.bias, module.blocks.20.ca.v_bias, module.blocks.20.ca.mat_q.bias, module.blocks.20.ca.proj.bias, module.blocks.20.ffn.fc1.bias, '\n"
                    " 'module.blocks.20.ffn.fc2.bias, module.blocks.20.ca_norm.bias, module.blocks.21.ada_gss, module.blocks.21.sa.scale_mul_1H11, module.blocks.21.sa.q_bias, module.blocks.21.sa.v_bias, '\n"
                    " 'module.blocks.21.sa.proj.bias, module.blocks.21.ca.v_bias, module.blocks.21.ca.mat_q.bias, module.blocks.21.ca.proj.bias, module.blocks.21.ffn.fc1.bias, module.blocks.21.ffn.fc2.bias, '\n"
                    " 'module.blocks.21.ca_norm.bias, module.blocks.22.ada_gss, module.blocks.22.sa.scale_mul_1H11, module.blocks.22.sa.q_bias, module.blocks.22.sa.v_bias, module.blocks.22.sa.proj.bias, '\n"
                    " 'module.blocks.22.ca.v_bias, module.blocks.22.ca.mat_q.bias, module.blocks.22.ca.proj.bias, module.blocks.22.ffn.fc1.bias, module.blocks.22.ffn.fc2.bias, module.blocks.22.ca_norm.bias, '\n"
                    " 'module.blocks.23.ada_gss, module.blocks.23.sa.scale_mul_1H11, module.blocks.23.sa.q_bias, module.blocks.23.sa.v_bias, module.blocks.23.sa.proj.bias, module.blocks.23.ca.v_bias, '\n"
                    " 'module.blocks.23.ca.mat_q.bias, module.blocks.23.ca.proj.bias, module.blocks.23.ffn.fc1.bias, module.blocks.23.ffn.fc2.bias, module.blocks.23.ca_norm.bias, module.blocks.24.ada_gss, '\n"
                    " 'module.blocks.24.sa.scale_mul_1H11, module.blocks.24.sa.q_bias, module.blocks.24.sa.v_bias, module.blocks.24.sa.proj.bias, module.blocks.24.ca.v_bias, module.blocks.24.ca.mat_q.bias, '\n"
                    " 'module.blocks.24.ca.proj.bias, module.blocks.24.ffn.fc1.bias, module.blocks.24.ffn.fc2.bias, module.blocks.24.ca_norm.bias, module.blocks.25.ada_gss, module.blocks.25.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.25.sa.q_bias, module.blocks.25.sa.v_bias, module.blocks.25.sa.proj.bias, module.blocks.25.ca.v_bias, module.blocks.25.ca.mat_q.bias, module.blocks.25.ca.proj.bias, '\n"
                    " 'module.blocks.25.ffn.fc1.bias, module.blocks.25.ffn.fc2.bias, module.blocks.25.ca_norm.bias, module.blocks.26.ada_gss, module.blocks.26.sa.scale_mul_1H11, module.blocks.26.sa.q_bias, '\n"
                    " 'module.blocks.26.sa.v_bias, module.blocks.26.sa.proj.bias, module.blocks.26.ca.v_bias, module.blocks.26.ca.mat_q.bias, module.blocks.26.ca.proj.bias, module.blocks.26.ffn.fc1.bias, '\n"
                    " 'module.blocks.26.ffn.fc2.bias, module.blocks.26.ca_norm.bias, module.blocks.27.ada_gss, module.blocks.27.sa.scale_mul_1H11, module.blocks.27.sa.q_bias, module.blocks.27.sa.v_bias, '\n"
                    " 'module.blocks.27.sa.proj.bias, module.blocks.27.ca.v_bias, module.blocks.27.ca.mat_q.bias, module.blocks.27.ca.proj.bias, module.blocks.27.ffn.fc1.bias, module.blocks.27.ffn.fc2.bias, '\n"
                    " 'module.blocks.27.ca_norm.bias, module.blocks.28.ada_gss, module.blocks.28.sa.scale_mul_1H11, module.blocks.28.sa.q_bias, module.blocks.28.sa.v_bias, module.blocks.28.sa.proj.bias, '\n"
                    " 'module.blocks.28.ca.v_bias, module.blocks.28.ca.mat_q.bias, module.blocks.28.ca.proj.bias, module.blocks.28.ffn.fc1.bias, module.blocks.28.ffn.fc2.bias, module.blocks.28.ca_norm.bias, '\n"
                    " 'module.blocks.29.ada_gss, module.blocks.29.sa.scale_mul_1H11, module.blocks.29.sa.q_bias, module.blocks.29.sa.v_bias, module.blocks.29.sa.proj.bias, module.blocks.29.ca.v_bias, '\n"
                    " 'module.blocks.29.ca.mat_q.bias, module.blocks.29.ca.proj.bias, module.blocks.29.ffn.fc1.bias, module.blocks.29.ffn.fc2.bias, module.blocks.29.ca_norm.bias, module.blocks.30.ada_gss, '\n"
                    " 'module.blocks.30.sa.scale_mul_1H11, module.blocks.30.sa.q_bias, module.blocks.30.sa.v_bias, module.blocks.30.sa.proj.bias, module.blocks.30.ca.v_bias, module.blocks.30.ca.mat_q.bias, '\n"
                    " 'module.blocks.30.ca.proj.bias, module.blocks.30.ffn.fc1.bias, module.blocks.30.ffn.fc2.bias, module.blocks.30.ca_norm.bias, module.blocks.31.ada_gss, module.blocks.31.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.31.sa.q_bias, module.blocks.31.sa.v_bias, module.blocks.31.sa.proj.bias, module.blocks.31.ca.v_bias, module.blocks.31.ca.mat_q.bias, module.blocks.31.ca.proj.bias, '\n"
                    " 'module.blocks.31.ffn.fc1.bias, module.blocks.31.ffn.fc2.bias, module.blocks.31.ca_norm.bias, module.blocks.32.ada_gss, module.blocks.32.sa.scale_mul_1H11, module.blocks.32.sa.q_bias, '\n"
                    " 'module.blocks.32.sa.v_bias, module.blocks.32.sa.proj.bias, module.blocks.32.ca.v_bias, module.blocks.32.ca.mat_q.bias, module.blocks.32.ca.proj.bias, module.blocks.32.ffn.fc1.bias, '\n"
                    " 'module.blocks.32.ffn.fc2.bias, module.blocks.32.ca_norm.bias, module.blocks.33.ada_gss, module.blocks.33.sa.scale_mul_1H11, module.blocks.33.sa.q_bias, module.blocks.33.sa.v_bias, '\n"
                    " 'module.blocks.33.sa.proj.bias, module.blocks.33.ca.v_bias, module.blocks.33.ca.mat_q.bias, module.blocks.33.ca.proj.bias, module.blocks.33.ffn.fc1.bias, module.blocks.33.ffn.fc2.bias, '\n"
                    " 'module.blocks.33.ca_norm.bias, module.blocks.34.ada_gss, module.blocks.34.sa.scale_mul_1H11, module.blocks.34.sa.q_bias, module.blocks.34.sa.v_bias, module.blocks.34.sa.proj.bias, '\n"
                    " 'module.blocks.34.ca.v_bias, module.blocks.34.ca.mat_q.bias, module.blocks.34.ca.proj.bias, module.blocks.34.ffn.fc1.bias, module.blocks.34.ffn.fc2.bias, module.blocks.34.ca_norm.bias, '\n"
                    " 'module.blocks.35.ada_gss, module.blocks.35.sa.scale_mul_1H11, module.blocks.35.sa.q_bias, module.blocks.35.sa.v_bias, module.blocks.35.sa.proj.bias, module.blocks.35.ca.v_bias, '\n"
                    " 'module.blocks.35.ca.mat_q.bias, module.blocks.35.ca.proj.bias, module.blocks.35.ffn.fc1.bias, module.blocks.35.ffn.fc2.bias, module.blocks.35.ca_norm.bias, module.blocks.36.ada_gss, '\n"
                    " 'module.blocks.36.sa.scale_mul_1H11, module.blocks.36.sa.q_bias, module.blocks.36.sa.v_bias, module.blocks.36.sa.proj.bias, module.blocks.36.ca.v_bias, module.blocks.36.ca.mat_q.bias, '\n"
                    " 'module.blocks.36.ca.proj.bias, module.blocks.36.ffn.fc1.bias, module.blocks.36.ffn.fc2.bias, module.blocks.36.ca_norm.bias, module.blocks.37.ada_gss, module.blocks.37.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.37.sa.q_bias, module.blocks.37.sa.v_bias, module.blocks.37.sa.proj.bias, module.blocks.37.ca.v_bias, module.blocks.37.ca.mat_q.bias, module.blocks.37.ca.proj.bias, '\n"
                    " 'module.blocks.37.ffn.fc1.bias, module.blocks.37.ffn.fc2.bias, module.blocks.37.ca_norm.bias, module.blocks.38.ada_gss, module.blocks.38.sa.scale_mul_1H11, module.blocks.38.sa.q_bias, '\n"
                    " 'module.blocks.38.sa.v_bias, module.blocks.38.sa.proj.bias, module.blocks.38.ca.v_bias, module.blocks.38.ca.mat_q.bias, module.blocks.38.ca.proj.bias, module.blocks.38.ffn.fc1.bias, '\n"
                    " 'module.blocks.38.ffn.fc2.bias, module.blocks.38.ca_norm.bias, module.blocks.39.ada_gss, module.blocks.39.sa.scale_mul_1H11, module.blocks.39.sa.q_bias, module.blocks.39.sa.v_bias, '\n"
                    " 'module.blocks.39.sa.proj.bias, module.blocks.39.ca.v_bias, module.blocks.39.ca.mat_q.bias, module.blocks.39.ca.proj.bias, module.blocks.39.ffn.fc1.bias, module.blocks.39.ffn.fc2.bias, '\n"
                    " 'module.blocks.39.ca_norm.bias, module.blocks.40.ada_gss, module.blocks.40.sa.scale_mul_1H11, module.blocks.40.sa.q_bias, module.blocks.40.sa.v_bias, module.blocks.40.sa.proj.bias, '\n"
                    " 'module.blocks.40.ca.v_bias, module.blocks.40.ca.mat_q.bias, module.blocks.40.ca.proj.bias, module.blocks.40.ffn.fc1.bias, module.blocks.40.ffn.fc2.bias, module.blocks.40.ca_norm.bias, '\n"
                    " 'module.blocks.41.ada_gss, module.blocks.41.sa.scale_mul_1H11, module.blocks.41.sa.q_bias, module.blocks.41.sa.v_bias, module.blocks.41.sa.proj.bias, module.blocks.41.ca.v_bias, '\n"
                    " 'module.blocks.41.ca.mat_q.bias, module.blocks.41.ca.proj.bias, module.blocks.41.ffn.fc1.bias, module.blocks.41.ffn.fc2.bias, module.blocks.41.ca_norm.bias, module.blocks.42.ada_gss, '\n"
                    " 'module.blocks.42.sa.scale_mul_1H11, module.blocks.42.sa.q_bias, module.blocks.42.sa.v_bias, module.blocks.42.sa.proj.bias, module.blocks.42.ca.v_bias, module.blocks.42.ca.mat_q.bias, '\n"
                    " 'module.blocks.42.ca.proj.bias, module.blocks.42.ffn.fc1.bias, module.blocks.42.ffn.fc2.bias, module.blocks.42.ca_norm.bias, module.blocks.43.ada_gss, module.blocks.43.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.43.sa.q_bias, module.blocks.43.sa.v_bias, module.blocks.43.sa.proj.bias, module.blocks.43.ca.v_bias, module.blocks.43.ca.mat_q.bias, module.blocks.43.ca.proj.bias, '\n"
                    " 'module.blocks.43.ffn.fc1.bias, module.blocks.43.ffn.fc2.bias, module.blocks.43.ca_norm.bias, module.blocks.44.ada_gss, module.blocks.44.sa.scale_mul_1H11, module.blocks.44.sa.q_bias, '\n"
                    " 'module.blocks.44.sa.v_bias, module.blocks.44.sa.proj.bias, module.blocks.44.ca.v_bias, module.blocks.44.ca.mat_q.bias, module.blocks.44.ca.proj.bias, module.blocks.44.ffn.fc1.bias, '\n"
                    " 'module.blocks.44.ffn.fc2.bias, module.blocks.44.ca_norm.bias, module.blocks.45.ada_gss, module.blocks.45.sa.scale_mul_1H11, module.blocks.45.sa.q_bias, module.blocks.45.sa.v_bias, '\n"
                    " 'module.blocks.45.sa.proj.bias, module.blocks.45.ca.v_bias, module.blocks.45.ca.mat_q.bias, module.blocks.45.ca.proj.bias, module.blocks.45.ffn.fc1.bias, module.blocks.45.ffn.fc2.bias, '\n"
                    " 'module.blocks.45.ca_norm.bias, module.blocks.46.ada_gss, module.blocks.46.sa.scale_mul_1H11, module.blocks.46.sa.q_bias, module.blocks.46.sa.v_bias, module.blocks.46.sa.proj.bias, '\n"
                    " 'module.blocks.46.ca.v_bias, module.blocks.46.ca.mat_q.bias, module.blocks.46.ca.proj.bias, module.blocks.46.ffn.fc1.bias, module.blocks.46.ffn.fc2.bias, module.blocks.46.ca_norm.bias, '\n"
                    " 'module.blocks.47.ada_gss, module.blocks.47.sa.scale_mul_1H11, module.blocks.47.sa.q_bias, module.blocks.47.sa.v_bias, module.blocks.47.sa.proj.bias, module.blocks.47.ca.v_bias, '\n"
                    " 'module.blocks.47.ca.mat_q.bias, module.blocks.47.ca.proj.bias, module.blocks.47.ffn.fc1.bias, module.blocks.47.ffn.fc2.bias, module.blocks.47.ca_norm.bias, module.blocks.48.ada_gss, '\n"
                    " 'module.blocks.48.sa.scale_mul_1H11, module.blocks.48.sa.q_bias, module.blocks.48.sa.v_bias, module.blocks.48.sa.proj.bias, module.blocks.48.ca.v_bias, module.blocks.48.ca.mat_q.bias, '\n"
                    " 'module.blocks.48.ca.proj.bias, module.blocks.48.ffn.fc1.bias, module.blocks.48.ffn.fc2.bias, module.blocks.48.ca_norm.bias, module.blocks.49.ada_gss, module.blocks.49.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.49.sa.q_bias, module.blocks.49.sa.v_bias, module.blocks.49.sa.proj.bias, module.blocks.49.ca.v_bias, module.blocks.49.ca.mat_q.bias, module.blocks.49.ca.proj.bias, '\n"
                    " 'module.blocks.49.ffn.fc1.bias, module.blocks.49.ffn.fc2.bias, module.blocks.49.ca_norm.bias, module.blocks.50.ada_gss, module.blocks.50.sa.scale_mul_1H11, module.blocks.50.sa.q_bias, '\n"
                    " 'module.blocks.50.sa.v_bias, module.blocks.50.sa.proj.bias, module.blocks.50.ca.v_bias, module.blocks.50.ca.mat_q.bias, module.blocks.50.ca.proj.bias, module.blocks.50.ffn.fc1.bias, '\n"
                    " 'module.blocks.50.ffn.fc2.bias, module.blocks.50.ca_norm.bias, module.blocks.51.ada_gss, module.blocks.51.sa.scale_mul_1H11, module.blocks.51.sa.q_bias, module.blocks.51.sa.v_bias, '\n"
                    " 'module.blocks.51.sa.proj.bias, module.blocks.51.ca.v_bias, module.blocks.51.ca.mat_q.bias, module.blocks.51.ca.proj.bias, module.blocks.51.ffn.fc1.bias, module.blocks.51.ffn.fc2.bias, '\n"
                    " 'module.blocks.51.ca_norm.bias, module.blocks.52.ada_gss, module.blocks.52.sa.scale_mul_1H11, module.blocks.52.sa.q_bias, module.blocks.52.sa.v_bias, module.blocks.52.sa.proj.bias, '\n"
                    " 'module.blocks.52.ca.v_bias, module.blocks.52.ca.mat_q.bias, module.blocks.52.ca.proj.bias, module.blocks.52.ffn.fc1.bias, module.blocks.52.ffn.fc2.bias, module.blocks.52.ca_norm.bias, '\n"
                    " 'module.blocks.53.ada_gss, module.blocks.53.sa.scale_mul_1H11, module.blocks.53.sa.q_bias, module.blocks.53.sa.v_bias, module.blocks.53.sa.proj.bias, module.blocks.53.ca.v_bias, '\n"
                    " 'module.blocks.53.ca.mat_q.bias, module.blocks.53.ca.proj.bias, module.blocks.53.ffn.fc1.bias, module.blocks.53.ffn.fc2.bias, module.blocks.53.ca_norm.bias, module.blocks.54.ada_gss, '\n"
                    " 'module.blocks.54.sa.scale_mul_1H11, module.blocks.54.sa.q_bias, module.blocks.54.sa.v_bias, module.blocks.54.sa.proj.bias, module.blocks.54.ca.v_bias, module.blocks.54.ca.mat_q.bias, '\n"
                    " 'module.blocks.54.ca.proj.bias, module.blocks.54.ffn.fc1.bias, module.blocks.54.ffn.fc2.bias, module.blocks.54.ca_norm.bias, module.blocks.55.ada_gss, module.blocks.55.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.55.sa.q_bias, module.blocks.55.sa.v_bias, module.blocks.55.sa.proj.bias, module.blocks.55.ca.v_bias, module.blocks.55.ca.mat_q.bias, module.blocks.55.ca.proj.bias, '\n"
                    " 'module.blocks.55.ffn.fc1.bias, module.blocks.55.ffn.fc2.bias, module.blocks.55.ca_norm.bias')",
          'wd_sc': 0.0}}

[09-06 23:26:28] (nity/utils/lr_control.py, line 116)=> [get_param_groups][rank0] type(model).__name__='NullDDP' count=1085, numel=8573892016
[09-06 23:26:28] (nity/utils/lr_control.py, line 117)=> 
[09-06 23:26:28] (inity_circuit_breaker.py, line 343)=> [vgpt] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.97), fused=True), opt_kw={'lr': 9.375e-05, 'weight_decay': 0}

[09-06 23:26:28] (inity_circuit_breaker.py, line 348)=> Loading T5 from google/flan-t5-xl...
[09-06 23:26:40] (_original/src/trainer.py, line  65)=> self.reweight_loss_by_scale: 1
[09-06 23:26:40] (inity_circuit_breaker.py, line 171)=> global bs=4, local bs=4
[09-06 23:26:40] (inity_circuit_breaker.py, line 172)=> initial args:
{
  local_out_path      : ./local_output
  data_path           : ./data
  bed                 : ./checkpoints
  vae_ckpt            : /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
  exp_name            : experiment
  ds                  : oi
  model               : infinity_8b
  short_cap_prob      : 0.2
  project_name        : infinity_circuit_breaker
  tf32                : True
  auto_resume         : True
  rush_resume         : 
  nowd                : 1
  enable_hybrid_shard : False
  inner_shard_degree  : 1
  zero                : 2
  buck                : chunk
  fsdp_orig           : True
  enable_checkpointing: full-block
  pad_to_multiplier   : 128
  log_every_iter      : False
  checkpoint_type     : torch
  seed                : None
  rand                : True
  task_id             : 123
  trial_id            : 123
  robust_run_id       : 0
  ckpt_trials         : []
  real_trial_id       : 123
  chunk_nodes         : 0
  is_master_node      : None
  log_txt_path        : ./local_output/log.txt
  t5_path             : google/flan-t5-xl
  online_t5           : True
  sdpa_mem            : True
  tfast               : 0
  model_alias         : 8b
  rms                 : False
  aln                 : 0.001
  alng                : 5e-06
  saln                : True
  haln                : True
  nm0                 : False
  tau                 : 1
  cos                 : True
  swi                 : False
  dp                  : -1
  drop                : 0.0
  hd                  : 0
  ca_gamma            : -1
  diva                : 1
  hd0                 : 0.02
  dec                 : 1
  cum                 : 3
  rwe                 : False
  tp                  : 0.0
  tk                  : 0.0
  tini                : 0.010416666666666666
  cfg                 : 0.1
  rand_uncond         : False
  ema                 : 0.9999
  tema                : 0
  fp16                : 2
  fuse                : False
  fused_norm          : False
  flash               : False
  xen                 : False
  use_flex_attn       : True
  stable              : False
  gblr                : 0.0001
  dblr                : 0.0001
  tblr                : 0.006
  glr                 : 1.5625e-06
  dlr                 : 1.5625e-06
  tlr                 : 9.375e-05
  gwd                 : 0.005
  dwd                 : 0.0005
  twd                 : 0.005
  gwde                : 0.005
  dwde                : 0.0005
  twde                : 0.005
  ls                  : 0.0
  lz                  : 0.0
  eq                  : 0
  ep                  : 100
  wp                  : 1e-08
  wp0                 : 0.005
  wpe                 : 1.0
  sche                : lin0
  log_freq            : 50
  gclip               : 6.0
  dclip               : 6.0
  tclip               : 5.0
  cdec                : False
  opt                 : adamw
  ada                 : 0.9_0.97
  dada                : 0.9_0.97
  oeps                : 0
  afuse               : True
  pn                  : 0.06M
  scale_schedule      : None
  patch_size          : None
  resos               : None
  data_load_reso      : None
  workers             : 4
  lbs                 : 4
  bs                  : 4
  batch_size          : 4
  glb_batch_size      : 4
  ac                  : 1
  r_accu              : 1.0
  norm_eps            : 1e-06
  tlen                : 512
  Ct5                 : 2048
  use_bit_label       : 1
  bitloss_type        : mean
  dynamic_resolution_across_gpus: 1
  enable_dynamic_length_prompt: 1
  use_streaming_dataset: 1
  iterable_data_buffersize: 30000
  save_model_iters_freq: 100
  noise_apply_layers  : 13
  noise_apply_strength: 0.3
  noise_apply_requant : 1
  rope2d_each_sa_layer: 1
  rope2d_normalized_by_hw: 2
  use_fsdp_model_ema  : 0
  add_lvl_embeding_only_first_block: 1
  reweight_loss_by_scale: 1
  always_training_scales: 100
  vae_type            : 14
  fake_vae_input      : False
  model_init_device   : cuda
  prefetch_factor     : 2
  apply_spatial_patchify: 1
  debug_bsc           : 0
  task_type           : t2i
  target_layers       : [10, 12, 14, 16, 18, 20]
  transform_layers    : [10, 12, 14, 16, 18, 20]
  lorra_alpha         : 5.0
  trainsets           : ['AlpacaSupervisedDataset', 'HarmfulDataset']
  valsets             : ['AlpacaSupervisedDataset', 'HarmfulDataset']
  adv_string          : Sure here's
  full_layers         : False
  harmful_prompts_path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
  sanitized_prompts_path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
  validation_ratio    : 0.1
  category            : hate
  lora_r              : 8
  lora_alpha          : 16
  lora_dropout        : 0.05
  lora_target_modules : ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
  lora_weight_path    : 
  lora_bias           : none
  q_lora              : False
  model_name_or_path  : meta-llama/Llama-2-7b-chat-hf
  adapter_name_or_path: 
  use_lora            : True
  cache_dir           : 
  optim               : adamw_torch
  model_max_length    : 512
  grouped_to_max_length: False
  use_refusal_retain  : True
  sc_train_subset     : ['']
  log_every           : 10
  sc_train_seq_type   : all_text
  coeff_schedule      : linear_converge
  sc_loss_type        : orig_act_dotprod
  branch              : master
  commit_id           : 
  commit_msg          : 
  cmd                 : --bed ./checkpoints --local_out_path ./local_output --project_name infinity_circuit_breaker --exp_name experiment --ep 100 --opt adamw --cum 3 --sche lin0 --fp16 2 --ada 0.9_0.97 --tini -1 --tclip 5 --flash 0 --alng 5e-06 --saln 1 --cos 1 --enable_checkpointing full-block --tblr 6e-3 --pn 0.06M --lbs 4 --workers 4 --short_cap_prob 0.2 --online_t5 1 --use_streaming_dataset 1 --iterable_data_buffersize 30000 --Ct5 2048 --t5_path google/flan-t5-xl --vae_type 14 --wp 0.00000001 --wpe 1 --dynamic_resolution_across_gpus 1 --enable_dynamic_length_prompt 1 --reweight_loss_by_scale 1 --add_lvl_embeding_only_first_block 1 --rope2d_each_sa_layer 1 --rope2d_normalized_by_hw 2 --use_fsdp_model_ema 0 --always_training_scales 100 --use_bit_label 1 --zero 2 --save_model_iters_freq 100 --log_freq 50 --checkpoint_type torch --prefetch_factor 2 --noise_apply_strength 0.3 --noise_apply_layers 13 --apply_spatial_patchify 1 --use_flex_attn True --pad 128 --data_path ./data --data_load_reso 512 --tlen 512 --workers 4 --prefetch_factor 2 --harmful_prompts_path /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts --sanitized_prompts_path /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts --validation_ratio 0.1 --category hate --target_layers 10,12,14,16,18,20 --transform_layers 10,12,14,16,18,20 --lorra_alpha 5.0 --trainsets AlpacaSupervisedDataset#HarmfulDataset --valsets AlpacaSupervisedDataset#HarmfulDataset --adv_string Sure here's --full_layers False --lora_r 8 --lora_alpha 16 --lora_dropout 0.05 --lora_target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj --lora_weight_path  --lora_bias none --q_lora False --model_name_or_path meta-llama/Llama-2-7b-chat-hf --adapter_name_or_path  --use_lora True --cache_dir  --optim adamw_torch --model_max_length 512 --grouped_to_max_length False --use_refusal_retain True --sc_train_subset  --log_every 10 --sc_train_seq_type all_text --coeff_schedule linear_converge --sc_loss_type orig_act_dotprod
  tag                 : UK
  acc_all             : None
  acc_real            : None
  acc_fake            : None
  last_Lnll           : None
  last_L1             : None
  last_Ld             : None
  last_wei_g          : None
  grad_boom           : None
  diff                : None
  diffs               : 
  diffs_ema           : None
  ca_performance      : 
  cur_phase           : 
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  iter_speed          : None
  img_per_day         : None
  max_nvidia_smi      : 0
  max_memory_allocated: None
  max_memory_reserved : None
  num_alloc_retries   : None
  MFU                 : None
  HFU                 : None
  dbg_modified        : False
  dbg_ks              : False
  dbg_ks_last         : None
  dbg                 : False
  ks                  : False
  nodata              : False
  nodata_tlen         : 320
  nova                : False
  prof                : 0
  prof_freq           : 50
  tos_profiler_file_prefix: vgpt_default/
  profall             : 0
  v_seed              : 0
  g_seed              : 0
}

[09-06 23:26:40] (inity_circuit_breaker.py, line 182)=> start_it=0, iters_train=100
[09-06 23:26:40] (inity_circuit_breaker.py, line 466)=> [PT info]  from ep0 it0, acc_str: [no acc str], diffs: ,    =======>  bed: ./checkpoints  <=======








=======================================================   RESTART [09-06 23:44:41]   =======================================================
[09-06 23:44:41] (inity_circuit_breaker.py, line 718)=> ============================================================
[09-06 23:44:41] (inity_circuit_breaker.py, line 719)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[09-06 23:44:41] (inity_circuit_breaker.py, line 720)=> Circuit Breaker Alpha: 0.1
[09-06 23:44:41] (inity_circuit_breaker.py, line 721)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[09-06 23:44:41] (inity_circuit_breaker.py, line 722)=> Circuit Breaker Enabled: True
[09-06 23:44:41] (inity_circuit_breaker.py, line 723)=> Number of Examples: 1000
[09-06 23:44:41] (inity_circuit_breaker.py, line 724)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[09-06 23:44:41] (inity_circuit_breaker.py, line 725)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[09-06 23:44:41] (inity_circuit_breaker.py, line 726)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[09-06 23:44:41] (inity_circuit_breaker.py, line 727)=> Validation Ratio: 0.1
[09-06 23:44:41] (inity_circuit_breaker.py, line 728)=> Category: hate
[09-06 23:44:41] (inity_circuit_breaker.py, line 729)=> Batch Size: 4
[09-06 23:44:41] (inity_circuit_breaker.py, line 730)=> Workers: 4
[09-06 23:44:41] (inity_circuit_breaker.py, line 731)=> Device: cuda:0
[09-06 23:44:41] (inity_circuit_breaker.py, line 732)=> Model: infinity_8b
[09-06 23:44:41] (inity_circuit_breaker.py, line 733)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-06 23:44:41] (inity_circuit_breaker.py, line 734)=> Rush Resume: 
[09-06 23:44:41] (inity_circuit_breaker.py, line 735)=> Selective Layers: 0,1,2,3,4,5
[09-06 23:44:41] (inity_circuit_breaker.py, line 736)=> ============================================================
[09-06 23:44:41] (inity_circuit_breaker.py, line 377)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[09-06 23:44:41] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-06 23:44:41] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-06 23:44:41] (rain_dataset_infinity.py, line 205)=> Split 1000 prompts: 900 retain, 100 validation
[09-06 23:44:41] (rain_dataset_infinity.py, line 236)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-06 23:44:41] (rain_dataset_infinity.py, line 245)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-06 23:44:42] (rain_dataset_infinity.py, line  84)=> Loaded 900 retain examples
[09-06 23:44:42] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[09-06 23:44:42] (rain_dataset_infinity.py, line  86)=> Loaded 100 validation examples
[09-06 23:44:42] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[09-06 23:44:42] (inity_circuit_breaker.py, line 395)=> args.batch_size=4, vbs=6
[09-06 23:44:42] (inity_circuit_breaker.py, line 418)=> len(dataloader): 100, len(dataset): 100
[09-06 23:44:42] (inity_circuit_breaker.py, line 420)=> total_samples: 200
[09-06 23:44:42] (inity_circuit_breaker.py, line 421)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[09-06 23:44:42] (inity_circuit_breaker.py, line 140)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[09-06 23:44:42] (inity_circuit_breaker.py, line 142)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[09-06 23:44:42] (inity_circuit_breaker.py, line 146)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-06 23:44:44] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[09-06 23:44:44] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[09-06 23:44:44] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[09-06 23:44:44] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[09-06 23:44:46] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[09-06 23:44:48] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[09-06 23:44:48] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[09-06 23:44:48] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[09-06 23:44:48] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[09-06 23:44:48] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

[09-06 23:44:59] (ity/models/init_param.py, line  15)=> [init_weights] Infinity with std=0.02
[09-06 23:44:59] (inity_circuit_breaker.py, line 246)=> [PT] GPT model = Infinity(
  drop_path_rate=0.2
  (norm0_cond): Identity()
  (text_norm): FastRMSNorm(C=2048, eps=1e-06, elementwise_affine=True)
  (text_proj_for_sos): TextAttentivePool(
    (ca): CrossAttention(
      Cq=3072, Ckv=2048, cos_attn=False
      (mat_kv): Linear(in_features=2048, out_features=6144, bias=False)
      (proj): Linear(in_features=3072, out_features=3072, bias=True)
      (proj_drop): Identity()
    )
  )
  (text_proj_for_ca): Sequential(
    (0): Linear(in_features=2048, out_features=3072, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=3072, out_features=3072, bias=True)
  )
  (lvl_embed): Embedding(15, 3072)
  (norm0_ve): Identity()
  (word_embed): Linear(in_features=56, out_features=3072, bias=True)
  (shared_ada_lin): Sequential(
    (0): SiLU()
    (1): SharedAdaLin(in_features=3072, out_features=18432, bias=True)
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=3072, out_features=6144, bias=True)
    )
  )
  (head): Linear(in_features=3072, out_features=112, bias=True)
  (blocks): ModuleList(
    (0): CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): Identity()
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
    (1-55): 55 x CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): DropPath(...)
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
  )
)


[09-06 23:44:59] (inity_circuit_breaker.py, line 248)=> [PT][#para] VAE=81.62, VAE.quant=0.00
[09-06 23:44:59] (inity_circuit_breaker.py, line 251)=> [PT][#para] GPT=8573.89


[09-06 23:44:59] (nity/utils/lr_control.py, line  71)=> [get_param_groups][lr decay] with_lr_scale=False, lr_scale=0.0
[09-06 23:44:59] (nity/utils/lr_control.py, line 111)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': '[no scale]',
         'params': "('module.text_norm.weight, module.text_proj_for_sos.ca.mat_kv.weight, module.text_proj_for_sos.ca.proj.weight, module.text_proj_for_ca.0.weight, module.text_proj_for_ca.2.weight, '\n"
                   " 'module.word_embed.weight, module.shared_ada_lin.1.weight, module.head_nm.ada_lin.1.weight, module.head.weight, module.blocks.0.sa.mat_qkv.weight, module.blocks.0.sa.proj.weight, '\n"
                   " 'module.blocks.0.ca.mat_q.weight, module.blocks.0.ca.mat_kv.weight, module.blocks.0.ca.proj.weight, module.blocks.0.ffn.fc1.weight, module.blocks.0.ffn.fc2.weight, module.blocks.0.ca_norm.weight, '\n"
                   " 'module.blocks.1.sa.mat_qkv.weight, module.blocks.1.sa.proj.weight, module.blocks.1.ca.mat_q.weight, module.blocks.1.ca.mat_kv.weight, module.blocks.1.ca.proj.weight, '\n"
                   " 'module.blocks.1.ffn.fc1.weight, module.blocks.1.ffn.fc2.weight, module.blocks.1.ca_norm.weight, module.blocks.2.sa.mat_qkv.weight, module.blocks.2.sa.proj.weight, module.blocks.2.ca.mat_q.weight, '\n"
                   " 'module.blocks.2.ca.mat_kv.weight, module.blocks.2.ca.proj.weight, module.blocks.2.ffn.fc1.weight, module.blocks.2.ffn.fc2.weight, module.blocks.2.ca_norm.weight, module.blocks.3.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.3.sa.proj.weight, module.blocks.3.ca.mat_q.weight, module.blocks.3.ca.mat_kv.weight, module.blocks.3.ca.proj.weight, module.blocks.3.ffn.fc1.weight, module.blocks.3.ffn.fc2.weight, '\n"
                   " 'module.blocks.3.ca_norm.weight, module.blocks.4.sa.mat_qkv.weight, module.blocks.4.sa.proj.weight, module.blocks.4.ca.mat_q.weight, module.blocks.4.ca.mat_kv.weight, '\n"
                   " 'module.blocks.4.ca.proj.weight, module.blocks.4.ffn.fc1.weight, module.blocks.4.ffn.fc2.weight, module.blocks.4.ca_norm.weight, module.blocks.5.sa.mat_qkv.weight, module.blocks.5.sa.proj.weight, '\n"
                   " 'module.blocks.5.ca.mat_q.weight, module.blocks.5.ca.mat_kv.weight, module.blocks.5.ca.proj.weight, module.blocks.5.ffn.fc1.weight, module.blocks.5.ffn.fc2.weight, module.blocks.5.ca_norm.weight, '\n"
                   " 'module.blocks.6.sa.mat_qkv.weight, module.blocks.6.sa.proj.weight, module.blocks.6.ca.mat_q.weight, module.blocks.6.ca.mat_kv.weight, module.blocks.6.ca.proj.weight, '\n"
                   " 'module.blocks.6.ffn.fc1.weight, module.blocks.6.ffn.fc2.weight, module.blocks.6.ca_norm.weight, module.blocks.7.sa.mat_qkv.weight, module.blocks.7.sa.proj.weight, module.blocks.7.ca.mat_q.weight, '\n"
                   " 'module.blocks.7.ca.mat_kv.weight, module.blocks.7.ca.proj.weight, module.blocks.7.ffn.fc1.weight, module.blocks.7.ffn.fc2.weight, module.blocks.7.ca_norm.weight, module.blocks.8.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.8.sa.proj.weight, module.blocks.8.ca.mat_q.weight, module.blocks.8.ca.mat_kv.weight, module.blocks.8.ca.proj.weight, module.blocks.8.ffn.fc1.weight, module.blocks.8.ffn.fc2.weight, '\n"
                   " 'module.blocks.8.ca_norm.weight, module.blocks.9.sa.mat_qkv.weight, module.blocks.9.sa.proj.weight, module.blocks.9.ca.mat_q.weight, module.blocks.9.ca.mat_kv.weight, '\n"
                   " 'module.blocks.9.ca.proj.weight, module.blocks.9.ffn.fc1.weight, module.blocks.9.ffn.fc2.weight, module.blocks.9.ca_norm.weight, module.blocks.10.sa.mat_qkv.weight, module.blocks.10.sa.proj.weight, '\n"
                   " 'module.blocks.10.ca.mat_q.weight, module.blocks.10.ca.mat_kv.weight, module.blocks.10.ca.proj.weight, module.blocks.10.ffn.fc1.weight, module.blocks.10.ffn.fc2.weight, '\n"
                   " 'module.blocks.10.ca_norm.weight, module.blocks.11.sa.mat_qkv.weight, module.blocks.11.sa.proj.weight, module.blocks.11.ca.mat_q.weight, module.blocks.11.ca.mat_kv.weight, '\n"
                   " 'module.blocks.11.ca.proj.weight, module.blocks.11.ffn.fc1.weight, module.blocks.11.ffn.fc2.weight, module.blocks.11.ca_norm.weight, module.blocks.12.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.12.sa.proj.weight, module.blocks.12.ca.mat_q.weight, module.blocks.12.ca.mat_kv.weight, module.blocks.12.ca.proj.weight, module.blocks.12.ffn.fc1.weight, '\n"
                   " 'module.blocks.12.ffn.fc2.weight, module.blocks.12.ca_norm.weight, module.blocks.13.sa.mat_qkv.weight, module.blocks.13.sa.proj.weight, module.blocks.13.ca.mat_q.weight, '\n"
                   " 'module.blocks.13.ca.mat_kv.weight, module.blocks.13.ca.proj.weight, module.blocks.13.ffn.fc1.weight, module.blocks.13.ffn.fc2.weight, module.blocks.13.ca_norm.weight, '\n"
                   " 'module.blocks.14.sa.mat_qkv.weight, module.blocks.14.sa.proj.weight, module.blocks.14.ca.mat_q.weight, module.blocks.14.ca.mat_kv.weight, module.blocks.14.ca.proj.weight, '\n"
                   " 'module.blocks.14.ffn.fc1.weight, module.blocks.14.ffn.fc2.weight, module.blocks.14.ca_norm.weight, module.blocks.15.sa.mat_qkv.weight, module.blocks.15.sa.proj.weight, '\n"
                   " 'module.blocks.15.ca.mat_q.weight, module.blocks.15.ca.mat_kv.weight, module.blocks.15.ca.proj.weight, module.blocks.15.ffn.fc1.weight, module.blocks.15.ffn.fc2.weight, '\n"
                   " 'module.blocks.15.ca_norm.weight, module.blocks.16.sa.mat_qkv.weight, module.blocks.16.sa.proj.weight, module.blocks.16.ca.mat_q.weight, module.blocks.16.ca.mat_kv.weight, '\n"
                   " 'module.blocks.16.ca.proj.weight, module.blocks.16.ffn.fc1.weight, module.blocks.16.ffn.fc2.weight, module.blocks.16.ca_norm.weight, module.blocks.17.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.17.sa.proj.weight, module.blocks.17.ca.mat_q.weight, module.blocks.17.ca.mat_kv.weight, module.blocks.17.ca.proj.weight, module.blocks.17.ffn.fc1.weight, '\n"
                   " 'module.blocks.17.ffn.fc2.weight, module.blocks.17.ca_norm.weight, module.blocks.18.sa.mat_qkv.weight, module.blocks.18.sa.proj.weight, module.blocks.18.ca.mat_q.weight, '\n"
                   " 'module.blocks.18.ca.mat_kv.weight, module.blocks.18.ca.proj.weight, module.blocks.18.ffn.fc1.weight, module.blocks.18.ffn.fc2.weight, module.blocks.18.ca_norm.weight, '\n"
                   " 'module.blocks.19.sa.mat_qkv.weight, module.blocks.19.sa.proj.weight, module.blocks.19.ca.mat_q.weight, module.blocks.19.ca.mat_kv.weight, module.blocks.19.ca.proj.weight, '\n"
                   " 'module.blocks.19.ffn.fc1.weight, module.blocks.19.ffn.fc2.weight, module.blocks.19.ca_norm.weight, module.blocks.20.sa.mat_qkv.weight, module.blocks.20.sa.proj.weight, '\n"
                   " 'module.blocks.20.ca.mat_q.weight, module.blocks.20.ca.mat_kv.weight, module.blocks.20.ca.proj.weight, module.blocks.20.ffn.fc1.weight, module.blocks.20.ffn.fc2.weight, '\n"
                   " 'module.blocks.20.ca_norm.weight, module.blocks.21.sa.mat_qkv.weight, module.blocks.21.sa.proj.weight, module.blocks.21.ca.mat_q.weight, module.blocks.21.ca.mat_kv.weight, '\n"
                   " 'module.blocks.21.ca.proj.weight, module.blocks.21.ffn.fc1.weight, module.blocks.21.ffn.fc2.weight, module.blocks.21.ca_norm.weight, module.blocks.22.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.22.sa.proj.weight, module.blocks.22.ca.mat_q.weight, module.blocks.22.ca.mat_kv.weight, module.blocks.22.ca.proj.weight, module.blocks.22.ffn.fc1.weight, '\n"
                   " 'module.blocks.22.ffn.fc2.weight, module.blocks.22.ca_norm.weight, module.blocks.23.sa.mat_qkv.weight, module.blocks.23.sa.proj.weight, module.blocks.23.ca.mat_q.weight, '\n"
                   " 'module.blocks.23.ca.mat_kv.weight, module.blocks.23.ca.proj.weight, module.blocks.23.ffn.fc1.weight, module.blocks.23.ffn.fc2.weight, module.blocks.23.ca_norm.weight, '\n"
                   " 'module.blocks.24.sa.mat_qkv.weight, module.blocks.24.sa.proj.weight, module.blocks.24.ca.mat_q.weight, module.blocks.24.ca.mat_kv.weight, module.blocks.24.ca.proj.weight, '\n"
                   " 'module.blocks.24.ffn.fc1.weight, module.blocks.24.ffn.fc2.weight, module.blocks.24.ca_norm.weight, module.blocks.25.sa.mat_qkv.weight, module.blocks.25.sa.proj.weight, '\n"
                   " 'module.blocks.25.ca.mat_q.weight, module.blocks.25.ca.mat_kv.weight, module.blocks.25.ca.proj.weight, module.blocks.25.ffn.fc1.weight, module.blocks.25.ffn.fc2.weight, '\n"
                   " 'module.blocks.25.ca_norm.weight, module.blocks.26.sa.mat_qkv.weight, module.blocks.26.sa.proj.weight, module.blocks.26.ca.mat_q.weight, module.blocks.26.ca.mat_kv.weight, '\n"
                   " 'module.blocks.26.ca.proj.weight, module.blocks.26.ffn.fc1.weight, module.blocks.26.ffn.fc2.weight, module.blocks.26.ca_norm.weight, module.blocks.27.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.27.sa.proj.weight, module.blocks.27.ca.mat_q.weight, module.blocks.27.ca.mat_kv.weight, module.blocks.27.ca.proj.weight, module.blocks.27.ffn.fc1.weight, '\n"
                   " 'module.blocks.27.ffn.fc2.weight, module.blocks.27.ca_norm.weight, module.blocks.28.sa.mat_qkv.weight, module.blocks.28.sa.proj.weight, module.blocks.28.ca.mat_q.weight, '\n"
                   " 'module.blocks.28.ca.mat_kv.weight, module.blocks.28.ca.proj.weight, module.blocks.28.ffn.fc1.weight, module.blocks.28.ffn.fc2.weight, module.blocks.28.ca_norm.weight, '\n"
                   " 'module.blocks.29.sa.mat_qkv.weight, module.blocks.29.sa.proj.weight, module.blocks.29.ca.mat_q.weight, module.blocks.29.ca.mat_kv.weight, module.blocks.29.ca.proj.weight, '\n"
                   " 'module.blocks.29.ffn.fc1.weight, module.blocks.29.ffn.fc2.weight, module.blocks.29.ca_norm.weight, module.blocks.30.sa.mat_qkv.weight, module.blocks.30.sa.proj.weight, '\n"
                   " 'module.blocks.30.ca.mat_q.weight, module.blocks.30.ca.mat_kv.weight, module.blocks.30.ca.proj.weight, module.blocks.30.ffn.fc1.weight, module.blocks.30.ffn.fc2.weight, '\n"
                   " 'module.blocks.30.ca_norm.weight, module.blocks.31.sa.mat_qkv.weight, module.blocks.31.sa.proj.weight, module.blocks.31.ca.mat_q.weight, module.blocks.31.ca.mat_kv.weight, '\n"
                   " 'module.blocks.31.ca.proj.weight, module.blocks.31.ffn.fc1.weight, module.blocks.31.ffn.fc2.weight, module.blocks.31.ca_norm.weight, module.blocks.32.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.32.sa.proj.weight, module.blocks.32.ca.mat_q.weight, module.blocks.32.ca.mat_kv.weight, module.blocks.32.ca.proj.weight, module.blocks.32.ffn.fc1.weight, '\n"
                   " 'module.blocks.32.ffn.fc2.weight, module.blocks.32.ca_norm.weight, module.blocks.33.sa.mat_qkv.weight, module.blocks.33.sa.proj.weight, module.blocks.33.ca.mat_q.weight, '\n"
                   " 'module.blocks.33.ca.mat_kv.weight, module.blocks.33.ca.proj.weight, module.blocks.33.ffn.fc1.weight, module.blocks.33.ffn.fc2.weight, module.blocks.33.ca_norm.weight, '\n"
                   " 'module.blocks.34.sa.mat_qkv.weight, module.blocks.34.sa.proj.weight, module.blocks.34.ca.mat_q.weight, module.blocks.34.ca.mat_kv.weight, module.blocks.34.ca.proj.weight, '\n"
                   " 'module.blocks.34.ffn.fc1.weight, module.blocks.34.ffn.fc2.weight, module.blocks.34.ca_norm.weight, module.blocks.35.sa.mat_qkv.weight, module.blocks.35.sa.proj.weight, '\n"
                   " 'module.blocks.35.ca.mat_q.weight, module.blocks.35.ca.mat_kv.weight, module.blocks.35.ca.proj.weight, module.blocks.35.ffn.fc1.weight, module.blocks.35.ffn.fc2.weight, '\n"
                   " 'module.blocks.35.ca_norm.weight, module.blocks.36.sa.mat_qkv.weight, module.blocks.36.sa.proj.weight, module.blocks.36.ca.mat_q.weight, module.blocks.36.ca.mat_kv.weight, '\n"
                   " 'module.blocks.36.ca.proj.weight, module.blocks.36.ffn.fc1.weight, module.blocks.36.ffn.fc2.weight, module.blocks.36.ca_norm.weight, module.blocks.37.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.37.sa.proj.weight, module.blocks.37.ca.mat_q.weight, module.blocks.37.ca.mat_kv.weight, module.blocks.37.ca.proj.weight, module.blocks.37.ffn.fc1.weight, '\n"
                   " 'module.blocks.37.ffn.fc2.weight, module.blocks.37.ca_norm.weight, module.blocks.38.sa.mat_qkv.weight, module.blocks.38.sa.proj.weight, module.blocks.38.ca.mat_q.weight, '\n"
                   " 'module.blocks.38.ca.mat_kv.weight, module.blocks.38.ca.proj.weight, module.blocks.38.ffn.fc1.weight, module.blocks.38.ffn.fc2.weight, module.blocks.38.ca_norm.weight, '\n"
                   " 'module.blocks.39.sa.mat_qkv.weight, module.blocks.39.sa.proj.weight, module.blocks.39.ca.mat_q.weight, module.blocks.39.ca.mat_kv.weight, module.blocks.39.ca.proj.weight, '\n"
                   " 'module.blocks.39.ffn.fc1.weight, module.blocks.39.ffn.fc2.weight, module.blocks.39.ca_norm.weight, module.blocks.40.sa.mat_qkv.weight, module.blocks.40.sa.proj.weight, '\n"
                   " 'module.blocks.40.ca.mat_q.weight, module.blocks.40.ca.mat_kv.weight, module.blocks.40.ca.proj.weight, module.blocks.40.ffn.fc1.weight, module.blocks.40.ffn.fc2.weight, '\n"
                   " 'module.blocks.40.ca_norm.weight, module.blocks.41.sa.mat_qkv.weight, module.blocks.41.sa.proj.weight, module.blocks.41.ca.mat_q.weight, module.blocks.41.ca.mat_kv.weight, '\n"
                   " 'module.blocks.41.ca.proj.weight, module.blocks.41.ffn.fc1.weight, module.blocks.41.ffn.fc2.weight, module.blocks.41.ca_norm.weight, module.blocks.42.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.42.sa.proj.weight, module.blocks.42.ca.mat_q.weight, module.blocks.42.ca.mat_kv.weight, module.blocks.42.ca.proj.weight, module.blocks.42.ffn.fc1.weight, '\n"
                   " 'module.blocks.42.ffn.fc2.weight, module.blocks.42.ca_norm.weight, module.blocks.43.sa.mat_qkv.weight, module.blocks.43.sa.proj.weight, module.blocks.43.ca.mat_q.weight, '\n"
                   " 'module.blocks.43.ca.mat_kv.weight, module.blocks.43.ca.proj.weight, module.blocks.43.ffn.fc1.weight, module.blocks.43.ffn.fc2.weight, module.blocks.43.ca_norm.weight, '\n"
                   " 'module.blocks.44.sa.mat_qkv.weight, module.blocks.44.sa.proj.weight, module.blocks.44.ca.mat_q.weight, module.blocks.44.ca.mat_kv.weight, module.blocks.44.ca.proj.weight, '\n"
                   " 'module.blocks.44.ffn.fc1.weight, module.blocks.44.ffn.fc2.weight, module.blocks.44.ca_norm.weight, module.blocks.45.sa.mat_qkv.weight, module.blocks.45.sa.proj.weight, '\n"
                   " 'module.blocks.45.ca.mat_q.weight, module.blocks.45.ca.mat_kv.weight, module.blocks.45.ca.proj.weight, module.blocks.45.ffn.fc1.weight, module.blocks.45.ffn.fc2.weight, '\n"
                   " 'module.blocks.45.ca_norm.weight, module.blocks.46.sa.mat_qkv.weight, module.blocks.46.sa.proj.weight, module.blocks.46.ca.mat_q.weight, module.blocks.46.ca.mat_kv.weight, '\n"
                   " 'module.blocks.46.ca.proj.weight, module.blocks.46.ffn.fc1.weight, module.blocks.46.ffn.fc2.weight, module.blocks.46.ca_norm.weight, module.blocks.47.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.47.sa.proj.weight, module.blocks.47.ca.mat_q.weight, module.blocks.47.ca.mat_kv.weight, module.blocks.47.ca.proj.weight, module.blocks.47.ffn.fc1.weight, '\n"
                   " 'module.blocks.47.ffn.fc2.weight, module.blocks.47.ca_norm.weight, module.blocks.48.sa.mat_qkv.weight, module.blocks.48.sa.proj.weight, module.blocks.48.ca.mat_q.weight, '\n"
                   " 'module.blocks.48.ca.mat_kv.weight, module.blocks.48.ca.proj.weight, module.blocks.48.ffn.fc1.weight, module.blocks.48.ffn.fc2.weight, module.blocks.48.ca_norm.weight, '\n"
                   " 'module.blocks.49.sa.mat_qkv.weight, module.blocks.49.sa.proj.weight, module.blocks.49.ca.mat_q.weight, module.blocks.49.ca.mat_kv.weight, module.blocks.49.ca.proj.weight, '\n"
                   " 'module.blocks.49.ffn.fc1.weight, module.blocks.49.ffn.fc2.weight, module.blocks.49.ca_norm.weight, module.blocks.50.sa.mat_qkv.weight, module.blocks.50.sa.proj.weight, '\n"
                   " 'module.blocks.50.ca.mat_q.weight, module.blocks.50.ca.mat_kv.weight, module.blocks.50.ca.proj.weight, module.blocks.50.ffn.fc1.weight, module.blocks.50.ffn.fc2.weight, '\n"
                   " 'module.blocks.50.ca_norm.weight, module.blocks.51.sa.mat_qkv.weight, module.blocks.51.sa.proj.weight, module.blocks.51.ca.mat_q.weight, module.blocks.51.ca.mat_kv.weight, '\n"
                   " 'module.blocks.51.ca.proj.weight, module.blocks.51.ffn.fc1.weight, module.blocks.51.ffn.fc2.weight, module.blocks.51.ca_norm.weight, module.blocks.52.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.52.sa.proj.weight, module.blocks.52.ca.mat_q.weight, module.blocks.52.ca.mat_kv.weight, module.blocks.52.ca.proj.weight, module.blocks.52.ffn.fc1.weight, '\n"
                   " 'module.blocks.52.ffn.fc2.weight, module.blocks.52.ca_norm.weight, module.blocks.53.sa.mat_qkv.weight, module.blocks.53.sa.proj.weight, module.blocks.53.ca.mat_q.weight, '\n"
                   " 'module.blocks.53.ca.mat_kv.weight, module.blocks.53.ca.proj.weight, module.blocks.53.ffn.fc1.weight, module.blocks.53.ffn.fc2.weight, module.blocks.53.ca_norm.weight, '\n"
                   " 'module.blocks.54.sa.mat_qkv.weight, module.blocks.54.sa.proj.weight, module.blocks.54.ca.mat_q.weight, module.blocks.54.ca.mat_kv.weight, module.blocks.54.ca.proj.weight, '\n"
                   " 'module.blocks.54.ffn.fc1.weight, module.blocks.54.ffn.fc2.weight, module.blocks.54.ca_norm.weight, module.blocks.55.sa.mat_qkv.weight, module.blocks.55.sa.proj.weight, '\n"
                   " 'module.blocks.55.ca.mat_q.weight, module.blocks.55.ca.mat_kv.weight, module.blocks.55.ca.proj.weight, module.blocks.55.ffn.fc1.weight, module.blocks.55.ffn.fc2.weight, '\n"
                   " 'module.blocks.55.ca_norm.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': '[no scale]',
          'params': "('module.cfg_uncond, module.pos_start, module.text_proj_for_sos.ca.mat_q, module.text_proj_for_sos.ca.v_bias, module.text_proj_for_sos.ca.proj.bias, module.text_proj_for_ca.0.bias, '\n"
                    " 'module.text_proj_for_ca.2.bias, module.lvl_embed.weight, module.word_embed.bias, module.shared_ada_lin.1.bias, module.head_nm.ada_lin.1.bias, module.head.bias, module.blocks.0.ada_gss, '\n"
                    " 'module.blocks.0.sa.scale_mul_1H11, module.blocks.0.sa.q_bias, module.blocks.0.sa.v_bias, module.blocks.0.sa.proj.bias, module.blocks.0.ca.v_bias, module.blocks.0.ca.mat_q.bias, '\n"
                    " 'module.blocks.0.ca.proj.bias, module.blocks.0.ffn.fc1.bias, module.blocks.0.ffn.fc2.bias, module.blocks.0.ca_norm.bias, module.blocks.1.ada_gss, module.blocks.1.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.1.sa.q_bias, module.blocks.1.sa.v_bias, module.blocks.1.sa.proj.bias, module.blocks.1.ca.v_bias, module.blocks.1.ca.mat_q.bias, module.blocks.1.ca.proj.bias, '\n"
                    " 'module.blocks.1.ffn.fc1.bias, module.blocks.1.ffn.fc2.bias, module.blocks.1.ca_norm.bias, module.blocks.2.ada_gss, module.blocks.2.sa.scale_mul_1H11, module.blocks.2.sa.q_bias, '\n"
                    " 'module.blocks.2.sa.v_bias, module.blocks.2.sa.proj.bias, module.blocks.2.ca.v_bias, module.blocks.2.ca.mat_q.bias, module.blocks.2.ca.proj.bias, module.blocks.2.ffn.fc1.bias, '\n"
                    " 'module.blocks.2.ffn.fc2.bias, module.blocks.2.ca_norm.bias, module.blocks.3.ada_gss, module.blocks.3.sa.scale_mul_1H11, module.blocks.3.sa.q_bias, module.blocks.3.sa.v_bias, '\n"
                    " 'module.blocks.3.sa.proj.bias, module.blocks.3.ca.v_bias, module.blocks.3.ca.mat_q.bias, module.blocks.3.ca.proj.bias, module.blocks.3.ffn.fc1.bias, module.blocks.3.ffn.fc2.bias, '\n"
                    " 'module.blocks.3.ca_norm.bias, module.blocks.4.ada_gss, module.blocks.4.sa.scale_mul_1H11, module.blocks.4.sa.q_bias, module.blocks.4.sa.v_bias, module.blocks.4.sa.proj.bias, '\n"
                    " 'module.blocks.4.ca.v_bias, module.blocks.4.ca.mat_q.bias, module.blocks.4.ca.proj.bias, module.blocks.4.ffn.fc1.bias, module.blocks.4.ffn.fc2.bias, module.blocks.4.ca_norm.bias, '\n"
                    " 'module.blocks.5.ada_gss, module.blocks.5.sa.scale_mul_1H11, module.blocks.5.sa.q_bias, module.blocks.5.sa.v_bias, module.blocks.5.sa.proj.bias, module.blocks.5.ca.v_bias, '\n"
                    " 'module.blocks.5.ca.mat_q.bias, module.blocks.5.ca.proj.bias, module.blocks.5.ffn.fc1.bias, module.blocks.5.ffn.fc2.bias, module.blocks.5.ca_norm.bias, module.blocks.6.ada_gss, '\n"
                    " 'module.blocks.6.sa.scale_mul_1H11, module.blocks.6.sa.q_bias, module.blocks.6.sa.v_bias, module.blocks.6.sa.proj.bias, module.blocks.6.ca.v_bias, module.blocks.6.ca.mat_q.bias, '\n"
                    " 'module.blocks.6.ca.proj.bias, module.blocks.6.ffn.fc1.bias, module.blocks.6.ffn.fc2.bias, module.blocks.6.ca_norm.bias, module.blocks.7.ada_gss, module.blocks.7.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.7.sa.q_bias, module.blocks.7.sa.v_bias, module.blocks.7.sa.proj.bias, module.blocks.7.ca.v_bias, module.blocks.7.ca.mat_q.bias, module.blocks.7.ca.proj.bias, '\n"
                    " 'module.blocks.7.ffn.fc1.bias, module.blocks.7.ffn.fc2.bias, module.blocks.7.ca_norm.bias, module.blocks.8.ada_gss, module.blocks.8.sa.scale_mul_1H11, module.blocks.8.sa.q_bias, '\n"
                    " 'module.blocks.8.sa.v_bias, module.blocks.8.sa.proj.bias, module.blocks.8.ca.v_bias, module.blocks.8.ca.mat_q.bias, module.blocks.8.ca.proj.bias, module.blocks.8.ffn.fc1.bias, '\n"
                    " 'module.blocks.8.ffn.fc2.bias, module.blocks.8.ca_norm.bias, module.blocks.9.ada_gss, module.blocks.9.sa.scale_mul_1H11, module.blocks.9.sa.q_bias, module.blocks.9.sa.v_bias, '\n"
                    " 'module.blocks.9.sa.proj.bias, module.blocks.9.ca.v_bias, module.blocks.9.ca.mat_q.bias, module.blocks.9.ca.proj.bias, module.blocks.9.ffn.fc1.bias, module.blocks.9.ffn.fc2.bias, '\n"
                    " 'module.blocks.9.ca_norm.bias, module.blocks.10.ada_gss, module.blocks.10.sa.scale_mul_1H11, module.blocks.10.sa.q_bias, module.blocks.10.sa.v_bias, module.blocks.10.sa.proj.bias, '\n"
                    " 'module.blocks.10.ca.v_bias, module.blocks.10.ca.mat_q.bias, module.blocks.10.ca.proj.bias, module.blocks.10.ffn.fc1.bias, module.blocks.10.ffn.fc2.bias, module.blocks.10.ca_norm.bias, '\n"
                    " 'module.blocks.11.ada_gss, module.blocks.11.sa.scale_mul_1H11, module.blocks.11.sa.q_bias, module.blocks.11.sa.v_bias, module.blocks.11.sa.proj.bias, module.blocks.11.ca.v_bias, '\n"
                    " 'module.blocks.11.ca.mat_q.bias, module.blocks.11.ca.proj.bias, module.blocks.11.ffn.fc1.bias, module.blocks.11.ffn.fc2.bias, module.blocks.11.ca_norm.bias, module.blocks.12.ada_gss, '\n"
                    " 'module.blocks.12.sa.scale_mul_1H11, module.blocks.12.sa.q_bias, module.blocks.12.sa.v_bias, module.blocks.12.sa.proj.bias, module.blocks.12.ca.v_bias, module.blocks.12.ca.mat_q.bias, '\n"
                    " 'module.blocks.12.ca.proj.bias, module.blocks.12.ffn.fc1.bias, module.blocks.12.ffn.fc2.bias, module.blocks.12.ca_norm.bias, module.blocks.13.ada_gss, module.blocks.13.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.13.sa.q_bias, module.blocks.13.sa.v_bias, module.blocks.13.sa.proj.bias, module.blocks.13.ca.v_bias, module.blocks.13.ca.mat_q.bias, module.blocks.13.ca.proj.bias, '\n"
                    " 'module.blocks.13.ffn.fc1.bias, module.blocks.13.ffn.fc2.bias, module.blocks.13.ca_norm.bias, module.blocks.14.ada_gss, module.blocks.14.sa.scale_mul_1H11, module.blocks.14.sa.q_bias, '\n"
                    " 'module.blocks.14.sa.v_bias, module.blocks.14.sa.proj.bias, module.blocks.14.ca.v_bias, module.blocks.14.ca.mat_q.bias, module.blocks.14.ca.proj.bias, module.blocks.14.ffn.fc1.bias, '\n"
                    " 'module.blocks.14.ffn.fc2.bias, module.blocks.14.ca_norm.bias, module.blocks.15.ada_gss, module.blocks.15.sa.scale_mul_1H11, module.blocks.15.sa.q_bias, module.blocks.15.sa.v_bias, '\n"
                    " 'module.blocks.15.sa.proj.bias, module.blocks.15.ca.v_bias, module.blocks.15.ca.mat_q.bias, module.blocks.15.ca.proj.bias, module.blocks.15.ffn.fc1.bias, module.blocks.15.ffn.fc2.bias, '\n"
                    " 'module.blocks.15.ca_norm.bias, module.blocks.16.ada_gss, module.blocks.16.sa.scale_mul_1H11, module.blocks.16.sa.q_bias, module.blocks.16.sa.v_bias, module.blocks.16.sa.proj.bias, '\n"
                    " 'module.blocks.16.ca.v_bias, module.blocks.16.ca.mat_q.bias, module.blocks.16.ca.proj.bias, module.blocks.16.ffn.fc1.bias, module.blocks.16.ffn.fc2.bias, module.blocks.16.ca_norm.bias, '\n"
                    " 'module.blocks.17.ada_gss, module.blocks.17.sa.scale_mul_1H11, module.blocks.17.sa.q_bias, module.blocks.17.sa.v_bias, module.blocks.17.sa.proj.bias, module.blocks.17.ca.v_bias, '\n"
                    " 'module.blocks.17.ca.mat_q.bias, module.blocks.17.ca.proj.bias, module.blocks.17.ffn.fc1.bias, module.blocks.17.ffn.fc2.bias, module.blocks.17.ca_norm.bias, module.blocks.18.ada_gss, '\n"
                    " 'module.blocks.18.sa.scale_mul_1H11, module.blocks.18.sa.q_bias, module.blocks.18.sa.v_bias, module.blocks.18.sa.proj.bias, module.blocks.18.ca.v_bias, module.blocks.18.ca.mat_q.bias, '\n"
                    " 'module.blocks.18.ca.proj.bias, module.blocks.18.ffn.fc1.bias, module.blocks.18.ffn.fc2.bias, module.blocks.18.ca_norm.bias, module.blocks.19.ada_gss, module.blocks.19.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.19.sa.q_bias, module.blocks.19.sa.v_bias, module.blocks.19.sa.proj.bias, module.blocks.19.ca.v_bias, module.blocks.19.ca.mat_q.bias, module.blocks.19.ca.proj.bias, '\n"
                    " 'module.blocks.19.ffn.fc1.bias, module.blocks.19.ffn.fc2.bias, module.blocks.19.ca_norm.bias, module.blocks.20.ada_gss, module.blocks.20.sa.scale_mul_1H11, module.blocks.20.sa.q_bias, '\n"
                    " 'module.blocks.20.sa.v_bias, module.blocks.20.sa.proj.bias, module.blocks.20.ca.v_bias, module.blocks.20.ca.mat_q.bias, module.blocks.20.ca.proj.bias, module.blocks.20.ffn.fc1.bias, '\n"
                    " 'module.blocks.20.ffn.fc2.bias, module.blocks.20.ca_norm.bias, module.blocks.21.ada_gss, module.blocks.21.sa.scale_mul_1H11, module.blocks.21.sa.q_bias, module.blocks.21.sa.v_bias, '\n"
                    " 'module.blocks.21.sa.proj.bias, module.blocks.21.ca.v_bias, module.blocks.21.ca.mat_q.bias, module.blocks.21.ca.proj.bias, module.blocks.21.ffn.fc1.bias, module.blocks.21.ffn.fc2.bias, '\n"
                    " 'module.blocks.21.ca_norm.bias, module.blocks.22.ada_gss, module.blocks.22.sa.scale_mul_1H11, module.blocks.22.sa.q_bias, module.blocks.22.sa.v_bias, module.blocks.22.sa.proj.bias, '\n"
                    " 'module.blocks.22.ca.v_bias, module.blocks.22.ca.mat_q.bias, module.blocks.22.ca.proj.bias, module.blocks.22.ffn.fc1.bias, module.blocks.22.ffn.fc2.bias, module.blocks.22.ca_norm.bias, '\n"
                    " 'module.blocks.23.ada_gss, module.blocks.23.sa.scale_mul_1H11, module.blocks.23.sa.q_bias, module.blocks.23.sa.v_bias, module.blocks.23.sa.proj.bias, module.blocks.23.ca.v_bias, '\n"
                    " 'module.blocks.23.ca.mat_q.bias, module.blocks.23.ca.proj.bias, module.blocks.23.ffn.fc1.bias, module.blocks.23.ffn.fc2.bias, module.blocks.23.ca_norm.bias, module.blocks.24.ada_gss, '\n"
                    " 'module.blocks.24.sa.scale_mul_1H11, module.blocks.24.sa.q_bias, module.blocks.24.sa.v_bias, module.blocks.24.sa.proj.bias, module.blocks.24.ca.v_bias, module.blocks.24.ca.mat_q.bias, '\n"
                    " 'module.blocks.24.ca.proj.bias, module.blocks.24.ffn.fc1.bias, module.blocks.24.ffn.fc2.bias, module.blocks.24.ca_norm.bias, module.blocks.25.ada_gss, module.blocks.25.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.25.sa.q_bias, module.blocks.25.sa.v_bias, module.blocks.25.sa.proj.bias, module.blocks.25.ca.v_bias, module.blocks.25.ca.mat_q.bias, module.blocks.25.ca.proj.bias, '\n"
                    " 'module.blocks.25.ffn.fc1.bias, module.blocks.25.ffn.fc2.bias, module.blocks.25.ca_norm.bias, module.blocks.26.ada_gss, module.blocks.26.sa.scale_mul_1H11, module.blocks.26.sa.q_bias, '\n"
                    " 'module.blocks.26.sa.v_bias, module.blocks.26.sa.proj.bias, module.blocks.26.ca.v_bias, module.blocks.26.ca.mat_q.bias, module.blocks.26.ca.proj.bias, module.blocks.26.ffn.fc1.bias, '\n"
                    " 'module.blocks.26.ffn.fc2.bias, module.blocks.26.ca_norm.bias, module.blocks.27.ada_gss, module.blocks.27.sa.scale_mul_1H11, module.blocks.27.sa.q_bias, module.blocks.27.sa.v_bias, '\n"
                    " 'module.blocks.27.sa.proj.bias, module.blocks.27.ca.v_bias, module.blocks.27.ca.mat_q.bias, module.blocks.27.ca.proj.bias, module.blocks.27.ffn.fc1.bias, module.blocks.27.ffn.fc2.bias, '\n"
                    " 'module.blocks.27.ca_norm.bias, module.blocks.28.ada_gss, module.blocks.28.sa.scale_mul_1H11, module.blocks.28.sa.q_bias, module.blocks.28.sa.v_bias, module.blocks.28.sa.proj.bias, '\n"
                    " 'module.blocks.28.ca.v_bias, module.blocks.28.ca.mat_q.bias, module.blocks.28.ca.proj.bias, module.blocks.28.ffn.fc1.bias, module.blocks.28.ffn.fc2.bias, module.blocks.28.ca_norm.bias, '\n"
                    " 'module.blocks.29.ada_gss, module.blocks.29.sa.scale_mul_1H11, module.blocks.29.sa.q_bias, module.blocks.29.sa.v_bias, module.blocks.29.sa.proj.bias, module.blocks.29.ca.v_bias, '\n"
                    " 'module.blocks.29.ca.mat_q.bias, module.blocks.29.ca.proj.bias, module.blocks.29.ffn.fc1.bias, module.blocks.29.ffn.fc2.bias, module.blocks.29.ca_norm.bias, module.blocks.30.ada_gss, '\n"
                    " 'module.blocks.30.sa.scale_mul_1H11, module.blocks.30.sa.q_bias, module.blocks.30.sa.v_bias, module.blocks.30.sa.proj.bias, module.blocks.30.ca.v_bias, module.blocks.30.ca.mat_q.bias, '\n"
                    " 'module.blocks.30.ca.proj.bias, module.blocks.30.ffn.fc1.bias, module.blocks.30.ffn.fc2.bias, module.blocks.30.ca_norm.bias, module.blocks.31.ada_gss, module.blocks.31.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.31.sa.q_bias, module.blocks.31.sa.v_bias, module.blocks.31.sa.proj.bias, module.blocks.31.ca.v_bias, module.blocks.31.ca.mat_q.bias, module.blocks.31.ca.proj.bias, '\n"
                    " 'module.blocks.31.ffn.fc1.bias, module.blocks.31.ffn.fc2.bias, module.blocks.31.ca_norm.bias, module.blocks.32.ada_gss, module.blocks.32.sa.scale_mul_1H11, module.blocks.32.sa.q_bias, '\n"
                    " 'module.blocks.32.sa.v_bias, module.blocks.32.sa.proj.bias, module.blocks.32.ca.v_bias, module.blocks.32.ca.mat_q.bias, module.blocks.32.ca.proj.bias, module.blocks.32.ffn.fc1.bias, '\n"
                    " 'module.blocks.32.ffn.fc2.bias, module.blocks.32.ca_norm.bias, module.blocks.33.ada_gss, module.blocks.33.sa.scale_mul_1H11, module.blocks.33.sa.q_bias, module.blocks.33.sa.v_bias, '\n"
                    " 'module.blocks.33.sa.proj.bias, module.blocks.33.ca.v_bias, module.blocks.33.ca.mat_q.bias, module.blocks.33.ca.proj.bias, module.blocks.33.ffn.fc1.bias, module.blocks.33.ffn.fc2.bias, '\n"
                    " 'module.blocks.33.ca_norm.bias, module.blocks.34.ada_gss, module.blocks.34.sa.scale_mul_1H11, module.blocks.34.sa.q_bias, module.blocks.34.sa.v_bias, module.blocks.34.sa.proj.bias, '\n"
                    " 'module.blocks.34.ca.v_bias, module.blocks.34.ca.mat_q.bias, module.blocks.34.ca.proj.bias, module.blocks.34.ffn.fc1.bias, module.blocks.34.ffn.fc2.bias, module.blocks.34.ca_norm.bias, '\n"
                    " 'module.blocks.35.ada_gss, module.blocks.35.sa.scale_mul_1H11, module.blocks.35.sa.q_bias, module.blocks.35.sa.v_bias, module.blocks.35.sa.proj.bias, module.blocks.35.ca.v_bias, '\n"
                    " 'module.blocks.35.ca.mat_q.bias, module.blocks.35.ca.proj.bias, module.blocks.35.ffn.fc1.bias, module.blocks.35.ffn.fc2.bias, module.blocks.35.ca_norm.bias, module.blocks.36.ada_gss, '\n"
                    " 'module.blocks.36.sa.scale_mul_1H11, module.blocks.36.sa.q_bias, module.blocks.36.sa.v_bias, module.blocks.36.sa.proj.bias, module.blocks.36.ca.v_bias, module.blocks.36.ca.mat_q.bias, '\n"
                    " 'module.blocks.36.ca.proj.bias, module.blocks.36.ffn.fc1.bias, module.blocks.36.ffn.fc2.bias, module.blocks.36.ca_norm.bias, module.blocks.37.ada_gss, module.blocks.37.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.37.sa.q_bias, module.blocks.37.sa.v_bias, module.blocks.37.sa.proj.bias, module.blocks.37.ca.v_bias, module.blocks.37.ca.mat_q.bias, module.blocks.37.ca.proj.bias, '\n"
                    " 'module.blocks.37.ffn.fc1.bias, module.blocks.37.ffn.fc2.bias, module.blocks.37.ca_norm.bias, module.blocks.38.ada_gss, module.blocks.38.sa.scale_mul_1H11, module.blocks.38.sa.q_bias, '\n"
                    " 'module.blocks.38.sa.v_bias, module.blocks.38.sa.proj.bias, module.blocks.38.ca.v_bias, module.blocks.38.ca.mat_q.bias, module.blocks.38.ca.proj.bias, module.blocks.38.ffn.fc1.bias, '\n"
                    " 'module.blocks.38.ffn.fc2.bias, module.blocks.38.ca_norm.bias, module.blocks.39.ada_gss, module.blocks.39.sa.scale_mul_1H11, module.blocks.39.sa.q_bias, module.blocks.39.sa.v_bias, '\n"
                    " 'module.blocks.39.sa.proj.bias, module.blocks.39.ca.v_bias, module.blocks.39.ca.mat_q.bias, module.blocks.39.ca.proj.bias, module.blocks.39.ffn.fc1.bias, module.blocks.39.ffn.fc2.bias, '\n"
                    " 'module.blocks.39.ca_norm.bias, module.blocks.40.ada_gss, module.blocks.40.sa.scale_mul_1H11, module.blocks.40.sa.q_bias, module.blocks.40.sa.v_bias, module.blocks.40.sa.proj.bias, '\n"
                    " 'module.blocks.40.ca.v_bias, module.blocks.40.ca.mat_q.bias, module.blocks.40.ca.proj.bias, module.blocks.40.ffn.fc1.bias, module.blocks.40.ffn.fc2.bias, module.blocks.40.ca_norm.bias, '\n"
                    " 'module.blocks.41.ada_gss, module.blocks.41.sa.scale_mul_1H11, module.blocks.41.sa.q_bias, module.blocks.41.sa.v_bias, module.blocks.41.sa.proj.bias, module.blocks.41.ca.v_bias, '\n"
                    " 'module.blocks.41.ca.mat_q.bias, module.blocks.41.ca.proj.bias, module.blocks.41.ffn.fc1.bias, module.blocks.41.ffn.fc2.bias, module.blocks.41.ca_norm.bias, module.blocks.42.ada_gss, '\n"
                    " 'module.blocks.42.sa.scale_mul_1H11, module.blocks.42.sa.q_bias, module.blocks.42.sa.v_bias, module.blocks.42.sa.proj.bias, module.blocks.42.ca.v_bias, module.blocks.42.ca.mat_q.bias, '\n"
                    " 'module.blocks.42.ca.proj.bias, module.blocks.42.ffn.fc1.bias, module.blocks.42.ffn.fc2.bias, module.blocks.42.ca_norm.bias, module.blocks.43.ada_gss, module.blocks.43.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.43.sa.q_bias, module.blocks.43.sa.v_bias, module.blocks.43.sa.proj.bias, module.blocks.43.ca.v_bias, module.blocks.43.ca.mat_q.bias, module.blocks.43.ca.proj.bias, '\n"
                    " 'module.blocks.43.ffn.fc1.bias, module.blocks.43.ffn.fc2.bias, module.blocks.43.ca_norm.bias, module.blocks.44.ada_gss, module.blocks.44.sa.scale_mul_1H11, module.blocks.44.sa.q_bias, '\n"
                    " 'module.blocks.44.sa.v_bias, module.blocks.44.sa.proj.bias, module.blocks.44.ca.v_bias, module.blocks.44.ca.mat_q.bias, module.blocks.44.ca.proj.bias, module.blocks.44.ffn.fc1.bias, '\n"
                    " 'module.blocks.44.ffn.fc2.bias, module.blocks.44.ca_norm.bias, module.blocks.45.ada_gss, module.blocks.45.sa.scale_mul_1H11, module.blocks.45.sa.q_bias, module.blocks.45.sa.v_bias, '\n"
                    " 'module.blocks.45.sa.proj.bias, module.blocks.45.ca.v_bias, module.blocks.45.ca.mat_q.bias, module.blocks.45.ca.proj.bias, module.blocks.45.ffn.fc1.bias, module.blocks.45.ffn.fc2.bias, '\n"
                    " 'module.blocks.45.ca_norm.bias, module.blocks.46.ada_gss, module.blocks.46.sa.scale_mul_1H11, module.blocks.46.sa.q_bias, module.blocks.46.sa.v_bias, module.blocks.46.sa.proj.bias, '\n"
                    " 'module.blocks.46.ca.v_bias, module.blocks.46.ca.mat_q.bias, module.blocks.46.ca.proj.bias, module.blocks.46.ffn.fc1.bias, module.blocks.46.ffn.fc2.bias, module.blocks.46.ca_norm.bias, '\n"
                    " 'module.blocks.47.ada_gss, module.blocks.47.sa.scale_mul_1H11, module.blocks.47.sa.q_bias, module.blocks.47.sa.v_bias, module.blocks.47.sa.proj.bias, module.blocks.47.ca.v_bias, '\n"
                    " 'module.blocks.47.ca.mat_q.bias, module.blocks.47.ca.proj.bias, module.blocks.47.ffn.fc1.bias, module.blocks.47.ffn.fc2.bias, module.blocks.47.ca_norm.bias, module.blocks.48.ada_gss, '\n"
                    " 'module.blocks.48.sa.scale_mul_1H11, module.blocks.48.sa.q_bias, module.blocks.48.sa.v_bias, module.blocks.48.sa.proj.bias, module.blocks.48.ca.v_bias, module.blocks.48.ca.mat_q.bias, '\n"
                    " 'module.blocks.48.ca.proj.bias, module.blocks.48.ffn.fc1.bias, module.blocks.48.ffn.fc2.bias, module.blocks.48.ca_norm.bias, module.blocks.49.ada_gss, module.blocks.49.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.49.sa.q_bias, module.blocks.49.sa.v_bias, module.blocks.49.sa.proj.bias, module.blocks.49.ca.v_bias, module.blocks.49.ca.mat_q.bias, module.blocks.49.ca.proj.bias, '\n"
                    " 'module.blocks.49.ffn.fc1.bias, module.blocks.49.ffn.fc2.bias, module.blocks.49.ca_norm.bias, module.blocks.50.ada_gss, module.blocks.50.sa.scale_mul_1H11, module.blocks.50.sa.q_bias, '\n"
                    " 'module.blocks.50.sa.v_bias, module.blocks.50.sa.proj.bias, module.blocks.50.ca.v_bias, module.blocks.50.ca.mat_q.bias, module.blocks.50.ca.proj.bias, module.blocks.50.ffn.fc1.bias, '\n"
                    " 'module.blocks.50.ffn.fc2.bias, module.blocks.50.ca_norm.bias, module.blocks.51.ada_gss, module.blocks.51.sa.scale_mul_1H11, module.blocks.51.sa.q_bias, module.blocks.51.sa.v_bias, '\n"
                    " 'module.blocks.51.sa.proj.bias, module.blocks.51.ca.v_bias, module.blocks.51.ca.mat_q.bias, module.blocks.51.ca.proj.bias, module.blocks.51.ffn.fc1.bias, module.blocks.51.ffn.fc2.bias, '\n"
                    " 'module.blocks.51.ca_norm.bias, module.blocks.52.ada_gss, module.blocks.52.sa.scale_mul_1H11, module.blocks.52.sa.q_bias, module.blocks.52.sa.v_bias, module.blocks.52.sa.proj.bias, '\n"
                    " 'module.blocks.52.ca.v_bias, module.blocks.52.ca.mat_q.bias, module.blocks.52.ca.proj.bias, module.blocks.52.ffn.fc1.bias, module.blocks.52.ffn.fc2.bias, module.blocks.52.ca_norm.bias, '\n"
                    " 'module.blocks.53.ada_gss, module.blocks.53.sa.scale_mul_1H11, module.blocks.53.sa.q_bias, module.blocks.53.sa.v_bias, module.blocks.53.sa.proj.bias, module.blocks.53.ca.v_bias, '\n"
                    " 'module.blocks.53.ca.mat_q.bias, module.blocks.53.ca.proj.bias, module.blocks.53.ffn.fc1.bias, module.blocks.53.ffn.fc2.bias, module.blocks.53.ca_norm.bias, module.blocks.54.ada_gss, '\n"
                    " 'module.blocks.54.sa.scale_mul_1H11, module.blocks.54.sa.q_bias, module.blocks.54.sa.v_bias, module.blocks.54.sa.proj.bias, module.blocks.54.ca.v_bias, module.blocks.54.ca.mat_q.bias, '\n"
                    " 'module.blocks.54.ca.proj.bias, module.blocks.54.ffn.fc1.bias, module.blocks.54.ffn.fc2.bias, module.blocks.54.ca_norm.bias, module.blocks.55.ada_gss, module.blocks.55.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.55.sa.q_bias, module.blocks.55.sa.v_bias, module.blocks.55.sa.proj.bias, module.blocks.55.ca.v_bias, module.blocks.55.ca.mat_q.bias, module.blocks.55.ca.proj.bias, '\n"
                    " 'module.blocks.55.ffn.fc1.bias, module.blocks.55.ffn.fc2.bias, module.blocks.55.ca_norm.bias')",
          'wd_sc': 0.0}}

[09-06 23:44:59] (nity/utils/lr_control.py, line 116)=> [get_param_groups][rank0] type(model).__name__='NullDDP' count=1085, numel=8573892016
[09-06 23:44:59] (nity/utils/lr_control.py, line 117)=> 
[09-06 23:44:59] (inity_circuit_breaker.py, line 343)=> [vgpt] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.97), fused=True), opt_kw={'lr': 9.375e-05, 'weight_decay': 0}

[09-06 23:44:59] (inity_circuit_breaker.py, line 348)=> Loading T5 from google/flan-t5-xl...
[09-06 23:45:01] (_original/src/trainer.py, line  65)=> self.reweight_loss_by_scale: 1
[09-06 23:45:01] (inity_circuit_breaker.py, line 171)=> global bs=4, local bs=4
[09-06 23:45:01] (inity_circuit_breaker.py, line 172)=> initial args:
{
  local_out_path      : ./local_output
  data_path           : ./data
  bed                 : ./checkpoints
  vae_ckpt            : /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
  exp_name            : experiment
  ds                  : oi
  model               : infinity_8b
  short_cap_prob      : 0.2
  project_name        : infinity_circuit_breaker
  tf32                : True
  auto_resume         : True
  rush_resume         : 
  nowd                : 1
  enable_hybrid_shard : False
  inner_shard_degree  : 1
  zero                : 2
  buck                : chunk
  fsdp_orig           : True
  enable_checkpointing: full-block
  pad_to_multiplier   : 128
  log_every_iter      : False
  checkpoint_type     : torch
  seed                : None
  rand                : True
  task_id             : 123
  trial_id            : 123
  robust_run_id       : 0
  ckpt_trials         : []
  real_trial_id       : 123
  chunk_nodes         : 0
  is_master_node      : None
  log_txt_path        : ./local_output/log.txt
  t5_path             : google/flan-t5-xl
  online_t5           : True
  sdpa_mem            : True
  tfast               : 0
  model_alias         : 8b
  rms                 : False
  aln                 : 0.001
  alng                : 5e-06
  saln                : True
  haln                : True
  nm0                 : False
  tau                 : 1
  cos                 : True
  swi                 : False
  dp                  : -1
  drop                : 0.0
  hd                  : 0
  ca_gamma            : -1
  diva                : 1
  hd0                 : 0.02
  dec                 : 1
  cum                 : 3
  rwe                 : False
  tp                  : 0.0
  tk                  : 0.0
  tini                : 0.010416666666666666
  cfg                 : 0.1
  rand_uncond         : False
  ema                 : 0.9999
  tema                : 0
  fp16                : 2
  fuse                : False
  fused_norm          : False
  flash               : False
  xen                 : False
  use_flex_attn       : True
  stable              : False
  gblr                : 0.0001
  dblr                : 0.0001
  tblr                : 0.006
  glr                 : 1.5625e-06
  dlr                 : 1.5625e-06
  tlr                 : 9.375e-05
  gwd                 : 0.005
  dwd                 : 0.0005
  twd                 : 0.005
  gwde                : 0.005
  dwde                : 0.0005
  twde                : 0.005
  ls                  : 0.0
  lz                  : 0.0
  eq                  : 0
  ep                  : 100
  wp                  : 1e-08
  wp0                 : 0.005
  wpe                 : 1.0
  sche                : lin0
  log_freq            : 50
  gclip               : 6.0
  dclip               : 6.0
  tclip               : 5.0
  cdec                : False
  opt                 : adamw
  ada                 : 0.9_0.97
  dada                : 0.9_0.97
  oeps                : 0
  afuse               : True
  pn                  : 0.06M
  scale_schedule      : None
  patch_size          : None
  resos               : None
  data_load_reso      : None
  workers             : 4
  lbs                 : 4
  bs                  : 4
  batch_size          : 4
  glb_batch_size      : 4
  ac                  : 1
  r_accu              : 1.0
  norm_eps            : 1e-06
  tlen                : 512
  Ct5                 : 2048
  use_bit_label       : 1
  bitloss_type        : mean
  dynamic_resolution_across_gpus: 1
  enable_dynamic_length_prompt: 1
  use_streaming_dataset: 1
  iterable_data_buffersize: 30000
  save_model_iters_freq: 100
  noise_apply_layers  : 13
  noise_apply_strength: 0.3
  noise_apply_requant : 1
  rope2d_each_sa_layer: 1
  rope2d_normalized_by_hw: 2
  use_fsdp_model_ema  : 0
  add_lvl_embeding_only_first_block: 1
  reweight_loss_by_scale: 1
  always_training_scales: 100
  vae_type            : 14
  fake_vae_input      : False
  model_init_device   : cuda
  prefetch_factor     : 2
  apply_spatial_patchify: 1
  debug_bsc           : 0
  task_type           : t2i
  target_layers       : [10, 12, 14, 16, 18, 20]
  transform_layers    : [10, 12, 14, 16, 18, 20]
  lorra_alpha         : 5.0
  trainsets           : ['AlpacaSupervisedDataset', 'HarmfulDataset']
  valsets             : ['AlpacaSupervisedDataset', 'HarmfulDataset']
  adv_string          : Sure here's
  full_layers         : False
  harmful_prompts_path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
  sanitized_prompts_path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
  validation_ratio    : 0.1
  category            : hate
  lora_r              : 8
  lora_alpha          : 16
  lora_dropout        : 0.05
  lora_target_modules : ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
  lora_weight_path    : 
  lora_bias           : none
  q_lora              : False
  model_name_or_path  : meta-llama/Llama-2-7b-chat-hf
  adapter_name_or_path: 
  use_lora            : True
  cache_dir           : 
  optim               : adamw_torch
  model_max_length    : 512
  grouped_to_max_length: False
  use_refusal_retain  : True
  sc_train_subset     : ['']
  log_every           : 10
  sc_train_seq_type   : all_text
  coeff_schedule      : linear_converge
  sc_loss_type        : orig_act_dotprod
  branch              : master
  commit_id           : 
  commit_msg          : 
  cmd                 : --bed ./checkpoints --local_out_path ./local_output --project_name infinity_circuit_breaker --exp_name experiment --ep 100 --opt adamw --cum 3 --sche lin0 --fp16 2 --ada 0.9_0.97 --tini -1 --tclip 5 --flash 0 --alng 5e-06 --saln 1 --cos 1 --enable_checkpointing full-block --tblr 6e-3 --pn 0.06M --lbs 4 --workers 4 --short_cap_prob 0.2 --online_t5 1 --use_streaming_dataset 1 --iterable_data_buffersize 30000 --Ct5 2048 --t5_path google/flan-t5-xl --vae_type 14 --wp 0.00000001 --wpe 1 --dynamic_resolution_across_gpus 1 --enable_dynamic_length_prompt 1 --reweight_loss_by_scale 1 --add_lvl_embeding_only_first_block 1 --rope2d_each_sa_layer 1 --rope2d_normalized_by_hw 2 --use_fsdp_model_ema 0 --always_training_scales 100 --use_bit_label 1 --zero 2 --save_model_iters_freq 100 --log_freq 50 --checkpoint_type torch --prefetch_factor 2 --noise_apply_strength 0.3 --noise_apply_layers 13 --apply_spatial_patchify 1 --use_flex_attn True --pad 128 --data_path ./data --data_load_reso 512 --tlen 512 --workers 4 --prefetch_factor 2 --harmful_prompts_path /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts --sanitized_prompts_path /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts --validation_ratio 0.1 --category hate --target_layers 10,12,14,16,18,20 --transform_layers 10,12,14,16,18,20 --lorra_alpha 5.0 --trainsets AlpacaSupervisedDataset#HarmfulDataset --valsets AlpacaSupervisedDataset#HarmfulDataset --adv_string Sure here's --full_layers False --lora_r 8 --lora_alpha 16 --lora_dropout 0.05 --lora_target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj --lora_weight_path  --lora_bias none --q_lora False --model_name_or_path meta-llama/Llama-2-7b-chat-hf --adapter_name_or_path  --use_lora True --cache_dir  --optim adamw_torch --model_max_length 512 --grouped_to_max_length False --use_refusal_retain True --sc_train_subset  --log_every 10 --sc_train_seq_type all_text --coeff_schedule linear_converge --sc_loss_type orig_act_dotprod
  tag                 : UK
  acc_all             : None
  acc_real            : None
  acc_fake            : None
  last_Lnll           : None
  last_L1             : None
  last_Ld             : None
  last_wei_g          : None
  grad_boom           : None
  diff                : None
  diffs               : 
  diffs_ema           : None
  ca_performance      : 
  cur_phase           : 
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  iter_speed          : None
  img_per_day         : None
  max_nvidia_smi      : 0
  max_memory_allocated: None
  max_memory_reserved : None
  num_alloc_retries   : None
  MFU                 : None
  HFU                 : None
  dbg_modified        : False
  dbg_ks              : False
  dbg_ks_last         : None
  dbg                 : False
  ks                  : False
  nodata              : False
  nodata_tlen         : 320
  nova                : False
  prof                : 0
  prof_freq           : 50
  tos_profiler_file_prefix: vgpt_default/
  profall             : 0
  v_seed              : 0
  g_seed              : 0
}

[09-06 23:45:01] (inity_circuit_breaker.py, line 182)=> start_it=0, iters_train=100
[09-06 23:45:01] (inity_circuit_breaker.py, line 466)=> [PT info]  from ep0 it0, acc_str: [no acc str], diffs: ,    =======>  bed: ./checkpoints  <=======

[09-06 23:45:07] (twise_self_correction.py, line  36)=> [dbg] raw_features: torch.Size([2, 14, 64, 64])
[09-06 23:45:07] (twise_self_correction.py, line  41)=> [dbg] target last scale: (1, 32, 32)







=======================================================   RESTART [09-07 00:05:36]   =======================================================
[09-07 00:05:37] (inity_circuit_breaker.py, line 718)=> ============================================================
[09-07 00:05:37] (inity_circuit_breaker.py, line 719)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[09-07 00:05:37] (inity_circuit_breaker.py, line 720)=> Circuit Breaker Alpha: 0.1
[09-07 00:05:37] (inity_circuit_breaker.py, line 721)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[09-07 00:05:37] (inity_circuit_breaker.py, line 722)=> Circuit Breaker Enabled: True
[09-07 00:05:37] (inity_circuit_breaker.py, line 723)=> Number of Examples: 1000
[09-07 00:05:37] (inity_circuit_breaker.py, line 724)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[09-07 00:05:37] (inity_circuit_breaker.py, line 725)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[09-07 00:05:37] (inity_circuit_breaker.py, line 726)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[09-07 00:05:37] (inity_circuit_breaker.py, line 727)=> Validation Ratio: 0.1
[09-07 00:05:37] (inity_circuit_breaker.py, line 728)=> Category: hate
[09-07 00:05:37] (inity_circuit_breaker.py, line 729)=> Batch Size: 4
[09-07 00:05:37] (inity_circuit_breaker.py, line 730)=> Workers: 4
[09-07 00:05:37] (inity_circuit_breaker.py, line 731)=> Device: cuda:0
[09-07 00:05:37] (inity_circuit_breaker.py, line 732)=> Model: infinity_8b
[09-07 00:05:37] (inity_circuit_breaker.py, line 733)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-07 00:05:37] (inity_circuit_breaker.py, line 734)=> Rush Resume: 
[09-07 00:05:37] (inity_circuit_breaker.py, line 735)=> Selective Layers: 0,1,2,3,4,5
[09-07 00:05:37] (inity_circuit_breaker.py, line 736)=> PN Parameter: 0.06M
[09-07 00:05:37] (inity_circuit_breaker.py, line 737)=> ============================================================
[09-07 00:05:37] (inity_circuit_breaker.py, line 377)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[09-07 00:05:37] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-07 00:05:37] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-07 00:05:37] (rain_dataset_infinity.py, line 205)=> Split 1000 prompts: 900 retain, 100 validation
[09-07 00:05:37] (rain_dataset_infinity.py, line 236)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-07 00:05:37] (rain_dataset_infinity.py, line 245)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-07 00:05:37] (rain_dataset_infinity.py, line  84)=> Loaded 900 retain examples
[09-07 00:05:37] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[09-07 00:05:37] (rain_dataset_infinity.py, line  86)=> Loaded 100 validation examples
[09-07 00:05:37] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[09-07 00:05:37] (inity_circuit_breaker.py, line 395)=> args.batch_size=4, vbs=6
[09-07 00:05:37] (inity_circuit_breaker.py, line 418)=> len(dataloader): 100, len(dataset): 100
[09-07 00:05:37] (inity_circuit_breaker.py, line 420)=> total_samples: 200
[09-07 00:05:37] (inity_circuit_breaker.py, line 421)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[09-07 00:05:37] (inity_circuit_breaker.py, line 140)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[09-07 00:05:37] (inity_circuit_breaker.py, line 142)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[09-07 00:05:37] (inity_circuit_breaker.py, line 146)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-07 00:05:39] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[09-07 00:05:39] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[09-07 00:05:39] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[09-07 00:05:39] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[09-07 00:05:42] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[09-07 00:05:43] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[09-07 00:05:43] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[09-07 00:05:43] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[09-07 00:05:44] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[09-07 00:05:44] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

[09-07 00:05:54] (ity/models/init_param.py, line  15)=> [init_weights] Infinity with std=0.02
[09-07 00:05:55] (inity_circuit_breaker.py, line 246)=> [PT] GPT model = Infinity(
  drop_path_rate=0.2
  (norm0_cond): Identity()
  (text_norm): FastRMSNorm(C=2048, eps=1e-06, elementwise_affine=True)
  (text_proj_for_sos): TextAttentivePool(
    (ca): CrossAttention(
      Cq=3072, Ckv=2048, cos_attn=False
      (mat_kv): Linear(in_features=2048, out_features=6144, bias=False)
      (proj): Linear(in_features=3072, out_features=3072, bias=True)
      (proj_drop): Identity()
    )
  )
  (text_proj_for_ca): Sequential(
    (0): Linear(in_features=2048, out_features=3072, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=3072, out_features=3072, bias=True)
  )
  (lvl_embed): Embedding(15, 3072)
  (norm0_ve): Identity()
  (word_embed): Linear(in_features=56, out_features=3072, bias=True)
  (shared_ada_lin): Sequential(
    (0): SiLU()
    (1): SharedAdaLin(in_features=3072, out_features=18432, bias=True)
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=3072, out_features=6144, bias=True)
    )
  )
  (head): Linear(in_features=3072, out_features=112, bias=True)
  (blocks): ModuleList(
    (0): CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): Identity()
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
    (1-55): 55 x CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): DropPath(...)
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
  )
)


[09-07 00:05:55] (inity_circuit_breaker.py, line 248)=> [PT][#para] VAE=81.62, VAE.quant=0.00
[09-07 00:05:55] (inity_circuit_breaker.py, line 251)=> [PT][#para] GPT=8573.89


[09-07 00:05:55] (nity/utils/lr_control.py, line  71)=> [get_param_groups][lr decay] with_lr_scale=False, lr_scale=0.0
[09-07 00:05:55] (nity/utils/lr_control.py, line 111)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': '[no scale]',
         'params': "('module.text_norm.weight, module.text_proj_for_sos.ca.mat_kv.weight, module.text_proj_for_sos.ca.proj.weight, module.text_proj_for_ca.0.weight, module.text_proj_for_ca.2.weight, '\n"
                   " 'module.word_embed.weight, module.shared_ada_lin.1.weight, module.head_nm.ada_lin.1.weight, module.head.weight, module.blocks.0.sa.mat_qkv.weight, module.blocks.0.sa.proj.weight, '\n"
                   " 'module.blocks.0.ca.mat_q.weight, module.blocks.0.ca.mat_kv.weight, module.blocks.0.ca.proj.weight, module.blocks.0.ffn.fc1.weight, module.blocks.0.ffn.fc2.weight, module.blocks.0.ca_norm.weight, '\n"
                   " 'module.blocks.1.sa.mat_qkv.weight, module.blocks.1.sa.proj.weight, module.blocks.1.ca.mat_q.weight, module.blocks.1.ca.mat_kv.weight, module.blocks.1.ca.proj.weight, '\n"
                   " 'module.blocks.1.ffn.fc1.weight, module.blocks.1.ffn.fc2.weight, module.blocks.1.ca_norm.weight, module.blocks.2.sa.mat_qkv.weight, module.blocks.2.sa.proj.weight, module.blocks.2.ca.mat_q.weight, '\n"
                   " 'module.blocks.2.ca.mat_kv.weight, module.blocks.2.ca.proj.weight, module.blocks.2.ffn.fc1.weight, module.blocks.2.ffn.fc2.weight, module.blocks.2.ca_norm.weight, module.blocks.3.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.3.sa.proj.weight, module.blocks.3.ca.mat_q.weight, module.blocks.3.ca.mat_kv.weight, module.blocks.3.ca.proj.weight, module.blocks.3.ffn.fc1.weight, module.blocks.3.ffn.fc2.weight, '\n"
                   " 'module.blocks.3.ca_norm.weight, module.blocks.4.sa.mat_qkv.weight, module.blocks.4.sa.proj.weight, module.blocks.4.ca.mat_q.weight, module.blocks.4.ca.mat_kv.weight, '\n"
                   " 'module.blocks.4.ca.proj.weight, module.blocks.4.ffn.fc1.weight, module.blocks.4.ffn.fc2.weight, module.blocks.4.ca_norm.weight, module.blocks.5.sa.mat_qkv.weight, module.blocks.5.sa.proj.weight, '\n"
                   " 'module.blocks.5.ca.mat_q.weight, module.blocks.5.ca.mat_kv.weight, module.blocks.5.ca.proj.weight, module.blocks.5.ffn.fc1.weight, module.blocks.5.ffn.fc2.weight, module.blocks.5.ca_norm.weight, '\n"
                   " 'module.blocks.6.sa.mat_qkv.weight, module.blocks.6.sa.proj.weight, module.blocks.6.ca.mat_q.weight, module.blocks.6.ca.mat_kv.weight, module.blocks.6.ca.proj.weight, '\n"
                   " 'module.blocks.6.ffn.fc1.weight, module.blocks.6.ffn.fc2.weight, module.blocks.6.ca_norm.weight, module.blocks.7.sa.mat_qkv.weight, module.blocks.7.sa.proj.weight, module.blocks.7.ca.mat_q.weight, '\n"
                   " 'module.blocks.7.ca.mat_kv.weight, module.blocks.7.ca.proj.weight, module.blocks.7.ffn.fc1.weight, module.blocks.7.ffn.fc2.weight, module.blocks.7.ca_norm.weight, module.blocks.8.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.8.sa.proj.weight, module.blocks.8.ca.mat_q.weight, module.blocks.8.ca.mat_kv.weight, module.blocks.8.ca.proj.weight, module.blocks.8.ffn.fc1.weight, module.blocks.8.ffn.fc2.weight, '\n"
                   " 'module.blocks.8.ca_norm.weight, module.blocks.9.sa.mat_qkv.weight, module.blocks.9.sa.proj.weight, module.blocks.9.ca.mat_q.weight, module.blocks.9.ca.mat_kv.weight, '\n"
                   " 'module.blocks.9.ca.proj.weight, module.blocks.9.ffn.fc1.weight, module.blocks.9.ffn.fc2.weight, module.blocks.9.ca_norm.weight, module.blocks.10.sa.mat_qkv.weight, module.blocks.10.sa.proj.weight, '\n"
                   " 'module.blocks.10.ca.mat_q.weight, module.blocks.10.ca.mat_kv.weight, module.blocks.10.ca.proj.weight, module.blocks.10.ffn.fc1.weight, module.blocks.10.ffn.fc2.weight, '\n"
                   " 'module.blocks.10.ca_norm.weight, module.blocks.11.sa.mat_qkv.weight, module.blocks.11.sa.proj.weight, module.blocks.11.ca.mat_q.weight, module.blocks.11.ca.mat_kv.weight, '\n"
                   " 'module.blocks.11.ca.proj.weight, module.blocks.11.ffn.fc1.weight, module.blocks.11.ffn.fc2.weight, module.blocks.11.ca_norm.weight, module.blocks.12.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.12.sa.proj.weight, module.blocks.12.ca.mat_q.weight, module.blocks.12.ca.mat_kv.weight, module.blocks.12.ca.proj.weight, module.blocks.12.ffn.fc1.weight, '\n"
                   " 'module.blocks.12.ffn.fc2.weight, module.blocks.12.ca_norm.weight, module.blocks.13.sa.mat_qkv.weight, module.blocks.13.sa.proj.weight, module.blocks.13.ca.mat_q.weight, '\n"
                   " 'module.blocks.13.ca.mat_kv.weight, module.blocks.13.ca.proj.weight, module.blocks.13.ffn.fc1.weight, module.blocks.13.ffn.fc2.weight, module.blocks.13.ca_norm.weight, '\n"
                   " 'module.blocks.14.sa.mat_qkv.weight, module.blocks.14.sa.proj.weight, module.blocks.14.ca.mat_q.weight, module.blocks.14.ca.mat_kv.weight, module.blocks.14.ca.proj.weight, '\n"
                   " 'module.blocks.14.ffn.fc1.weight, module.blocks.14.ffn.fc2.weight, module.blocks.14.ca_norm.weight, module.blocks.15.sa.mat_qkv.weight, module.blocks.15.sa.proj.weight, '\n"
                   " 'module.blocks.15.ca.mat_q.weight, module.blocks.15.ca.mat_kv.weight, module.blocks.15.ca.proj.weight, module.blocks.15.ffn.fc1.weight, module.blocks.15.ffn.fc2.weight, '\n"
                   " 'module.blocks.15.ca_norm.weight, module.blocks.16.sa.mat_qkv.weight, module.blocks.16.sa.proj.weight, module.blocks.16.ca.mat_q.weight, module.blocks.16.ca.mat_kv.weight, '\n"
                   " 'module.blocks.16.ca.proj.weight, module.blocks.16.ffn.fc1.weight, module.blocks.16.ffn.fc2.weight, module.blocks.16.ca_norm.weight, module.blocks.17.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.17.sa.proj.weight, module.blocks.17.ca.mat_q.weight, module.blocks.17.ca.mat_kv.weight, module.blocks.17.ca.proj.weight, module.blocks.17.ffn.fc1.weight, '\n"
                   " 'module.blocks.17.ffn.fc2.weight, module.blocks.17.ca_norm.weight, module.blocks.18.sa.mat_qkv.weight, module.blocks.18.sa.proj.weight, module.blocks.18.ca.mat_q.weight, '\n"
                   " 'module.blocks.18.ca.mat_kv.weight, module.blocks.18.ca.proj.weight, module.blocks.18.ffn.fc1.weight, module.blocks.18.ffn.fc2.weight, module.blocks.18.ca_norm.weight, '\n"
                   " 'module.blocks.19.sa.mat_qkv.weight, module.blocks.19.sa.proj.weight, module.blocks.19.ca.mat_q.weight, module.blocks.19.ca.mat_kv.weight, module.blocks.19.ca.proj.weight, '\n"
                   " 'module.blocks.19.ffn.fc1.weight, module.blocks.19.ffn.fc2.weight, module.blocks.19.ca_norm.weight, module.blocks.20.sa.mat_qkv.weight, module.blocks.20.sa.proj.weight, '\n"
                   " 'module.blocks.20.ca.mat_q.weight, module.blocks.20.ca.mat_kv.weight, module.blocks.20.ca.proj.weight, module.blocks.20.ffn.fc1.weight, module.blocks.20.ffn.fc2.weight, '\n"
                   " 'module.blocks.20.ca_norm.weight, module.blocks.21.sa.mat_qkv.weight, module.blocks.21.sa.proj.weight, module.blocks.21.ca.mat_q.weight, module.blocks.21.ca.mat_kv.weight, '\n"
                   " 'module.blocks.21.ca.proj.weight, module.blocks.21.ffn.fc1.weight, module.blocks.21.ffn.fc2.weight, module.blocks.21.ca_norm.weight, module.blocks.22.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.22.sa.proj.weight, module.blocks.22.ca.mat_q.weight, module.blocks.22.ca.mat_kv.weight, module.blocks.22.ca.proj.weight, module.blocks.22.ffn.fc1.weight, '\n"
                   " 'module.blocks.22.ffn.fc2.weight, module.blocks.22.ca_norm.weight, module.blocks.23.sa.mat_qkv.weight, module.blocks.23.sa.proj.weight, module.blocks.23.ca.mat_q.weight, '\n"
                   " 'module.blocks.23.ca.mat_kv.weight, module.blocks.23.ca.proj.weight, module.blocks.23.ffn.fc1.weight, module.blocks.23.ffn.fc2.weight, module.blocks.23.ca_norm.weight, '\n"
                   " 'module.blocks.24.sa.mat_qkv.weight, module.blocks.24.sa.proj.weight, module.blocks.24.ca.mat_q.weight, module.blocks.24.ca.mat_kv.weight, module.blocks.24.ca.proj.weight, '\n"
                   " 'module.blocks.24.ffn.fc1.weight, module.blocks.24.ffn.fc2.weight, module.blocks.24.ca_norm.weight, module.blocks.25.sa.mat_qkv.weight, module.blocks.25.sa.proj.weight, '\n"
                   " 'module.blocks.25.ca.mat_q.weight, module.blocks.25.ca.mat_kv.weight, module.blocks.25.ca.proj.weight, module.blocks.25.ffn.fc1.weight, module.blocks.25.ffn.fc2.weight, '\n"
                   " 'module.blocks.25.ca_norm.weight, module.blocks.26.sa.mat_qkv.weight, module.blocks.26.sa.proj.weight, module.blocks.26.ca.mat_q.weight, module.blocks.26.ca.mat_kv.weight, '\n"
                   " 'module.blocks.26.ca.proj.weight, module.blocks.26.ffn.fc1.weight, module.blocks.26.ffn.fc2.weight, module.blocks.26.ca_norm.weight, module.blocks.27.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.27.sa.proj.weight, module.blocks.27.ca.mat_q.weight, module.blocks.27.ca.mat_kv.weight, module.blocks.27.ca.proj.weight, module.blocks.27.ffn.fc1.weight, '\n"
                   " 'module.blocks.27.ffn.fc2.weight, module.blocks.27.ca_norm.weight, module.blocks.28.sa.mat_qkv.weight, module.blocks.28.sa.proj.weight, module.blocks.28.ca.mat_q.weight, '\n"
                   " 'module.blocks.28.ca.mat_kv.weight, module.blocks.28.ca.proj.weight, module.blocks.28.ffn.fc1.weight, module.blocks.28.ffn.fc2.weight, module.blocks.28.ca_norm.weight, '\n"
                   " 'module.blocks.29.sa.mat_qkv.weight, module.blocks.29.sa.proj.weight, module.blocks.29.ca.mat_q.weight, module.blocks.29.ca.mat_kv.weight, module.blocks.29.ca.proj.weight, '\n"
                   " 'module.blocks.29.ffn.fc1.weight, module.blocks.29.ffn.fc2.weight, module.blocks.29.ca_norm.weight, module.blocks.30.sa.mat_qkv.weight, module.blocks.30.sa.proj.weight, '\n"
                   " 'module.blocks.30.ca.mat_q.weight, module.blocks.30.ca.mat_kv.weight, module.blocks.30.ca.proj.weight, module.blocks.30.ffn.fc1.weight, module.blocks.30.ffn.fc2.weight, '\n"
                   " 'module.blocks.30.ca_norm.weight, module.blocks.31.sa.mat_qkv.weight, module.blocks.31.sa.proj.weight, module.blocks.31.ca.mat_q.weight, module.blocks.31.ca.mat_kv.weight, '\n"
                   " 'module.blocks.31.ca.proj.weight, module.blocks.31.ffn.fc1.weight, module.blocks.31.ffn.fc2.weight, module.blocks.31.ca_norm.weight, module.blocks.32.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.32.sa.proj.weight, module.blocks.32.ca.mat_q.weight, module.blocks.32.ca.mat_kv.weight, module.blocks.32.ca.proj.weight, module.blocks.32.ffn.fc1.weight, '\n"
                   " 'module.blocks.32.ffn.fc2.weight, module.blocks.32.ca_norm.weight, module.blocks.33.sa.mat_qkv.weight, module.blocks.33.sa.proj.weight, module.blocks.33.ca.mat_q.weight, '\n"
                   " 'module.blocks.33.ca.mat_kv.weight, module.blocks.33.ca.proj.weight, module.blocks.33.ffn.fc1.weight, module.blocks.33.ffn.fc2.weight, module.blocks.33.ca_norm.weight, '\n"
                   " 'module.blocks.34.sa.mat_qkv.weight, module.blocks.34.sa.proj.weight, module.blocks.34.ca.mat_q.weight, module.blocks.34.ca.mat_kv.weight, module.blocks.34.ca.proj.weight, '\n"
                   " 'module.blocks.34.ffn.fc1.weight, module.blocks.34.ffn.fc2.weight, module.blocks.34.ca_norm.weight, module.blocks.35.sa.mat_qkv.weight, module.blocks.35.sa.proj.weight, '\n"
                   " 'module.blocks.35.ca.mat_q.weight, module.blocks.35.ca.mat_kv.weight, module.blocks.35.ca.proj.weight, module.blocks.35.ffn.fc1.weight, module.blocks.35.ffn.fc2.weight, '\n"
                   " 'module.blocks.35.ca_norm.weight, module.blocks.36.sa.mat_qkv.weight, module.blocks.36.sa.proj.weight, module.blocks.36.ca.mat_q.weight, module.blocks.36.ca.mat_kv.weight, '\n"
                   " 'module.blocks.36.ca.proj.weight, module.blocks.36.ffn.fc1.weight, module.blocks.36.ffn.fc2.weight, module.blocks.36.ca_norm.weight, module.blocks.37.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.37.sa.proj.weight, module.blocks.37.ca.mat_q.weight, module.blocks.37.ca.mat_kv.weight, module.blocks.37.ca.proj.weight, module.blocks.37.ffn.fc1.weight, '\n"
                   " 'module.blocks.37.ffn.fc2.weight, module.blocks.37.ca_norm.weight, module.blocks.38.sa.mat_qkv.weight, module.blocks.38.sa.proj.weight, module.blocks.38.ca.mat_q.weight, '\n"
                   " 'module.blocks.38.ca.mat_kv.weight, module.blocks.38.ca.proj.weight, module.blocks.38.ffn.fc1.weight, module.blocks.38.ffn.fc2.weight, module.blocks.38.ca_norm.weight, '\n"
                   " 'module.blocks.39.sa.mat_qkv.weight, module.blocks.39.sa.proj.weight, module.blocks.39.ca.mat_q.weight, module.blocks.39.ca.mat_kv.weight, module.blocks.39.ca.proj.weight, '\n"
                   " 'module.blocks.39.ffn.fc1.weight, module.blocks.39.ffn.fc2.weight, module.blocks.39.ca_norm.weight, module.blocks.40.sa.mat_qkv.weight, module.blocks.40.sa.proj.weight, '\n"
                   " 'module.blocks.40.ca.mat_q.weight, module.blocks.40.ca.mat_kv.weight, module.blocks.40.ca.proj.weight, module.blocks.40.ffn.fc1.weight, module.blocks.40.ffn.fc2.weight, '\n"
                   " 'module.blocks.40.ca_norm.weight, module.blocks.41.sa.mat_qkv.weight, module.blocks.41.sa.proj.weight, module.blocks.41.ca.mat_q.weight, module.blocks.41.ca.mat_kv.weight, '\n"
                   " 'module.blocks.41.ca.proj.weight, module.blocks.41.ffn.fc1.weight, module.blocks.41.ffn.fc2.weight, module.blocks.41.ca_norm.weight, module.blocks.42.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.42.sa.proj.weight, module.blocks.42.ca.mat_q.weight, module.blocks.42.ca.mat_kv.weight, module.blocks.42.ca.proj.weight, module.blocks.42.ffn.fc1.weight, '\n"
                   " 'module.blocks.42.ffn.fc2.weight, module.blocks.42.ca_norm.weight, module.blocks.43.sa.mat_qkv.weight, module.blocks.43.sa.proj.weight, module.blocks.43.ca.mat_q.weight, '\n"
                   " 'module.blocks.43.ca.mat_kv.weight, module.blocks.43.ca.proj.weight, module.blocks.43.ffn.fc1.weight, module.blocks.43.ffn.fc2.weight, module.blocks.43.ca_norm.weight, '\n"
                   " 'module.blocks.44.sa.mat_qkv.weight, module.blocks.44.sa.proj.weight, module.blocks.44.ca.mat_q.weight, module.blocks.44.ca.mat_kv.weight, module.blocks.44.ca.proj.weight, '\n"
                   " 'module.blocks.44.ffn.fc1.weight, module.blocks.44.ffn.fc2.weight, module.blocks.44.ca_norm.weight, module.blocks.45.sa.mat_qkv.weight, module.blocks.45.sa.proj.weight, '\n"
                   " 'module.blocks.45.ca.mat_q.weight, module.blocks.45.ca.mat_kv.weight, module.blocks.45.ca.proj.weight, module.blocks.45.ffn.fc1.weight, module.blocks.45.ffn.fc2.weight, '\n"
                   " 'module.blocks.45.ca_norm.weight, module.blocks.46.sa.mat_qkv.weight, module.blocks.46.sa.proj.weight, module.blocks.46.ca.mat_q.weight, module.blocks.46.ca.mat_kv.weight, '\n"
                   " 'module.blocks.46.ca.proj.weight, module.blocks.46.ffn.fc1.weight, module.blocks.46.ffn.fc2.weight, module.blocks.46.ca_norm.weight, module.blocks.47.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.47.sa.proj.weight, module.blocks.47.ca.mat_q.weight, module.blocks.47.ca.mat_kv.weight, module.blocks.47.ca.proj.weight, module.blocks.47.ffn.fc1.weight, '\n"
                   " 'module.blocks.47.ffn.fc2.weight, module.blocks.47.ca_norm.weight, module.blocks.48.sa.mat_qkv.weight, module.blocks.48.sa.proj.weight, module.blocks.48.ca.mat_q.weight, '\n"
                   " 'module.blocks.48.ca.mat_kv.weight, module.blocks.48.ca.proj.weight, module.blocks.48.ffn.fc1.weight, module.blocks.48.ffn.fc2.weight, module.blocks.48.ca_norm.weight, '\n"
                   " 'module.blocks.49.sa.mat_qkv.weight, module.blocks.49.sa.proj.weight, module.blocks.49.ca.mat_q.weight, module.blocks.49.ca.mat_kv.weight, module.blocks.49.ca.proj.weight, '\n"
                   " 'module.blocks.49.ffn.fc1.weight, module.blocks.49.ffn.fc2.weight, module.blocks.49.ca_norm.weight, module.blocks.50.sa.mat_qkv.weight, module.blocks.50.sa.proj.weight, '\n"
                   " 'module.blocks.50.ca.mat_q.weight, module.blocks.50.ca.mat_kv.weight, module.blocks.50.ca.proj.weight, module.blocks.50.ffn.fc1.weight, module.blocks.50.ffn.fc2.weight, '\n"
                   " 'module.blocks.50.ca_norm.weight, module.blocks.51.sa.mat_qkv.weight, module.blocks.51.sa.proj.weight, module.blocks.51.ca.mat_q.weight, module.blocks.51.ca.mat_kv.weight, '\n"
                   " 'module.blocks.51.ca.proj.weight, module.blocks.51.ffn.fc1.weight, module.blocks.51.ffn.fc2.weight, module.blocks.51.ca_norm.weight, module.blocks.52.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.52.sa.proj.weight, module.blocks.52.ca.mat_q.weight, module.blocks.52.ca.mat_kv.weight, module.blocks.52.ca.proj.weight, module.blocks.52.ffn.fc1.weight, '\n"
                   " 'module.blocks.52.ffn.fc2.weight, module.blocks.52.ca_norm.weight, module.blocks.53.sa.mat_qkv.weight, module.blocks.53.sa.proj.weight, module.blocks.53.ca.mat_q.weight, '\n"
                   " 'module.blocks.53.ca.mat_kv.weight, module.blocks.53.ca.proj.weight, module.blocks.53.ffn.fc1.weight, module.blocks.53.ffn.fc2.weight, module.blocks.53.ca_norm.weight, '\n"
                   " 'module.blocks.54.sa.mat_qkv.weight, module.blocks.54.sa.proj.weight, module.blocks.54.ca.mat_q.weight, module.blocks.54.ca.mat_kv.weight, module.blocks.54.ca.proj.weight, '\n"
                   " 'module.blocks.54.ffn.fc1.weight, module.blocks.54.ffn.fc2.weight, module.blocks.54.ca_norm.weight, module.blocks.55.sa.mat_qkv.weight, module.blocks.55.sa.proj.weight, '\n"
                   " 'module.blocks.55.ca.mat_q.weight, module.blocks.55.ca.mat_kv.weight, module.blocks.55.ca.proj.weight, module.blocks.55.ffn.fc1.weight, module.blocks.55.ffn.fc2.weight, '\n"
                   " 'module.blocks.55.ca_norm.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': '[no scale]',
          'params': "('module.cfg_uncond, module.pos_start, module.text_proj_for_sos.ca.mat_q, module.text_proj_for_sos.ca.v_bias, module.text_proj_for_sos.ca.proj.bias, module.text_proj_for_ca.0.bias, '\n"
                    " 'module.text_proj_for_ca.2.bias, module.lvl_embed.weight, module.word_embed.bias, module.shared_ada_lin.1.bias, module.head_nm.ada_lin.1.bias, module.head.bias, module.blocks.0.ada_gss, '\n"
                    " 'module.blocks.0.sa.scale_mul_1H11, module.blocks.0.sa.q_bias, module.blocks.0.sa.v_bias, module.blocks.0.sa.proj.bias, module.blocks.0.ca.v_bias, module.blocks.0.ca.mat_q.bias, '\n"
                    " 'module.blocks.0.ca.proj.bias, module.blocks.0.ffn.fc1.bias, module.blocks.0.ffn.fc2.bias, module.blocks.0.ca_norm.bias, module.blocks.1.ada_gss, module.blocks.1.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.1.sa.q_bias, module.blocks.1.sa.v_bias, module.blocks.1.sa.proj.bias, module.blocks.1.ca.v_bias, module.blocks.1.ca.mat_q.bias, module.blocks.1.ca.proj.bias, '\n"
                    " 'module.blocks.1.ffn.fc1.bias, module.blocks.1.ffn.fc2.bias, module.blocks.1.ca_norm.bias, module.blocks.2.ada_gss, module.blocks.2.sa.scale_mul_1H11, module.blocks.2.sa.q_bias, '\n"
                    " 'module.blocks.2.sa.v_bias, module.blocks.2.sa.proj.bias, module.blocks.2.ca.v_bias, module.blocks.2.ca.mat_q.bias, module.blocks.2.ca.proj.bias, module.blocks.2.ffn.fc1.bias, '\n"
                    " 'module.blocks.2.ffn.fc2.bias, module.blocks.2.ca_norm.bias, module.blocks.3.ada_gss, module.blocks.3.sa.scale_mul_1H11, module.blocks.3.sa.q_bias, module.blocks.3.sa.v_bias, '\n"
                    " 'module.blocks.3.sa.proj.bias, module.blocks.3.ca.v_bias, module.blocks.3.ca.mat_q.bias, module.blocks.3.ca.proj.bias, module.blocks.3.ffn.fc1.bias, module.blocks.3.ffn.fc2.bias, '\n"
                    " 'module.blocks.3.ca_norm.bias, module.blocks.4.ada_gss, module.blocks.4.sa.scale_mul_1H11, module.blocks.4.sa.q_bias, module.blocks.4.sa.v_bias, module.blocks.4.sa.proj.bias, '\n"
                    " 'module.blocks.4.ca.v_bias, module.blocks.4.ca.mat_q.bias, module.blocks.4.ca.proj.bias, module.blocks.4.ffn.fc1.bias, module.blocks.4.ffn.fc2.bias, module.blocks.4.ca_norm.bias, '\n"
                    " 'module.blocks.5.ada_gss, module.blocks.5.sa.scale_mul_1H11, module.blocks.5.sa.q_bias, module.blocks.5.sa.v_bias, module.blocks.5.sa.proj.bias, module.blocks.5.ca.v_bias, '\n"
                    " 'module.blocks.5.ca.mat_q.bias, module.blocks.5.ca.proj.bias, module.blocks.5.ffn.fc1.bias, module.blocks.5.ffn.fc2.bias, module.blocks.5.ca_norm.bias, module.blocks.6.ada_gss, '\n"
                    " 'module.blocks.6.sa.scale_mul_1H11, module.blocks.6.sa.q_bias, module.blocks.6.sa.v_bias, module.blocks.6.sa.proj.bias, module.blocks.6.ca.v_bias, module.blocks.6.ca.mat_q.bias, '\n"
                    " 'module.blocks.6.ca.proj.bias, module.blocks.6.ffn.fc1.bias, module.blocks.6.ffn.fc2.bias, module.blocks.6.ca_norm.bias, module.blocks.7.ada_gss, module.blocks.7.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.7.sa.q_bias, module.blocks.7.sa.v_bias, module.blocks.7.sa.proj.bias, module.blocks.7.ca.v_bias, module.blocks.7.ca.mat_q.bias, module.blocks.7.ca.proj.bias, '\n"
                    " 'module.blocks.7.ffn.fc1.bias, module.blocks.7.ffn.fc2.bias, module.blocks.7.ca_norm.bias, module.blocks.8.ada_gss, module.blocks.8.sa.scale_mul_1H11, module.blocks.8.sa.q_bias, '\n"
                    " 'module.blocks.8.sa.v_bias, module.blocks.8.sa.proj.bias, module.blocks.8.ca.v_bias, module.blocks.8.ca.mat_q.bias, module.blocks.8.ca.proj.bias, module.blocks.8.ffn.fc1.bias, '\n"
                    " 'module.blocks.8.ffn.fc2.bias, module.blocks.8.ca_norm.bias, module.blocks.9.ada_gss, module.blocks.9.sa.scale_mul_1H11, module.blocks.9.sa.q_bias, module.blocks.9.sa.v_bias, '\n"
                    " 'module.blocks.9.sa.proj.bias, module.blocks.9.ca.v_bias, module.blocks.9.ca.mat_q.bias, module.blocks.9.ca.proj.bias, module.blocks.9.ffn.fc1.bias, module.blocks.9.ffn.fc2.bias, '\n"
                    " 'module.blocks.9.ca_norm.bias, module.blocks.10.ada_gss, module.blocks.10.sa.scale_mul_1H11, module.blocks.10.sa.q_bias, module.blocks.10.sa.v_bias, module.blocks.10.sa.proj.bias, '\n"
                    " 'module.blocks.10.ca.v_bias, module.blocks.10.ca.mat_q.bias, module.blocks.10.ca.proj.bias, module.blocks.10.ffn.fc1.bias, module.blocks.10.ffn.fc2.bias, module.blocks.10.ca_norm.bias, '\n"
                    " 'module.blocks.11.ada_gss, module.blocks.11.sa.scale_mul_1H11, module.blocks.11.sa.q_bias, module.blocks.11.sa.v_bias, module.blocks.11.sa.proj.bias, module.blocks.11.ca.v_bias, '\n"
                    " 'module.blocks.11.ca.mat_q.bias, module.blocks.11.ca.proj.bias, module.blocks.11.ffn.fc1.bias, module.blocks.11.ffn.fc2.bias, module.blocks.11.ca_norm.bias, module.blocks.12.ada_gss, '\n"
                    " 'module.blocks.12.sa.scale_mul_1H11, module.blocks.12.sa.q_bias, module.blocks.12.sa.v_bias, module.blocks.12.sa.proj.bias, module.blocks.12.ca.v_bias, module.blocks.12.ca.mat_q.bias, '\n"
                    " 'module.blocks.12.ca.proj.bias, module.blocks.12.ffn.fc1.bias, module.blocks.12.ffn.fc2.bias, module.blocks.12.ca_norm.bias, module.blocks.13.ada_gss, module.blocks.13.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.13.sa.q_bias, module.blocks.13.sa.v_bias, module.blocks.13.sa.proj.bias, module.blocks.13.ca.v_bias, module.blocks.13.ca.mat_q.bias, module.blocks.13.ca.proj.bias, '\n"
                    " 'module.blocks.13.ffn.fc1.bias, module.blocks.13.ffn.fc2.bias, module.blocks.13.ca_norm.bias, module.blocks.14.ada_gss, module.blocks.14.sa.scale_mul_1H11, module.blocks.14.sa.q_bias, '\n"
                    " 'module.blocks.14.sa.v_bias, module.blocks.14.sa.proj.bias, module.blocks.14.ca.v_bias, module.blocks.14.ca.mat_q.bias, module.blocks.14.ca.proj.bias, module.blocks.14.ffn.fc1.bias, '\n"
                    " 'module.blocks.14.ffn.fc2.bias, module.blocks.14.ca_norm.bias, module.blocks.15.ada_gss, module.blocks.15.sa.scale_mul_1H11, module.blocks.15.sa.q_bias, module.blocks.15.sa.v_bias, '\n"
                    " 'module.blocks.15.sa.proj.bias, module.blocks.15.ca.v_bias, module.blocks.15.ca.mat_q.bias, module.blocks.15.ca.proj.bias, module.blocks.15.ffn.fc1.bias, module.blocks.15.ffn.fc2.bias, '\n"
                    " 'module.blocks.15.ca_norm.bias, module.blocks.16.ada_gss, module.blocks.16.sa.scale_mul_1H11, module.blocks.16.sa.q_bias, module.blocks.16.sa.v_bias, module.blocks.16.sa.proj.bias, '\n"
                    " 'module.blocks.16.ca.v_bias, module.blocks.16.ca.mat_q.bias, module.blocks.16.ca.proj.bias, module.blocks.16.ffn.fc1.bias, module.blocks.16.ffn.fc2.bias, module.blocks.16.ca_norm.bias, '\n"
                    " 'module.blocks.17.ada_gss, module.blocks.17.sa.scale_mul_1H11, module.blocks.17.sa.q_bias, module.blocks.17.sa.v_bias, module.blocks.17.sa.proj.bias, module.blocks.17.ca.v_bias, '\n"
                    " 'module.blocks.17.ca.mat_q.bias, module.blocks.17.ca.proj.bias, module.blocks.17.ffn.fc1.bias, module.blocks.17.ffn.fc2.bias, module.blocks.17.ca_norm.bias, module.blocks.18.ada_gss, '\n"
                    " 'module.blocks.18.sa.scale_mul_1H11, module.blocks.18.sa.q_bias, module.blocks.18.sa.v_bias, module.blocks.18.sa.proj.bias, module.blocks.18.ca.v_bias, module.blocks.18.ca.mat_q.bias, '\n"
                    " 'module.blocks.18.ca.proj.bias, module.blocks.18.ffn.fc1.bias, module.blocks.18.ffn.fc2.bias, module.blocks.18.ca_norm.bias, module.blocks.19.ada_gss, module.blocks.19.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.19.sa.q_bias, module.blocks.19.sa.v_bias, module.blocks.19.sa.proj.bias, module.blocks.19.ca.v_bias, module.blocks.19.ca.mat_q.bias, module.blocks.19.ca.proj.bias, '\n"
                    " 'module.blocks.19.ffn.fc1.bias, module.blocks.19.ffn.fc2.bias, module.blocks.19.ca_norm.bias, module.blocks.20.ada_gss, module.blocks.20.sa.scale_mul_1H11, module.blocks.20.sa.q_bias, '\n"
                    " 'module.blocks.20.sa.v_bias, module.blocks.20.sa.proj.bias, module.blocks.20.ca.v_bias, module.blocks.20.ca.mat_q.bias, module.blocks.20.ca.proj.bias, module.blocks.20.ffn.fc1.bias, '\n"
                    " 'module.blocks.20.ffn.fc2.bias, module.blocks.20.ca_norm.bias, module.blocks.21.ada_gss, module.blocks.21.sa.scale_mul_1H11, module.blocks.21.sa.q_bias, module.blocks.21.sa.v_bias, '\n"
                    " 'module.blocks.21.sa.proj.bias, module.blocks.21.ca.v_bias, module.blocks.21.ca.mat_q.bias, module.blocks.21.ca.proj.bias, module.blocks.21.ffn.fc1.bias, module.blocks.21.ffn.fc2.bias, '\n"
                    " 'module.blocks.21.ca_norm.bias, module.blocks.22.ada_gss, module.blocks.22.sa.scale_mul_1H11, module.blocks.22.sa.q_bias, module.blocks.22.sa.v_bias, module.blocks.22.sa.proj.bias, '\n"
                    " 'module.blocks.22.ca.v_bias, module.blocks.22.ca.mat_q.bias, module.blocks.22.ca.proj.bias, module.blocks.22.ffn.fc1.bias, module.blocks.22.ffn.fc2.bias, module.blocks.22.ca_norm.bias, '\n"
                    " 'module.blocks.23.ada_gss, module.blocks.23.sa.scale_mul_1H11, module.blocks.23.sa.q_bias, module.blocks.23.sa.v_bias, module.blocks.23.sa.proj.bias, module.blocks.23.ca.v_bias, '\n"
                    " 'module.blocks.23.ca.mat_q.bias, module.blocks.23.ca.proj.bias, module.blocks.23.ffn.fc1.bias, module.blocks.23.ffn.fc2.bias, module.blocks.23.ca_norm.bias, module.blocks.24.ada_gss, '\n"
                    " 'module.blocks.24.sa.scale_mul_1H11, module.blocks.24.sa.q_bias, module.blocks.24.sa.v_bias, module.blocks.24.sa.proj.bias, module.blocks.24.ca.v_bias, module.blocks.24.ca.mat_q.bias, '\n"
                    " 'module.blocks.24.ca.proj.bias, module.blocks.24.ffn.fc1.bias, module.blocks.24.ffn.fc2.bias, module.blocks.24.ca_norm.bias, module.blocks.25.ada_gss, module.blocks.25.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.25.sa.q_bias, module.blocks.25.sa.v_bias, module.blocks.25.sa.proj.bias, module.blocks.25.ca.v_bias, module.blocks.25.ca.mat_q.bias, module.blocks.25.ca.proj.bias, '\n"
                    " 'module.blocks.25.ffn.fc1.bias, module.blocks.25.ffn.fc2.bias, module.blocks.25.ca_norm.bias, module.blocks.26.ada_gss, module.blocks.26.sa.scale_mul_1H11, module.blocks.26.sa.q_bias, '\n"
                    " 'module.blocks.26.sa.v_bias, module.blocks.26.sa.proj.bias, module.blocks.26.ca.v_bias, module.blocks.26.ca.mat_q.bias, module.blocks.26.ca.proj.bias, module.blocks.26.ffn.fc1.bias, '\n"
                    " 'module.blocks.26.ffn.fc2.bias, module.blocks.26.ca_norm.bias, module.blocks.27.ada_gss, module.blocks.27.sa.scale_mul_1H11, module.blocks.27.sa.q_bias, module.blocks.27.sa.v_bias, '\n"
                    " 'module.blocks.27.sa.proj.bias, module.blocks.27.ca.v_bias, module.blocks.27.ca.mat_q.bias, module.blocks.27.ca.proj.bias, module.blocks.27.ffn.fc1.bias, module.blocks.27.ffn.fc2.bias, '\n"
                    " 'module.blocks.27.ca_norm.bias, module.blocks.28.ada_gss, module.blocks.28.sa.scale_mul_1H11, module.blocks.28.sa.q_bias, module.blocks.28.sa.v_bias, module.blocks.28.sa.proj.bias, '\n"
                    " 'module.blocks.28.ca.v_bias, module.blocks.28.ca.mat_q.bias, module.blocks.28.ca.proj.bias, module.blocks.28.ffn.fc1.bias, module.blocks.28.ffn.fc2.bias, module.blocks.28.ca_norm.bias, '\n"
                    " 'module.blocks.29.ada_gss, module.blocks.29.sa.scale_mul_1H11, module.blocks.29.sa.q_bias, module.blocks.29.sa.v_bias, module.blocks.29.sa.proj.bias, module.blocks.29.ca.v_bias, '\n"
                    " 'module.blocks.29.ca.mat_q.bias, module.blocks.29.ca.proj.bias, module.blocks.29.ffn.fc1.bias, module.blocks.29.ffn.fc2.bias, module.blocks.29.ca_norm.bias, module.blocks.30.ada_gss, '\n"
                    " 'module.blocks.30.sa.scale_mul_1H11, module.blocks.30.sa.q_bias, module.blocks.30.sa.v_bias, module.blocks.30.sa.proj.bias, module.blocks.30.ca.v_bias, module.blocks.30.ca.mat_q.bias, '\n"
                    " 'module.blocks.30.ca.proj.bias, module.blocks.30.ffn.fc1.bias, module.blocks.30.ffn.fc2.bias, module.blocks.30.ca_norm.bias, module.blocks.31.ada_gss, module.blocks.31.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.31.sa.q_bias, module.blocks.31.sa.v_bias, module.blocks.31.sa.proj.bias, module.blocks.31.ca.v_bias, module.blocks.31.ca.mat_q.bias, module.blocks.31.ca.proj.bias, '\n"
                    " 'module.blocks.31.ffn.fc1.bias, module.blocks.31.ffn.fc2.bias, module.blocks.31.ca_norm.bias, module.blocks.32.ada_gss, module.blocks.32.sa.scale_mul_1H11, module.blocks.32.sa.q_bias, '\n"
                    " 'module.blocks.32.sa.v_bias, module.blocks.32.sa.proj.bias, module.blocks.32.ca.v_bias, module.blocks.32.ca.mat_q.bias, module.blocks.32.ca.proj.bias, module.blocks.32.ffn.fc1.bias, '\n"
                    " 'module.blocks.32.ffn.fc2.bias, module.blocks.32.ca_norm.bias, module.blocks.33.ada_gss, module.blocks.33.sa.scale_mul_1H11, module.blocks.33.sa.q_bias, module.blocks.33.sa.v_bias, '\n"
                    " 'module.blocks.33.sa.proj.bias, module.blocks.33.ca.v_bias, module.blocks.33.ca.mat_q.bias, module.blocks.33.ca.proj.bias, module.blocks.33.ffn.fc1.bias, module.blocks.33.ffn.fc2.bias, '\n"
                    " 'module.blocks.33.ca_norm.bias, module.blocks.34.ada_gss, module.blocks.34.sa.scale_mul_1H11, module.blocks.34.sa.q_bias, module.blocks.34.sa.v_bias, module.blocks.34.sa.proj.bias, '\n"
                    " 'module.blocks.34.ca.v_bias, module.blocks.34.ca.mat_q.bias, module.blocks.34.ca.proj.bias, module.blocks.34.ffn.fc1.bias, module.blocks.34.ffn.fc2.bias, module.blocks.34.ca_norm.bias, '\n"
                    " 'module.blocks.35.ada_gss, module.blocks.35.sa.scale_mul_1H11, module.blocks.35.sa.q_bias, module.blocks.35.sa.v_bias, module.blocks.35.sa.proj.bias, module.blocks.35.ca.v_bias, '\n"
                    " 'module.blocks.35.ca.mat_q.bias, module.blocks.35.ca.proj.bias, module.blocks.35.ffn.fc1.bias, module.blocks.35.ffn.fc2.bias, module.blocks.35.ca_norm.bias, module.blocks.36.ada_gss, '\n"
                    " 'module.blocks.36.sa.scale_mul_1H11, module.blocks.36.sa.q_bias, module.blocks.36.sa.v_bias, module.blocks.36.sa.proj.bias, module.blocks.36.ca.v_bias, module.blocks.36.ca.mat_q.bias, '\n"
                    " 'module.blocks.36.ca.proj.bias, module.blocks.36.ffn.fc1.bias, module.blocks.36.ffn.fc2.bias, module.blocks.36.ca_norm.bias, module.blocks.37.ada_gss, module.blocks.37.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.37.sa.q_bias, module.blocks.37.sa.v_bias, module.blocks.37.sa.proj.bias, module.blocks.37.ca.v_bias, module.blocks.37.ca.mat_q.bias, module.blocks.37.ca.proj.bias, '\n"
                    " 'module.blocks.37.ffn.fc1.bias, module.blocks.37.ffn.fc2.bias, module.blocks.37.ca_norm.bias, module.blocks.38.ada_gss, module.blocks.38.sa.scale_mul_1H11, module.blocks.38.sa.q_bias, '\n"
                    " 'module.blocks.38.sa.v_bias, module.blocks.38.sa.proj.bias, module.blocks.38.ca.v_bias, module.blocks.38.ca.mat_q.bias, module.blocks.38.ca.proj.bias, module.blocks.38.ffn.fc1.bias, '\n"
                    " 'module.blocks.38.ffn.fc2.bias, module.blocks.38.ca_norm.bias, module.blocks.39.ada_gss, module.blocks.39.sa.scale_mul_1H11, module.blocks.39.sa.q_bias, module.blocks.39.sa.v_bias, '\n"
                    " 'module.blocks.39.sa.proj.bias, module.blocks.39.ca.v_bias, module.blocks.39.ca.mat_q.bias, module.blocks.39.ca.proj.bias, module.blocks.39.ffn.fc1.bias, module.blocks.39.ffn.fc2.bias, '\n"
                    " 'module.blocks.39.ca_norm.bias, module.blocks.40.ada_gss, module.blocks.40.sa.scale_mul_1H11, module.blocks.40.sa.q_bias, module.blocks.40.sa.v_bias, module.blocks.40.sa.proj.bias, '\n"
                    " 'module.blocks.40.ca.v_bias, module.blocks.40.ca.mat_q.bias, module.blocks.40.ca.proj.bias, module.blocks.40.ffn.fc1.bias, module.blocks.40.ffn.fc2.bias, module.blocks.40.ca_norm.bias, '\n"
                    " 'module.blocks.41.ada_gss, module.blocks.41.sa.scale_mul_1H11, module.blocks.41.sa.q_bias, module.blocks.41.sa.v_bias, module.blocks.41.sa.proj.bias, module.blocks.41.ca.v_bias, '\n"
                    " 'module.blocks.41.ca.mat_q.bias, module.blocks.41.ca.proj.bias, module.blocks.41.ffn.fc1.bias, module.blocks.41.ffn.fc2.bias, module.blocks.41.ca_norm.bias, module.blocks.42.ada_gss, '\n"
                    " 'module.blocks.42.sa.scale_mul_1H11, module.blocks.42.sa.q_bias, module.blocks.42.sa.v_bias, module.blocks.42.sa.proj.bias, module.blocks.42.ca.v_bias, module.blocks.42.ca.mat_q.bias, '\n"
                    " 'module.blocks.42.ca.proj.bias, module.blocks.42.ffn.fc1.bias, module.blocks.42.ffn.fc2.bias, module.blocks.42.ca_norm.bias, module.blocks.43.ada_gss, module.blocks.43.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.43.sa.q_bias, module.blocks.43.sa.v_bias, module.blocks.43.sa.proj.bias, module.blocks.43.ca.v_bias, module.blocks.43.ca.mat_q.bias, module.blocks.43.ca.proj.bias, '\n"
                    " 'module.blocks.43.ffn.fc1.bias, module.blocks.43.ffn.fc2.bias, module.blocks.43.ca_norm.bias, module.blocks.44.ada_gss, module.blocks.44.sa.scale_mul_1H11, module.blocks.44.sa.q_bias, '\n"
                    " 'module.blocks.44.sa.v_bias, module.blocks.44.sa.proj.bias, module.blocks.44.ca.v_bias, module.blocks.44.ca.mat_q.bias, module.blocks.44.ca.proj.bias, module.blocks.44.ffn.fc1.bias, '\n"
                    " 'module.blocks.44.ffn.fc2.bias, module.blocks.44.ca_norm.bias, module.blocks.45.ada_gss, module.blocks.45.sa.scale_mul_1H11, module.blocks.45.sa.q_bias, module.blocks.45.sa.v_bias, '\n"
                    " 'module.blocks.45.sa.proj.bias, module.blocks.45.ca.v_bias, module.blocks.45.ca.mat_q.bias, module.blocks.45.ca.proj.bias, module.blocks.45.ffn.fc1.bias, module.blocks.45.ffn.fc2.bias, '\n"
                    " 'module.blocks.45.ca_norm.bias, module.blocks.46.ada_gss, module.blocks.46.sa.scale_mul_1H11, module.blocks.46.sa.q_bias, module.blocks.46.sa.v_bias, module.blocks.46.sa.proj.bias, '\n"
                    " 'module.blocks.46.ca.v_bias, module.blocks.46.ca.mat_q.bias, module.blocks.46.ca.proj.bias, module.blocks.46.ffn.fc1.bias, module.blocks.46.ffn.fc2.bias, module.blocks.46.ca_norm.bias, '\n"
                    " 'module.blocks.47.ada_gss, module.blocks.47.sa.scale_mul_1H11, module.blocks.47.sa.q_bias, module.blocks.47.sa.v_bias, module.blocks.47.sa.proj.bias, module.blocks.47.ca.v_bias, '\n"
                    " 'module.blocks.47.ca.mat_q.bias, module.blocks.47.ca.proj.bias, module.blocks.47.ffn.fc1.bias, module.blocks.47.ffn.fc2.bias, module.blocks.47.ca_norm.bias, module.blocks.48.ada_gss, '\n"
                    " 'module.blocks.48.sa.scale_mul_1H11, module.blocks.48.sa.q_bias, module.blocks.48.sa.v_bias, module.blocks.48.sa.proj.bias, module.blocks.48.ca.v_bias, module.blocks.48.ca.mat_q.bias, '\n"
                    " 'module.blocks.48.ca.proj.bias, module.blocks.48.ffn.fc1.bias, module.blocks.48.ffn.fc2.bias, module.blocks.48.ca_norm.bias, module.blocks.49.ada_gss, module.blocks.49.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.49.sa.q_bias, module.blocks.49.sa.v_bias, module.blocks.49.sa.proj.bias, module.blocks.49.ca.v_bias, module.blocks.49.ca.mat_q.bias, module.blocks.49.ca.proj.bias, '\n"
                    " 'module.blocks.49.ffn.fc1.bias, module.blocks.49.ffn.fc2.bias, module.blocks.49.ca_norm.bias, module.blocks.50.ada_gss, module.blocks.50.sa.scale_mul_1H11, module.blocks.50.sa.q_bias, '\n"
                    " 'module.blocks.50.sa.v_bias, module.blocks.50.sa.proj.bias, module.blocks.50.ca.v_bias, module.blocks.50.ca.mat_q.bias, module.blocks.50.ca.proj.bias, module.blocks.50.ffn.fc1.bias, '\n"
                    " 'module.blocks.50.ffn.fc2.bias, module.blocks.50.ca_norm.bias, module.blocks.51.ada_gss, module.blocks.51.sa.scale_mul_1H11, module.blocks.51.sa.q_bias, module.blocks.51.sa.v_bias, '\n"
                    " 'module.blocks.51.sa.proj.bias, module.blocks.51.ca.v_bias, module.blocks.51.ca.mat_q.bias, module.blocks.51.ca.proj.bias, module.blocks.51.ffn.fc1.bias, module.blocks.51.ffn.fc2.bias, '\n"
                    " 'module.blocks.51.ca_norm.bias, module.blocks.52.ada_gss, module.blocks.52.sa.scale_mul_1H11, module.blocks.52.sa.q_bias, module.blocks.52.sa.v_bias, module.blocks.52.sa.proj.bias, '\n"
                    " 'module.blocks.52.ca.v_bias, module.blocks.52.ca.mat_q.bias, module.blocks.52.ca.proj.bias, module.blocks.52.ffn.fc1.bias, module.blocks.52.ffn.fc2.bias, module.blocks.52.ca_norm.bias, '\n"
                    " 'module.blocks.53.ada_gss, module.blocks.53.sa.scale_mul_1H11, module.blocks.53.sa.q_bias, module.blocks.53.sa.v_bias, module.blocks.53.sa.proj.bias, module.blocks.53.ca.v_bias, '\n"
                    " 'module.blocks.53.ca.mat_q.bias, module.blocks.53.ca.proj.bias, module.blocks.53.ffn.fc1.bias, module.blocks.53.ffn.fc2.bias, module.blocks.53.ca_norm.bias, module.blocks.54.ada_gss, '\n"
                    " 'module.blocks.54.sa.scale_mul_1H11, module.blocks.54.sa.q_bias, module.blocks.54.sa.v_bias, module.blocks.54.sa.proj.bias, module.blocks.54.ca.v_bias, module.blocks.54.ca.mat_q.bias, '\n"
                    " 'module.blocks.54.ca.proj.bias, module.blocks.54.ffn.fc1.bias, module.blocks.54.ffn.fc2.bias, module.blocks.54.ca_norm.bias, module.blocks.55.ada_gss, module.blocks.55.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.55.sa.q_bias, module.blocks.55.sa.v_bias, module.blocks.55.sa.proj.bias, module.blocks.55.ca.v_bias, module.blocks.55.ca.mat_q.bias, module.blocks.55.ca.proj.bias, '\n"
                    " 'module.blocks.55.ffn.fc1.bias, module.blocks.55.ffn.fc2.bias, module.blocks.55.ca_norm.bias')",
          'wd_sc': 0.0}}

[09-07 00:05:55] (nity/utils/lr_control.py, line 116)=> [get_param_groups][rank0] type(model).__name__='NullDDP' count=1085, numel=8573892016
[09-07 00:05:55] (nity/utils/lr_control.py, line 117)=> 
[09-07 00:05:55] (inity_circuit_breaker.py, line 343)=> [vgpt] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.97), fused=True), opt_kw={'lr': 9.375e-05, 'weight_decay': 0}

[09-07 00:05:55] (inity_circuit_breaker.py, line 348)=> Loading T5 from google/flan-t5-xl...
[09-07 00:05:56] (_original/src/trainer.py, line  65)=> self.reweight_loss_by_scale: 1
[09-07 00:05:56] (inity_circuit_breaker.py, line 171)=> global bs=4, local bs=4
[09-07 00:05:56] (inity_circuit_breaker.py, line 172)=> initial args:
{
  local_out_path      : ./local_output
  data_path           : ./data
  bed                 : ./checkpoints
  vae_ckpt            : /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
  exp_name            : experiment
  ds                  : oi
  model               : infinity_8b
  short_cap_prob      : 0.2
  project_name        : infinity_circuit_breaker
  tf32                : True
  auto_resume         : True
  rush_resume         : 
  nowd                : 1
  enable_hybrid_shard : False
  inner_shard_degree  : 1
  zero                : 2
  buck                : chunk
  fsdp_orig           : True
  enable_checkpointing: full-block
  pad_to_multiplier   : 128
  log_every_iter      : False
  checkpoint_type     : torch
  seed                : None
  rand                : True
  task_id             : 123
  trial_id            : 123
  robust_run_id       : 0
  ckpt_trials         : []
  real_trial_id       : 123
  chunk_nodes         : 0
  is_master_node      : None
  log_txt_path        : ./local_output/log.txt
  t5_path             : google/flan-t5-xl
  online_t5           : True
  sdpa_mem            : True
  tfast               : 0
  model_alias         : 8b
  rms                 : False
  aln                 : 0.001
  alng                : 5e-06
  saln                : True
  haln                : True
  nm0                 : False
  tau                 : 1
  cos                 : True
  swi                 : False
  dp                  : -1
  drop                : 0.0
  hd                  : 0
  ca_gamma            : -1
  diva                : 1
  hd0                 : 0.02
  dec                 : 1
  cum                 : 3
  rwe                 : False
  tp                  : 0.0
  tk                  : 0.0
  tini                : 0.010416666666666666
  cfg                 : 0.1
  rand_uncond         : False
  ema                 : 0.9999
  tema                : 0
  fp16                : 2
  fuse                : False
  fused_norm          : False
  flash               : False
  xen                 : False
  use_flex_attn       : True
  stable              : False
  gblr                : 0.0001
  dblr                : 0.0001
  tblr                : 0.006
  glr                 : 1.5625e-06
  dlr                 : 1.5625e-06
  tlr                 : 9.375e-05
  gwd                 : 0.005
  dwd                 : 0.0005
  twd                 : 0.005
  gwde                : 0.005
  dwde                : 0.0005
  twde                : 0.005
  ls                  : 0.0
  lz                  : 0.0
  eq                  : 0
  ep                  : 100
  wp                  : 1e-08
  wp0                 : 0.005
  wpe                 : 1.0
  sche                : lin0
  log_freq            : 50
  gclip               : 6.0
  dclip               : 6.0
  tclip               : 5.0
  cdec                : False
  opt                 : adamw
  ada                 : 0.9_0.97
  dada                : 0.9_0.97
  oeps                : 0
  afuse               : True
  pn                  : 0.06M
  scale_schedule      : None
  patch_size          : None
  resos               : None
  data_load_reso      : None
  workers             : 4
  lbs                 : 4
  bs                  : 4
  batch_size          : 4
  glb_batch_size      : 4
  ac                  : 1
  r_accu              : 1.0
  norm_eps            : 1e-06
  tlen                : 512
  Ct5                 : 2048
  use_bit_label       : 1
  bitloss_type        : mean
  dynamic_resolution_across_gpus: 1
  enable_dynamic_length_prompt: 1
  use_streaming_dataset: 1
  iterable_data_buffersize: 30000
  save_model_iters_freq: 100
  noise_apply_layers  : 13
  noise_apply_strength: 0.3
  noise_apply_requant : 1
  rope2d_each_sa_layer: 1
  rope2d_normalized_by_hw: 2
  use_fsdp_model_ema  : 0
  add_lvl_embeding_only_first_block: 1
  reweight_loss_by_scale: 1
  always_training_scales: 100
  vae_type            : 14
  fake_vae_input      : False
  model_init_device   : cuda
  prefetch_factor     : 2
  apply_spatial_patchify: 1
  debug_bsc           : 0
  task_type           : t2i
  target_layers       : [10, 12, 14, 16, 18, 20]
  transform_layers    : [10, 12, 14, 16, 18, 20]
  lorra_alpha         : 5.0
  trainsets           : ['AlpacaSupervisedDataset', 'HarmfulDataset']
  valsets             : ['AlpacaSupervisedDataset', 'HarmfulDataset']
  adv_string          : Sure here's
  full_layers         : False
  harmful_prompts_path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
  sanitized_prompts_path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
  validation_ratio    : 0.1
  category            : hate
  lora_r              : 8
  lora_alpha          : 16
  lora_dropout        : 0.05
  lora_target_modules : ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
  lora_weight_path    : 
  lora_bias           : none
  q_lora              : False
  model_name_or_path  : meta-llama/Llama-2-7b-chat-hf
  adapter_name_or_path: 
  use_lora            : True
  cache_dir           : 
  optim               : adamw_torch
  model_max_length    : 512
  grouped_to_max_length: False
  use_refusal_retain  : True
  sc_train_subset     : ['']
  log_every           : 10
  sc_train_seq_type   : all_text
  coeff_schedule      : linear_converge
  sc_loss_type        : orig_act_dotprod
  branch              : master
  commit_id           : 
  commit_msg          : 
  cmd                 : --bed ./checkpoints --local_out_path ./local_output --project_name infinity_circuit_breaker --exp_name experiment --ep 100 --opt adamw --cum 3 --sche lin0 --fp16 2 --ada 0.9_0.97 --tini -1 --tclip 5 --flash 0 --alng 5e-06 --saln 1 --cos 1 --enable_checkpointing full-block --tblr 6e-3 --pn 0.06M --lbs 4 --workers 4 --short_cap_prob 0.2 --online_t5 1 --use_streaming_dataset 1 --iterable_data_buffersize 30000 --Ct5 2048 --t5_path google/flan-t5-xl --vae_type 14 --wp 0.00000001 --wpe 1 --dynamic_resolution_across_gpus 1 --enable_dynamic_length_prompt 1 --reweight_loss_by_scale 1 --add_lvl_embeding_only_first_block 1 --rope2d_each_sa_layer 1 --rope2d_normalized_by_hw 2 --use_fsdp_model_ema 0 --always_training_scales 100 --use_bit_label 1 --zero 2 --save_model_iters_freq 100 --log_freq 50 --checkpoint_type torch --prefetch_factor 2 --noise_apply_strength 0.3 --noise_apply_layers 13 --apply_spatial_patchify 1 --use_flex_attn True --pad 128 --data_path ./data --data_load_reso 512 --tlen 512 --workers 4 --prefetch_factor 2 --harmful_prompts_path /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts --sanitized_prompts_path /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts --validation_ratio 0.1 --category hate --target_layers 10,12,14,16,18,20 --transform_layers 10,12,14,16,18,20 --lorra_alpha 5.0 --trainsets AlpacaSupervisedDataset#HarmfulDataset --valsets AlpacaSupervisedDataset#HarmfulDataset --adv_string Sure here's --full_layers False --lora_r 8 --lora_alpha 16 --lora_dropout 0.05 --lora_target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj --lora_weight_path  --lora_bias none --q_lora False --model_name_or_path meta-llama/Llama-2-7b-chat-hf --adapter_name_or_path  --use_lora True --cache_dir  --optim adamw_torch --model_max_length 512 --grouped_to_max_length False --use_refusal_retain True --sc_train_subset  --log_every 10 --sc_train_seq_type all_text --coeff_schedule linear_converge --sc_loss_type orig_act_dotprod
  tag                 : UK
  acc_all             : None
  acc_real            : None
  acc_fake            : None
  last_Lnll           : None
  last_L1             : None
  last_Ld             : None
  last_wei_g          : None
  grad_boom           : None
  diff                : None
  diffs               : 
  diffs_ema           : None
  ca_performance      : 
  cur_phase           : 
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  iter_speed          : None
  img_per_day         : None
  max_nvidia_smi      : 0
  max_memory_allocated: None
  max_memory_reserved : None
  num_alloc_retries   : None
  MFU                 : None
  HFU                 : None
  dbg_modified        : False
  dbg_ks              : False
  dbg_ks_last         : None
  dbg                 : False
  ks                  : False
  nodata              : False
  nodata_tlen         : 320
  nova                : False
  prof                : 0
  prof_freq           : 50
  tos_profiler_file_prefix: vgpt_default/
  profall             : 0
  v_seed              : 0
  g_seed              : 0
}

[09-07 00:05:56] (inity_circuit_breaker.py, line 182)=> start_it=0, iters_train=100
[09-07 00:05:57] (inity_circuit_breaker.py, line 466)=> [PT info]  from ep0 it0, acc_str: [no acc str], diffs: ,    =======>  bed: ./checkpoints  <=======

[09-07 00:05:59] (_original/src/trainer.py, line 179)=> [DEBUG] raw_features shape: torch.Size([2, 14, 64, 64])
[09-07 00:05:59] (_original/src/trainer.py, line 180)=> [DEBUG] vae_scale_schedule: [(1, 2, 2), (1, 4, 4), (1, 8, 8), (1, 12, 12), (1, 16, 16), (1, 24, 24), (1, 32, 32)]
[09-07 00:05:59] (_original/src/trainer.py, line 181)=> [DEBUG] vae_scale_schedule[-1]: (1, 32, 32)
[09-07 00:05:59] (twise_self_correction.py, line  36)=> [dbg] raw_features: torch.Size([2, 14, 64, 64])
[09-07 00:05:59] (twise_self_correction.py, line  41)=> [dbg] target last scale: (1, 32, 32)







=======================================================   RESTART [09-07 00:07:42]   =======================================================
[09-07 00:07:42] (inity_circuit_breaker.py, line 718)=> ============================================================
[09-07 00:07:42] (inity_circuit_breaker.py, line 719)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[09-07 00:07:42] (inity_circuit_breaker.py, line 720)=> Circuit Breaker Alpha: 0.1
[09-07 00:07:42] (inity_circuit_breaker.py, line 721)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[09-07 00:07:42] (inity_circuit_breaker.py, line 722)=> Circuit Breaker Enabled: True
[09-07 00:07:42] (inity_circuit_breaker.py, line 723)=> Number of Examples: 1000
[09-07 00:07:42] (inity_circuit_breaker.py, line 724)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[09-07 00:07:42] (inity_circuit_breaker.py, line 725)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[09-07 00:07:42] (inity_circuit_breaker.py, line 726)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[09-07 00:07:42] (inity_circuit_breaker.py, line 727)=> Validation Ratio: 0.1
[09-07 00:07:42] (inity_circuit_breaker.py, line 728)=> Category: hate
[09-07 00:07:42] (inity_circuit_breaker.py, line 729)=> Batch Size: 4
[09-07 00:07:42] (inity_circuit_breaker.py, line 730)=> Workers: 4
[09-07 00:07:42] (inity_circuit_breaker.py, line 731)=> Device: cuda:0
[09-07 00:07:42] (inity_circuit_breaker.py, line 732)=> Model: infinity_8b
[09-07 00:07:42] (inity_circuit_breaker.py, line 733)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-07 00:07:42] (inity_circuit_breaker.py, line 734)=> Rush Resume: 
[09-07 00:07:42] (inity_circuit_breaker.py, line 735)=> Selective Layers: 0,1,2,3,4,5
[09-07 00:07:42] (inity_circuit_breaker.py, line 736)=> ============================================================
[09-07 00:07:42] (inity_circuit_breaker.py, line 377)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[09-07 00:07:42] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-07 00:07:42] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-07 00:07:42] (rain_dataset_infinity.py, line 205)=> Split 1000 prompts: 900 retain, 100 validation
[09-07 00:07:42] (rain_dataset_infinity.py, line 236)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-07 00:07:42] (rain_dataset_infinity.py, line 245)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-07 00:07:42] (rain_dataset_infinity.py, line  84)=> Loaded 900 retain examples
[09-07 00:07:42] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[09-07 00:07:42] (rain_dataset_infinity.py, line  86)=> Loaded 100 validation examples
[09-07 00:07:42] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[09-07 00:07:42] (inity_circuit_breaker.py, line 395)=> args.batch_size=4, vbs=6
[09-07 00:07:42] (inity_circuit_breaker.py, line 418)=> len(dataloader): 100, len(dataset): 100
[09-07 00:07:42] (inity_circuit_breaker.py, line 420)=> total_samples: 200
[09-07 00:07:42] (inity_circuit_breaker.py, line 421)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[09-07 00:07:42] (inity_circuit_breaker.py, line 140)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[09-07 00:07:42] (inity_circuit_breaker.py, line 142)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[09-07 00:07:42] (inity_circuit_breaker.py, line 146)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-07 00:07:45] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[09-07 00:07:45] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[09-07 00:07:45] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[09-07 00:07:45] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[09-07 00:07:47] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[09-07 00:07:49] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[09-07 00:07:49] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[09-07 00:07:49] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[09-07 00:07:49] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[09-07 00:07:49] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

[09-07 00:07:59] (ity/models/init_param.py, line  15)=> [init_weights] Infinity with std=0.02
[09-07 00:08:00] (inity_circuit_breaker.py, line 246)=> [PT] GPT model = Infinity(
  drop_path_rate=0.2
  (norm0_cond): Identity()
  (text_norm): FastRMSNorm(C=2048, eps=1e-06, elementwise_affine=True)
  (text_proj_for_sos): TextAttentivePool(
    (ca): CrossAttention(
      Cq=3072, Ckv=2048, cos_attn=False
      (mat_kv): Linear(in_features=2048, out_features=6144, bias=False)
      (proj): Linear(in_features=3072, out_features=3072, bias=True)
      (proj_drop): Identity()
    )
  )
  (text_proj_for_ca): Sequential(
    (0): Linear(in_features=2048, out_features=3072, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=3072, out_features=3072, bias=True)
  )
  (lvl_embed): Embedding(15, 3072)
  (norm0_ve): Identity()
  (word_embed): Linear(in_features=56, out_features=3072, bias=True)
  (shared_ada_lin): Sequential(
    (0): SiLU()
    (1): SharedAdaLin(in_features=3072, out_features=18432, bias=True)
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=3072, out_features=6144, bias=True)
    )
  )
  (head): Linear(in_features=3072, out_features=112, bias=True)
  (blocks): ModuleList(
    (0): CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): Identity()
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
    (1-55): 55 x CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): DropPath(...)
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
  )
)


[09-07 00:08:00] (inity_circuit_breaker.py, line 248)=> [PT][#para] VAE=81.62, VAE.quant=0.00
[09-07 00:08:00] (inity_circuit_breaker.py, line 251)=> [PT][#para] GPT=8573.89


[09-07 00:08:00] (nity/utils/lr_control.py, line  71)=> [get_param_groups][lr decay] with_lr_scale=False, lr_scale=0.0
[09-07 00:08:00] (nity/utils/lr_control.py, line 111)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': '[no scale]',
         'params': "('module.text_norm.weight, module.text_proj_for_sos.ca.mat_kv.weight, module.text_proj_for_sos.ca.proj.weight, module.text_proj_for_ca.0.weight, module.text_proj_for_ca.2.weight, '\n"
                   " 'module.word_embed.weight, module.shared_ada_lin.1.weight, module.head_nm.ada_lin.1.weight, module.head.weight, module.blocks.0.sa.mat_qkv.weight, module.blocks.0.sa.proj.weight, '\n"
                   " 'module.blocks.0.ca.mat_q.weight, module.blocks.0.ca.mat_kv.weight, module.blocks.0.ca.proj.weight, module.blocks.0.ffn.fc1.weight, module.blocks.0.ffn.fc2.weight, module.blocks.0.ca_norm.weight, '\n"
                   " 'module.blocks.1.sa.mat_qkv.weight, module.blocks.1.sa.proj.weight, module.blocks.1.ca.mat_q.weight, module.blocks.1.ca.mat_kv.weight, module.blocks.1.ca.proj.weight, '\n"
                   " 'module.blocks.1.ffn.fc1.weight, module.blocks.1.ffn.fc2.weight, module.blocks.1.ca_norm.weight, module.blocks.2.sa.mat_qkv.weight, module.blocks.2.sa.proj.weight, module.blocks.2.ca.mat_q.weight, '\n"
                   " 'module.blocks.2.ca.mat_kv.weight, module.blocks.2.ca.proj.weight, module.blocks.2.ffn.fc1.weight, module.blocks.2.ffn.fc2.weight, module.blocks.2.ca_norm.weight, module.blocks.3.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.3.sa.proj.weight, module.blocks.3.ca.mat_q.weight, module.blocks.3.ca.mat_kv.weight, module.blocks.3.ca.proj.weight, module.blocks.3.ffn.fc1.weight, module.blocks.3.ffn.fc2.weight, '\n"
                   " 'module.blocks.3.ca_norm.weight, module.blocks.4.sa.mat_qkv.weight, module.blocks.4.sa.proj.weight, module.blocks.4.ca.mat_q.weight, module.blocks.4.ca.mat_kv.weight, '\n"
                   " 'module.blocks.4.ca.proj.weight, module.blocks.4.ffn.fc1.weight, module.blocks.4.ffn.fc2.weight, module.blocks.4.ca_norm.weight, module.blocks.5.sa.mat_qkv.weight, module.blocks.5.sa.proj.weight, '\n"
                   " 'module.blocks.5.ca.mat_q.weight, module.blocks.5.ca.mat_kv.weight, module.blocks.5.ca.proj.weight, module.blocks.5.ffn.fc1.weight, module.blocks.5.ffn.fc2.weight, module.blocks.5.ca_norm.weight, '\n"
                   " 'module.blocks.6.sa.mat_qkv.weight, module.blocks.6.sa.proj.weight, module.blocks.6.ca.mat_q.weight, module.blocks.6.ca.mat_kv.weight, module.blocks.6.ca.proj.weight, '\n"
                   " 'module.blocks.6.ffn.fc1.weight, module.blocks.6.ffn.fc2.weight, module.blocks.6.ca_norm.weight, module.blocks.7.sa.mat_qkv.weight, module.blocks.7.sa.proj.weight, module.blocks.7.ca.mat_q.weight, '\n"
                   " 'module.blocks.7.ca.mat_kv.weight, module.blocks.7.ca.proj.weight, module.blocks.7.ffn.fc1.weight, module.blocks.7.ffn.fc2.weight, module.blocks.7.ca_norm.weight, module.blocks.8.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.8.sa.proj.weight, module.blocks.8.ca.mat_q.weight, module.blocks.8.ca.mat_kv.weight, module.blocks.8.ca.proj.weight, module.blocks.8.ffn.fc1.weight, module.blocks.8.ffn.fc2.weight, '\n"
                   " 'module.blocks.8.ca_norm.weight, module.blocks.9.sa.mat_qkv.weight, module.blocks.9.sa.proj.weight, module.blocks.9.ca.mat_q.weight, module.blocks.9.ca.mat_kv.weight, '\n"
                   " 'module.blocks.9.ca.proj.weight, module.blocks.9.ffn.fc1.weight, module.blocks.9.ffn.fc2.weight, module.blocks.9.ca_norm.weight, module.blocks.10.sa.mat_qkv.weight, module.blocks.10.sa.proj.weight, '\n"
                   " 'module.blocks.10.ca.mat_q.weight, module.blocks.10.ca.mat_kv.weight, module.blocks.10.ca.proj.weight, module.blocks.10.ffn.fc1.weight, module.blocks.10.ffn.fc2.weight, '\n"
                   " 'module.blocks.10.ca_norm.weight, module.blocks.11.sa.mat_qkv.weight, module.blocks.11.sa.proj.weight, module.blocks.11.ca.mat_q.weight, module.blocks.11.ca.mat_kv.weight, '\n"
                   " 'module.blocks.11.ca.proj.weight, module.blocks.11.ffn.fc1.weight, module.blocks.11.ffn.fc2.weight, module.blocks.11.ca_norm.weight, module.blocks.12.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.12.sa.proj.weight, module.blocks.12.ca.mat_q.weight, module.blocks.12.ca.mat_kv.weight, module.blocks.12.ca.proj.weight, module.blocks.12.ffn.fc1.weight, '\n"
                   " 'module.blocks.12.ffn.fc2.weight, module.blocks.12.ca_norm.weight, module.blocks.13.sa.mat_qkv.weight, module.blocks.13.sa.proj.weight, module.blocks.13.ca.mat_q.weight, '\n"
                   " 'module.blocks.13.ca.mat_kv.weight, module.blocks.13.ca.proj.weight, module.blocks.13.ffn.fc1.weight, module.blocks.13.ffn.fc2.weight, module.blocks.13.ca_norm.weight, '\n"
                   " 'module.blocks.14.sa.mat_qkv.weight, module.blocks.14.sa.proj.weight, module.blocks.14.ca.mat_q.weight, module.blocks.14.ca.mat_kv.weight, module.blocks.14.ca.proj.weight, '\n"
                   " 'module.blocks.14.ffn.fc1.weight, module.blocks.14.ffn.fc2.weight, module.blocks.14.ca_norm.weight, module.blocks.15.sa.mat_qkv.weight, module.blocks.15.sa.proj.weight, '\n"
                   " 'module.blocks.15.ca.mat_q.weight, module.blocks.15.ca.mat_kv.weight, module.blocks.15.ca.proj.weight, module.blocks.15.ffn.fc1.weight, module.blocks.15.ffn.fc2.weight, '\n"
                   " 'module.blocks.15.ca_norm.weight, module.blocks.16.sa.mat_qkv.weight, module.blocks.16.sa.proj.weight, module.blocks.16.ca.mat_q.weight, module.blocks.16.ca.mat_kv.weight, '\n"
                   " 'module.blocks.16.ca.proj.weight, module.blocks.16.ffn.fc1.weight, module.blocks.16.ffn.fc2.weight, module.blocks.16.ca_norm.weight, module.blocks.17.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.17.sa.proj.weight, module.blocks.17.ca.mat_q.weight, module.blocks.17.ca.mat_kv.weight, module.blocks.17.ca.proj.weight, module.blocks.17.ffn.fc1.weight, '\n"
                   " 'module.blocks.17.ffn.fc2.weight, module.blocks.17.ca_norm.weight, module.blocks.18.sa.mat_qkv.weight, module.blocks.18.sa.proj.weight, module.blocks.18.ca.mat_q.weight, '\n"
                   " 'module.blocks.18.ca.mat_kv.weight, module.blocks.18.ca.proj.weight, module.blocks.18.ffn.fc1.weight, module.blocks.18.ffn.fc2.weight, module.blocks.18.ca_norm.weight, '\n"
                   " 'module.blocks.19.sa.mat_qkv.weight, module.blocks.19.sa.proj.weight, module.blocks.19.ca.mat_q.weight, module.blocks.19.ca.mat_kv.weight, module.blocks.19.ca.proj.weight, '\n"
                   " 'module.blocks.19.ffn.fc1.weight, module.blocks.19.ffn.fc2.weight, module.blocks.19.ca_norm.weight, module.blocks.20.sa.mat_qkv.weight, module.blocks.20.sa.proj.weight, '\n"
                   " 'module.blocks.20.ca.mat_q.weight, module.blocks.20.ca.mat_kv.weight, module.blocks.20.ca.proj.weight, module.blocks.20.ffn.fc1.weight, module.blocks.20.ffn.fc2.weight, '\n"
                   " 'module.blocks.20.ca_norm.weight, module.blocks.21.sa.mat_qkv.weight, module.blocks.21.sa.proj.weight, module.blocks.21.ca.mat_q.weight, module.blocks.21.ca.mat_kv.weight, '\n"
                   " 'module.blocks.21.ca.proj.weight, module.blocks.21.ffn.fc1.weight, module.blocks.21.ffn.fc2.weight, module.blocks.21.ca_norm.weight, module.blocks.22.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.22.sa.proj.weight, module.blocks.22.ca.mat_q.weight, module.blocks.22.ca.mat_kv.weight, module.blocks.22.ca.proj.weight, module.blocks.22.ffn.fc1.weight, '\n"
                   " 'module.blocks.22.ffn.fc2.weight, module.blocks.22.ca_norm.weight, module.blocks.23.sa.mat_qkv.weight, module.blocks.23.sa.proj.weight, module.blocks.23.ca.mat_q.weight, '\n"
                   " 'module.blocks.23.ca.mat_kv.weight, module.blocks.23.ca.proj.weight, module.blocks.23.ffn.fc1.weight, module.blocks.23.ffn.fc2.weight, module.blocks.23.ca_norm.weight, '\n"
                   " 'module.blocks.24.sa.mat_qkv.weight, module.blocks.24.sa.proj.weight, module.blocks.24.ca.mat_q.weight, module.blocks.24.ca.mat_kv.weight, module.blocks.24.ca.proj.weight, '\n"
                   " 'module.blocks.24.ffn.fc1.weight, module.blocks.24.ffn.fc2.weight, module.blocks.24.ca_norm.weight, module.blocks.25.sa.mat_qkv.weight, module.blocks.25.sa.proj.weight, '\n"
                   " 'module.blocks.25.ca.mat_q.weight, module.blocks.25.ca.mat_kv.weight, module.blocks.25.ca.proj.weight, module.blocks.25.ffn.fc1.weight, module.blocks.25.ffn.fc2.weight, '\n"
                   " 'module.blocks.25.ca_norm.weight, module.blocks.26.sa.mat_qkv.weight, module.blocks.26.sa.proj.weight, module.blocks.26.ca.mat_q.weight, module.blocks.26.ca.mat_kv.weight, '\n"
                   " 'module.blocks.26.ca.proj.weight, module.blocks.26.ffn.fc1.weight, module.blocks.26.ffn.fc2.weight, module.blocks.26.ca_norm.weight, module.blocks.27.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.27.sa.proj.weight, module.blocks.27.ca.mat_q.weight, module.blocks.27.ca.mat_kv.weight, module.blocks.27.ca.proj.weight, module.blocks.27.ffn.fc1.weight, '\n"
                   " 'module.blocks.27.ffn.fc2.weight, module.blocks.27.ca_norm.weight, module.blocks.28.sa.mat_qkv.weight, module.blocks.28.sa.proj.weight, module.blocks.28.ca.mat_q.weight, '\n"
                   " 'module.blocks.28.ca.mat_kv.weight, module.blocks.28.ca.proj.weight, module.blocks.28.ffn.fc1.weight, module.blocks.28.ffn.fc2.weight, module.blocks.28.ca_norm.weight, '\n"
                   " 'module.blocks.29.sa.mat_qkv.weight, module.blocks.29.sa.proj.weight, module.blocks.29.ca.mat_q.weight, module.blocks.29.ca.mat_kv.weight, module.blocks.29.ca.proj.weight, '\n"
                   " 'module.blocks.29.ffn.fc1.weight, module.blocks.29.ffn.fc2.weight, module.blocks.29.ca_norm.weight, module.blocks.30.sa.mat_qkv.weight, module.blocks.30.sa.proj.weight, '\n"
                   " 'module.blocks.30.ca.mat_q.weight, module.blocks.30.ca.mat_kv.weight, module.blocks.30.ca.proj.weight, module.blocks.30.ffn.fc1.weight, module.blocks.30.ffn.fc2.weight, '\n"
                   " 'module.blocks.30.ca_norm.weight, module.blocks.31.sa.mat_qkv.weight, module.blocks.31.sa.proj.weight, module.blocks.31.ca.mat_q.weight, module.blocks.31.ca.mat_kv.weight, '\n"
                   " 'module.blocks.31.ca.proj.weight, module.blocks.31.ffn.fc1.weight, module.blocks.31.ffn.fc2.weight, module.blocks.31.ca_norm.weight, module.blocks.32.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.32.sa.proj.weight, module.blocks.32.ca.mat_q.weight, module.blocks.32.ca.mat_kv.weight, module.blocks.32.ca.proj.weight, module.blocks.32.ffn.fc1.weight, '\n"
                   " 'module.blocks.32.ffn.fc2.weight, module.blocks.32.ca_norm.weight, module.blocks.33.sa.mat_qkv.weight, module.blocks.33.sa.proj.weight, module.blocks.33.ca.mat_q.weight, '\n"
                   " 'module.blocks.33.ca.mat_kv.weight, module.blocks.33.ca.proj.weight, module.blocks.33.ffn.fc1.weight, module.blocks.33.ffn.fc2.weight, module.blocks.33.ca_norm.weight, '\n"
                   " 'module.blocks.34.sa.mat_qkv.weight, module.blocks.34.sa.proj.weight, module.blocks.34.ca.mat_q.weight, module.blocks.34.ca.mat_kv.weight, module.blocks.34.ca.proj.weight, '\n"
                   " 'module.blocks.34.ffn.fc1.weight, module.blocks.34.ffn.fc2.weight, module.blocks.34.ca_norm.weight, module.blocks.35.sa.mat_qkv.weight, module.blocks.35.sa.proj.weight, '\n"
                   " 'module.blocks.35.ca.mat_q.weight, module.blocks.35.ca.mat_kv.weight, module.blocks.35.ca.proj.weight, module.blocks.35.ffn.fc1.weight, module.blocks.35.ffn.fc2.weight, '\n"
                   " 'module.blocks.35.ca_norm.weight, module.blocks.36.sa.mat_qkv.weight, module.blocks.36.sa.proj.weight, module.blocks.36.ca.mat_q.weight, module.blocks.36.ca.mat_kv.weight, '\n"
                   " 'module.blocks.36.ca.proj.weight, module.blocks.36.ffn.fc1.weight, module.blocks.36.ffn.fc2.weight, module.blocks.36.ca_norm.weight, module.blocks.37.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.37.sa.proj.weight, module.blocks.37.ca.mat_q.weight, module.blocks.37.ca.mat_kv.weight, module.blocks.37.ca.proj.weight, module.blocks.37.ffn.fc1.weight, '\n"
                   " 'module.blocks.37.ffn.fc2.weight, module.blocks.37.ca_norm.weight, module.blocks.38.sa.mat_qkv.weight, module.blocks.38.sa.proj.weight, module.blocks.38.ca.mat_q.weight, '\n"
                   " 'module.blocks.38.ca.mat_kv.weight, module.blocks.38.ca.proj.weight, module.blocks.38.ffn.fc1.weight, module.blocks.38.ffn.fc2.weight, module.blocks.38.ca_norm.weight, '\n"
                   " 'module.blocks.39.sa.mat_qkv.weight, module.blocks.39.sa.proj.weight, module.blocks.39.ca.mat_q.weight, module.blocks.39.ca.mat_kv.weight, module.blocks.39.ca.proj.weight, '\n"
                   " 'module.blocks.39.ffn.fc1.weight, module.blocks.39.ffn.fc2.weight, module.blocks.39.ca_norm.weight, module.blocks.40.sa.mat_qkv.weight, module.blocks.40.sa.proj.weight, '\n"
                   " 'module.blocks.40.ca.mat_q.weight, module.blocks.40.ca.mat_kv.weight, module.blocks.40.ca.proj.weight, module.blocks.40.ffn.fc1.weight, module.blocks.40.ffn.fc2.weight, '\n"
                   " 'module.blocks.40.ca_norm.weight, module.blocks.41.sa.mat_qkv.weight, module.blocks.41.sa.proj.weight, module.blocks.41.ca.mat_q.weight, module.blocks.41.ca.mat_kv.weight, '\n"
                   " 'module.blocks.41.ca.proj.weight, module.blocks.41.ffn.fc1.weight, module.blocks.41.ffn.fc2.weight, module.blocks.41.ca_norm.weight, module.blocks.42.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.42.sa.proj.weight, module.blocks.42.ca.mat_q.weight, module.blocks.42.ca.mat_kv.weight, module.blocks.42.ca.proj.weight, module.blocks.42.ffn.fc1.weight, '\n"
                   " 'module.blocks.42.ffn.fc2.weight, module.blocks.42.ca_norm.weight, module.blocks.43.sa.mat_qkv.weight, module.blocks.43.sa.proj.weight, module.blocks.43.ca.mat_q.weight, '\n"
                   " 'module.blocks.43.ca.mat_kv.weight, module.blocks.43.ca.proj.weight, module.blocks.43.ffn.fc1.weight, module.blocks.43.ffn.fc2.weight, module.blocks.43.ca_norm.weight, '\n"
                   " 'module.blocks.44.sa.mat_qkv.weight, module.blocks.44.sa.proj.weight, module.blocks.44.ca.mat_q.weight, module.blocks.44.ca.mat_kv.weight, module.blocks.44.ca.proj.weight, '\n"
                   " 'module.blocks.44.ffn.fc1.weight, module.blocks.44.ffn.fc2.weight, module.blocks.44.ca_norm.weight, module.blocks.45.sa.mat_qkv.weight, module.blocks.45.sa.proj.weight, '\n"
                   " 'module.blocks.45.ca.mat_q.weight, module.blocks.45.ca.mat_kv.weight, module.blocks.45.ca.proj.weight, module.blocks.45.ffn.fc1.weight, module.blocks.45.ffn.fc2.weight, '\n"
                   " 'module.blocks.45.ca_norm.weight, module.blocks.46.sa.mat_qkv.weight, module.blocks.46.sa.proj.weight, module.blocks.46.ca.mat_q.weight, module.blocks.46.ca.mat_kv.weight, '\n"
                   " 'module.blocks.46.ca.proj.weight, module.blocks.46.ffn.fc1.weight, module.blocks.46.ffn.fc2.weight, module.blocks.46.ca_norm.weight, module.blocks.47.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.47.sa.proj.weight, module.blocks.47.ca.mat_q.weight, module.blocks.47.ca.mat_kv.weight, module.blocks.47.ca.proj.weight, module.blocks.47.ffn.fc1.weight, '\n"
                   " 'module.blocks.47.ffn.fc2.weight, module.blocks.47.ca_norm.weight, module.blocks.48.sa.mat_qkv.weight, module.blocks.48.sa.proj.weight, module.blocks.48.ca.mat_q.weight, '\n"
                   " 'module.blocks.48.ca.mat_kv.weight, module.blocks.48.ca.proj.weight, module.blocks.48.ffn.fc1.weight, module.blocks.48.ffn.fc2.weight, module.blocks.48.ca_norm.weight, '\n"
                   " 'module.blocks.49.sa.mat_qkv.weight, module.blocks.49.sa.proj.weight, module.blocks.49.ca.mat_q.weight, module.blocks.49.ca.mat_kv.weight, module.blocks.49.ca.proj.weight, '\n"
                   " 'module.blocks.49.ffn.fc1.weight, module.blocks.49.ffn.fc2.weight, module.blocks.49.ca_norm.weight, module.blocks.50.sa.mat_qkv.weight, module.blocks.50.sa.proj.weight, '\n"
                   " 'module.blocks.50.ca.mat_q.weight, module.blocks.50.ca.mat_kv.weight, module.blocks.50.ca.proj.weight, module.blocks.50.ffn.fc1.weight, module.blocks.50.ffn.fc2.weight, '\n"
                   " 'module.blocks.50.ca_norm.weight, module.blocks.51.sa.mat_qkv.weight, module.blocks.51.sa.proj.weight, module.blocks.51.ca.mat_q.weight, module.blocks.51.ca.mat_kv.weight, '\n"
                   " 'module.blocks.51.ca.proj.weight, module.blocks.51.ffn.fc1.weight, module.blocks.51.ffn.fc2.weight, module.blocks.51.ca_norm.weight, module.blocks.52.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.52.sa.proj.weight, module.blocks.52.ca.mat_q.weight, module.blocks.52.ca.mat_kv.weight, module.blocks.52.ca.proj.weight, module.blocks.52.ffn.fc1.weight, '\n"
                   " 'module.blocks.52.ffn.fc2.weight, module.blocks.52.ca_norm.weight, module.blocks.53.sa.mat_qkv.weight, module.blocks.53.sa.proj.weight, module.blocks.53.ca.mat_q.weight, '\n"
                   " 'module.blocks.53.ca.mat_kv.weight, module.blocks.53.ca.proj.weight, module.blocks.53.ffn.fc1.weight, module.blocks.53.ffn.fc2.weight, module.blocks.53.ca_norm.weight, '\n"
                   " 'module.blocks.54.sa.mat_qkv.weight, module.blocks.54.sa.proj.weight, module.blocks.54.ca.mat_q.weight, module.blocks.54.ca.mat_kv.weight, module.blocks.54.ca.proj.weight, '\n"
                   " 'module.blocks.54.ffn.fc1.weight, module.blocks.54.ffn.fc2.weight, module.blocks.54.ca_norm.weight, module.blocks.55.sa.mat_qkv.weight, module.blocks.55.sa.proj.weight, '\n"
                   " 'module.blocks.55.ca.mat_q.weight, module.blocks.55.ca.mat_kv.weight, module.blocks.55.ca.proj.weight, module.blocks.55.ffn.fc1.weight, module.blocks.55.ffn.fc2.weight, '\n"
                   " 'module.blocks.55.ca_norm.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': '[no scale]',
          'params': "('module.cfg_uncond, module.pos_start, module.text_proj_for_sos.ca.mat_q, module.text_proj_for_sos.ca.v_bias, module.text_proj_for_sos.ca.proj.bias, module.text_proj_for_ca.0.bias, '\n"
                    " 'module.text_proj_for_ca.2.bias, module.lvl_embed.weight, module.word_embed.bias, module.shared_ada_lin.1.bias, module.head_nm.ada_lin.1.bias, module.head.bias, module.blocks.0.ada_gss, '\n"
                    " 'module.blocks.0.sa.scale_mul_1H11, module.blocks.0.sa.q_bias, module.blocks.0.sa.v_bias, module.blocks.0.sa.proj.bias, module.blocks.0.ca.v_bias, module.blocks.0.ca.mat_q.bias, '\n"
                    " 'module.blocks.0.ca.proj.bias, module.blocks.0.ffn.fc1.bias, module.blocks.0.ffn.fc2.bias, module.blocks.0.ca_norm.bias, module.blocks.1.ada_gss, module.blocks.1.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.1.sa.q_bias, module.blocks.1.sa.v_bias, module.blocks.1.sa.proj.bias, module.blocks.1.ca.v_bias, module.blocks.1.ca.mat_q.bias, module.blocks.1.ca.proj.bias, '\n"
                    " 'module.blocks.1.ffn.fc1.bias, module.blocks.1.ffn.fc2.bias, module.blocks.1.ca_norm.bias, module.blocks.2.ada_gss, module.blocks.2.sa.scale_mul_1H11, module.blocks.2.sa.q_bias, '\n"
                    " 'module.blocks.2.sa.v_bias, module.blocks.2.sa.proj.bias, module.blocks.2.ca.v_bias, module.blocks.2.ca.mat_q.bias, module.blocks.2.ca.proj.bias, module.blocks.2.ffn.fc1.bias, '\n"
                    " 'module.blocks.2.ffn.fc2.bias, module.blocks.2.ca_norm.bias, module.blocks.3.ada_gss, module.blocks.3.sa.scale_mul_1H11, module.blocks.3.sa.q_bias, module.blocks.3.sa.v_bias, '\n"
                    " 'module.blocks.3.sa.proj.bias, module.blocks.3.ca.v_bias, module.blocks.3.ca.mat_q.bias, module.blocks.3.ca.proj.bias, module.blocks.3.ffn.fc1.bias, module.blocks.3.ffn.fc2.bias, '\n"
                    " 'module.blocks.3.ca_norm.bias, module.blocks.4.ada_gss, module.blocks.4.sa.scale_mul_1H11, module.blocks.4.sa.q_bias, module.blocks.4.sa.v_bias, module.blocks.4.sa.proj.bias, '\n"
                    " 'module.blocks.4.ca.v_bias, module.blocks.4.ca.mat_q.bias, module.blocks.4.ca.proj.bias, module.blocks.4.ffn.fc1.bias, module.blocks.4.ffn.fc2.bias, module.blocks.4.ca_norm.bias, '\n"
                    " 'module.blocks.5.ada_gss, module.blocks.5.sa.scale_mul_1H11, module.blocks.5.sa.q_bias, module.blocks.5.sa.v_bias, module.blocks.5.sa.proj.bias, module.blocks.5.ca.v_bias, '\n"
                    " 'module.blocks.5.ca.mat_q.bias, module.blocks.5.ca.proj.bias, module.blocks.5.ffn.fc1.bias, module.blocks.5.ffn.fc2.bias, module.blocks.5.ca_norm.bias, module.blocks.6.ada_gss, '\n"
                    " 'module.blocks.6.sa.scale_mul_1H11, module.blocks.6.sa.q_bias, module.blocks.6.sa.v_bias, module.blocks.6.sa.proj.bias, module.blocks.6.ca.v_bias, module.blocks.6.ca.mat_q.bias, '\n"
                    " 'module.blocks.6.ca.proj.bias, module.blocks.6.ffn.fc1.bias, module.blocks.6.ffn.fc2.bias, module.blocks.6.ca_norm.bias, module.blocks.7.ada_gss, module.blocks.7.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.7.sa.q_bias, module.blocks.7.sa.v_bias, module.blocks.7.sa.proj.bias, module.blocks.7.ca.v_bias, module.blocks.7.ca.mat_q.bias, module.blocks.7.ca.proj.bias, '\n"
                    " 'module.blocks.7.ffn.fc1.bias, module.blocks.7.ffn.fc2.bias, module.blocks.7.ca_norm.bias, module.blocks.8.ada_gss, module.blocks.8.sa.scale_mul_1H11, module.blocks.8.sa.q_bias, '\n"
                    " 'module.blocks.8.sa.v_bias, module.blocks.8.sa.proj.bias, module.blocks.8.ca.v_bias, module.blocks.8.ca.mat_q.bias, module.blocks.8.ca.proj.bias, module.blocks.8.ffn.fc1.bias, '\n"
                    " 'module.blocks.8.ffn.fc2.bias, module.blocks.8.ca_norm.bias, module.blocks.9.ada_gss, module.blocks.9.sa.scale_mul_1H11, module.blocks.9.sa.q_bias, module.blocks.9.sa.v_bias, '\n"
                    " 'module.blocks.9.sa.proj.bias, module.blocks.9.ca.v_bias, module.blocks.9.ca.mat_q.bias, module.blocks.9.ca.proj.bias, module.blocks.9.ffn.fc1.bias, module.blocks.9.ffn.fc2.bias, '\n"
                    " 'module.blocks.9.ca_norm.bias, module.blocks.10.ada_gss, module.blocks.10.sa.scale_mul_1H11, module.blocks.10.sa.q_bias, module.blocks.10.sa.v_bias, module.blocks.10.sa.proj.bias, '\n"
                    " 'module.blocks.10.ca.v_bias, module.blocks.10.ca.mat_q.bias, module.blocks.10.ca.proj.bias, module.blocks.10.ffn.fc1.bias, module.blocks.10.ffn.fc2.bias, module.blocks.10.ca_norm.bias, '\n"
                    " 'module.blocks.11.ada_gss, module.blocks.11.sa.scale_mul_1H11, module.blocks.11.sa.q_bias, module.blocks.11.sa.v_bias, module.blocks.11.sa.proj.bias, module.blocks.11.ca.v_bias, '\n"
                    " 'module.blocks.11.ca.mat_q.bias, module.blocks.11.ca.proj.bias, module.blocks.11.ffn.fc1.bias, module.blocks.11.ffn.fc2.bias, module.blocks.11.ca_norm.bias, module.blocks.12.ada_gss, '\n"
                    " 'module.blocks.12.sa.scale_mul_1H11, module.blocks.12.sa.q_bias, module.blocks.12.sa.v_bias, module.blocks.12.sa.proj.bias, module.blocks.12.ca.v_bias, module.blocks.12.ca.mat_q.bias, '\n"
                    " 'module.blocks.12.ca.proj.bias, module.blocks.12.ffn.fc1.bias, module.blocks.12.ffn.fc2.bias, module.blocks.12.ca_norm.bias, module.blocks.13.ada_gss, module.blocks.13.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.13.sa.q_bias, module.blocks.13.sa.v_bias, module.blocks.13.sa.proj.bias, module.blocks.13.ca.v_bias, module.blocks.13.ca.mat_q.bias, module.blocks.13.ca.proj.bias, '\n"
                    " 'module.blocks.13.ffn.fc1.bias, module.blocks.13.ffn.fc2.bias, module.blocks.13.ca_norm.bias, module.blocks.14.ada_gss, module.blocks.14.sa.scale_mul_1H11, module.blocks.14.sa.q_bias, '\n"
                    " 'module.blocks.14.sa.v_bias, module.blocks.14.sa.proj.bias, module.blocks.14.ca.v_bias, module.blocks.14.ca.mat_q.bias, module.blocks.14.ca.proj.bias, module.blocks.14.ffn.fc1.bias, '\n"
                    " 'module.blocks.14.ffn.fc2.bias, module.blocks.14.ca_norm.bias, module.blocks.15.ada_gss, module.blocks.15.sa.scale_mul_1H11, module.blocks.15.sa.q_bias, module.blocks.15.sa.v_bias, '\n"
                    " 'module.blocks.15.sa.proj.bias, module.blocks.15.ca.v_bias, module.blocks.15.ca.mat_q.bias, module.blocks.15.ca.proj.bias, module.blocks.15.ffn.fc1.bias, module.blocks.15.ffn.fc2.bias, '\n"
                    " 'module.blocks.15.ca_norm.bias, module.blocks.16.ada_gss, module.blocks.16.sa.scale_mul_1H11, module.blocks.16.sa.q_bias, module.blocks.16.sa.v_bias, module.blocks.16.sa.proj.bias, '\n"
                    " 'module.blocks.16.ca.v_bias, module.blocks.16.ca.mat_q.bias, module.blocks.16.ca.proj.bias, module.blocks.16.ffn.fc1.bias, module.blocks.16.ffn.fc2.bias, module.blocks.16.ca_norm.bias, '\n"
                    " 'module.blocks.17.ada_gss, module.blocks.17.sa.scale_mul_1H11, module.blocks.17.sa.q_bias, module.blocks.17.sa.v_bias, module.blocks.17.sa.proj.bias, module.blocks.17.ca.v_bias, '\n"
                    " 'module.blocks.17.ca.mat_q.bias, module.blocks.17.ca.proj.bias, module.blocks.17.ffn.fc1.bias, module.blocks.17.ffn.fc2.bias, module.blocks.17.ca_norm.bias, module.blocks.18.ada_gss, '\n"
                    " 'module.blocks.18.sa.scale_mul_1H11, module.blocks.18.sa.q_bias, module.blocks.18.sa.v_bias, module.blocks.18.sa.proj.bias, module.blocks.18.ca.v_bias, module.blocks.18.ca.mat_q.bias, '\n"
                    " 'module.blocks.18.ca.proj.bias, module.blocks.18.ffn.fc1.bias, module.blocks.18.ffn.fc2.bias, module.blocks.18.ca_norm.bias, module.blocks.19.ada_gss, module.blocks.19.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.19.sa.q_bias, module.blocks.19.sa.v_bias, module.blocks.19.sa.proj.bias, module.blocks.19.ca.v_bias, module.blocks.19.ca.mat_q.bias, module.blocks.19.ca.proj.bias, '\n"
                    " 'module.blocks.19.ffn.fc1.bias, module.blocks.19.ffn.fc2.bias, module.blocks.19.ca_norm.bias, module.blocks.20.ada_gss, module.blocks.20.sa.scale_mul_1H11, module.blocks.20.sa.q_bias, '\n"
                    " 'module.blocks.20.sa.v_bias, module.blocks.20.sa.proj.bias, module.blocks.20.ca.v_bias, module.blocks.20.ca.mat_q.bias, module.blocks.20.ca.proj.bias, module.blocks.20.ffn.fc1.bias, '\n"
                    " 'module.blocks.20.ffn.fc2.bias, module.blocks.20.ca_norm.bias, module.blocks.21.ada_gss, module.blocks.21.sa.scale_mul_1H11, module.blocks.21.sa.q_bias, module.blocks.21.sa.v_bias, '\n"
                    " 'module.blocks.21.sa.proj.bias, module.blocks.21.ca.v_bias, module.blocks.21.ca.mat_q.bias, module.blocks.21.ca.proj.bias, module.blocks.21.ffn.fc1.bias, module.blocks.21.ffn.fc2.bias, '\n"
                    " 'module.blocks.21.ca_norm.bias, module.blocks.22.ada_gss, module.blocks.22.sa.scale_mul_1H11, module.blocks.22.sa.q_bias, module.blocks.22.sa.v_bias, module.blocks.22.sa.proj.bias, '\n"
                    " 'module.blocks.22.ca.v_bias, module.blocks.22.ca.mat_q.bias, module.blocks.22.ca.proj.bias, module.blocks.22.ffn.fc1.bias, module.blocks.22.ffn.fc2.bias, module.blocks.22.ca_norm.bias, '\n"
                    " 'module.blocks.23.ada_gss, module.blocks.23.sa.scale_mul_1H11, module.blocks.23.sa.q_bias, module.blocks.23.sa.v_bias, module.blocks.23.sa.proj.bias, module.blocks.23.ca.v_bias, '\n"
                    " 'module.blocks.23.ca.mat_q.bias, module.blocks.23.ca.proj.bias, module.blocks.23.ffn.fc1.bias, module.blocks.23.ffn.fc2.bias, module.blocks.23.ca_norm.bias, module.blocks.24.ada_gss, '\n"
                    " 'module.blocks.24.sa.scale_mul_1H11, module.blocks.24.sa.q_bias, module.blocks.24.sa.v_bias, module.blocks.24.sa.proj.bias, module.blocks.24.ca.v_bias, module.blocks.24.ca.mat_q.bias, '\n"
                    " 'module.blocks.24.ca.proj.bias, module.blocks.24.ffn.fc1.bias, module.blocks.24.ffn.fc2.bias, module.blocks.24.ca_norm.bias, module.blocks.25.ada_gss, module.blocks.25.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.25.sa.q_bias, module.blocks.25.sa.v_bias, module.blocks.25.sa.proj.bias, module.blocks.25.ca.v_bias, module.blocks.25.ca.mat_q.bias, module.blocks.25.ca.proj.bias, '\n"
                    " 'module.blocks.25.ffn.fc1.bias, module.blocks.25.ffn.fc2.bias, module.blocks.25.ca_norm.bias, module.blocks.26.ada_gss, module.blocks.26.sa.scale_mul_1H11, module.blocks.26.sa.q_bias, '\n"
                    " 'module.blocks.26.sa.v_bias, module.blocks.26.sa.proj.bias, module.blocks.26.ca.v_bias, module.blocks.26.ca.mat_q.bias, module.blocks.26.ca.proj.bias, module.blocks.26.ffn.fc1.bias, '\n"
                    " 'module.blocks.26.ffn.fc2.bias, module.blocks.26.ca_norm.bias, module.blocks.27.ada_gss, module.blocks.27.sa.scale_mul_1H11, module.blocks.27.sa.q_bias, module.blocks.27.sa.v_bias, '\n"
                    " 'module.blocks.27.sa.proj.bias, module.blocks.27.ca.v_bias, module.blocks.27.ca.mat_q.bias, module.blocks.27.ca.proj.bias, module.blocks.27.ffn.fc1.bias, module.blocks.27.ffn.fc2.bias, '\n"
                    " 'module.blocks.27.ca_norm.bias, module.blocks.28.ada_gss, module.blocks.28.sa.scale_mul_1H11, module.blocks.28.sa.q_bias, module.blocks.28.sa.v_bias, module.blocks.28.sa.proj.bias, '\n"
                    " 'module.blocks.28.ca.v_bias, module.blocks.28.ca.mat_q.bias, module.blocks.28.ca.proj.bias, module.blocks.28.ffn.fc1.bias, module.blocks.28.ffn.fc2.bias, module.blocks.28.ca_norm.bias, '\n"
                    " 'module.blocks.29.ada_gss, module.blocks.29.sa.scale_mul_1H11, module.blocks.29.sa.q_bias, module.blocks.29.sa.v_bias, module.blocks.29.sa.proj.bias, module.blocks.29.ca.v_bias, '\n"
                    " 'module.blocks.29.ca.mat_q.bias, module.blocks.29.ca.proj.bias, module.blocks.29.ffn.fc1.bias, module.blocks.29.ffn.fc2.bias, module.blocks.29.ca_norm.bias, module.blocks.30.ada_gss, '\n"
                    " 'module.blocks.30.sa.scale_mul_1H11, module.blocks.30.sa.q_bias, module.blocks.30.sa.v_bias, module.blocks.30.sa.proj.bias, module.blocks.30.ca.v_bias, module.blocks.30.ca.mat_q.bias, '\n"
                    " 'module.blocks.30.ca.proj.bias, module.blocks.30.ffn.fc1.bias, module.blocks.30.ffn.fc2.bias, module.blocks.30.ca_norm.bias, module.blocks.31.ada_gss, module.blocks.31.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.31.sa.q_bias, module.blocks.31.sa.v_bias, module.blocks.31.sa.proj.bias, module.blocks.31.ca.v_bias, module.blocks.31.ca.mat_q.bias, module.blocks.31.ca.proj.bias, '\n"
                    " 'module.blocks.31.ffn.fc1.bias, module.blocks.31.ffn.fc2.bias, module.blocks.31.ca_norm.bias, module.blocks.32.ada_gss, module.blocks.32.sa.scale_mul_1H11, module.blocks.32.sa.q_bias, '\n"
                    " 'module.blocks.32.sa.v_bias, module.blocks.32.sa.proj.bias, module.blocks.32.ca.v_bias, module.blocks.32.ca.mat_q.bias, module.blocks.32.ca.proj.bias, module.blocks.32.ffn.fc1.bias, '\n"
                    " 'module.blocks.32.ffn.fc2.bias, module.blocks.32.ca_norm.bias, module.blocks.33.ada_gss, module.blocks.33.sa.scale_mul_1H11, module.blocks.33.sa.q_bias, module.blocks.33.sa.v_bias, '\n"
                    " 'module.blocks.33.sa.proj.bias, module.blocks.33.ca.v_bias, module.blocks.33.ca.mat_q.bias, module.blocks.33.ca.proj.bias, module.blocks.33.ffn.fc1.bias, module.blocks.33.ffn.fc2.bias, '\n"
                    " 'module.blocks.33.ca_norm.bias, module.blocks.34.ada_gss, module.blocks.34.sa.scale_mul_1H11, module.blocks.34.sa.q_bias, module.blocks.34.sa.v_bias, module.blocks.34.sa.proj.bias, '\n"
                    " 'module.blocks.34.ca.v_bias, module.blocks.34.ca.mat_q.bias, module.blocks.34.ca.proj.bias, module.blocks.34.ffn.fc1.bias, module.blocks.34.ffn.fc2.bias, module.blocks.34.ca_norm.bias, '\n"
                    " 'module.blocks.35.ada_gss, module.blocks.35.sa.scale_mul_1H11, module.blocks.35.sa.q_bias, module.blocks.35.sa.v_bias, module.blocks.35.sa.proj.bias, module.blocks.35.ca.v_bias, '\n"
                    " 'module.blocks.35.ca.mat_q.bias, module.blocks.35.ca.proj.bias, module.blocks.35.ffn.fc1.bias, module.blocks.35.ffn.fc2.bias, module.blocks.35.ca_norm.bias, module.blocks.36.ada_gss, '\n"
                    " 'module.blocks.36.sa.scale_mul_1H11, module.blocks.36.sa.q_bias, module.blocks.36.sa.v_bias, module.blocks.36.sa.proj.bias, module.blocks.36.ca.v_bias, module.blocks.36.ca.mat_q.bias, '\n"
                    " 'module.blocks.36.ca.proj.bias, module.blocks.36.ffn.fc1.bias, module.blocks.36.ffn.fc2.bias, module.blocks.36.ca_norm.bias, module.blocks.37.ada_gss, module.blocks.37.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.37.sa.q_bias, module.blocks.37.sa.v_bias, module.blocks.37.sa.proj.bias, module.blocks.37.ca.v_bias, module.blocks.37.ca.mat_q.bias, module.blocks.37.ca.proj.bias, '\n"
                    " 'module.blocks.37.ffn.fc1.bias, module.blocks.37.ffn.fc2.bias, module.blocks.37.ca_norm.bias, module.blocks.38.ada_gss, module.blocks.38.sa.scale_mul_1H11, module.blocks.38.sa.q_bias, '\n"
                    " 'module.blocks.38.sa.v_bias, module.blocks.38.sa.proj.bias, module.blocks.38.ca.v_bias, module.blocks.38.ca.mat_q.bias, module.blocks.38.ca.proj.bias, module.blocks.38.ffn.fc1.bias, '\n"
                    " 'module.blocks.38.ffn.fc2.bias, module.blocks.38.ca_norm.bias, module.blocks.39.ada_gss, module.blocks.39.sa.scale_mul_1H11, module.blocks.39.sa.q_bias, module.blocks.39.sa.v_bias, '\n"
                    " 'module.blocks.39.sa.proj.bias, module.blocks.39.ca.v_bias, module.blocks.39.ca.mat_q.bias, module.blocks.39.ca.proj.bias, module.blocks.39.ffn.fc1.bias, module.blocks.39.ffn.fc2.bias, '\n"
                    " 'module.blocks.39.ca_norm.bias, module.blocks.40.ada_gss, module.blocks.40.sa.scale_mul_1H11, module.blocks.40.sa.q_bias, module.blocks.40.sa.v_bias, module.blocks.40.sa.proj.bias, '\n"
                    " 'module.blocks.40.ca.v_bias, module.blocks.40.ca.mat_q.bias, module.blocks.40.ca.proj.bias, module.blocks.40.ffn.fc1.bias, module.blocks.40.ffn.fc2.bias, module.blocks.40.ca_norm.bias, '\n"
                    " 'module.blocks.41.ada_gss, module.blocks.41.sa.scale_mul_1H11, module.blocks.41.sa.q_bias, module.blocks.41.sa.v_bias, module.blocks.41.sa.proj.bias, module.blocks.41.ca.v_bias, '\n"
                    " 'module.blocks.41.ca.mat_q.bias, module.blocks.41.ca.proj.bias, module.blocks.41.ffn.fc1.bias, module.blocks.41.ffn.fc2.bias, module.blocks.41.ca_norm.bias, module.blocks.42.ada_gss, '\n"
                    " 'module.blocks.42.sa.scale_mul_1H11, module.blocks.42.sa.q_bias, module.blocks.42.sa.v_bias, module.blocks.42.sa.proj.bias, module.blocks.42.ca.v_bias, module.blocks.42.ca.mat_q.bias, '\n"
                    " 'module.blocks.42.ca.proj.bias, module.blocks.42.ffn.fc1.bias, module.blocks.42.ffn.fc2.bias, module.blocks.42.ca_norm.bias, module.blocks.43.ada_gss, module.blocks.43.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.43.sa.q_bias, module.blocks.43.sa.v_bias, module.blocks.43.sa.proj.bias, module.blocks.43.ca.v_bias, module.blocks.43.ca.mat_q.bias, module.blocks.43.ca.proj.bias, '\n"
                    " 'module.blocks.43.ffn.fc1.bias, module.blocks.43.ffn.fc2.bias, module.blocks.43.ca_norm.bias, module.blocks.44.ada_gss, module.blocks.44.sa.scale_mul_1H11, module.blocks.44.sa.q_bias, '\n"
                    " 'module.blocks.44.sa.v_bias, module.blocks.44.sa.proj.bias, module.blocks.44.ca.v_bias, module.blocks.44.ca.mat_q.bias, module.blocks.44.ca.proj.bias, module.blocks.44.ffn.fc1.bias, '\n"
                    " 'module.blocks.44.ffn.fc2.bias, module.blocks.44.ca_norm.bias, module.blocks.45.ada_gss, module.blocks.45.sa.scale_mul_1H11, module.blocks.45.sa.q_bias, module.blocks.45.sa.v_bias, '\n"
                    " 'module.blocks.45.sa.proj.bias, module.blocks.45.ca.v_bias, module.blocks.45.ca.mat_q.bias, module.blocks.45.ca.proj.bias, module.blocks.45.ffn.fc1.bias, module.blocks.45.ffn.fc2.bias, '\n"
                    " 'module.blocks.45.ca_norm.bias, module.blocks.46.ada_gss, module.blocks.46.sa.scale_mul_1H11, module.blocks.46.sa.q_bias, module.blocks.46.sa.v_bias, module.blocks.46.sa.proj.bias, '\n"
                    " 'module.blocks.46.ca.v_bias, module.blocks.46.ca.mat_q.bias, module.blocks.46.ca.proj.bias, module.blocks.46.ffn.fc1.bias, module.blocks.46.ffn.fc2.bias, module.blocks.46.ca_norm.bias, '\n"
                    " 'module.blocks.47.ada_gss, module.blocks.47.sa.scale_mul_1H11, module.blocks.47.sa.q_bias, module.blocks.47.sa.v_bias, module.blocks.47.sa.proj.bias, module.blocks.47.ca.v_bias, '\n"
                    " 'module.blocks.47.ca.mat_q.bias, module.blocks.47.ca.proj.bias, module.blocks.47.ffn.fc1.bias, module.blocks.47.ffn.fc2.bias, module.blocks.47.ca_norm.bias, module.blocks.48.ada_gss, '\n"
                    " 'module.blocks.48.sa.scale_mul_1H11, module.blocks.48.sa.q_bias, module.blocks.48.sa.v_bias, module.blocks.48.sa.proj.bias, module.blocks.48.ca.v_bias, module.blocks.48.ca.mat_q.bias, '\n"
                    " 'module.blocks.48.ca.proj.bias, module.blocks.48.ffn.fc1.bias, module.blocks.48.ffn.fc2.bias, module.blocks.48.ca_norm.bias, module.blocks.49.ada_gss, module.blocks.49.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.49.sa.q_bias, module.blocks.49.sa.v_bias, module.blocks.49.sa.proj.bias, module.blocks.49.ca.v_bias, module.blocks.49.ca.mat_q.bias, module.blocks.49.ca.proj.bias, '\n"
                    " 'module.blocks.49.ffn.fc1.bias, module.blocks.49.ffn.fc2.bias, module.blocks.49.ca_norm.bias, module.blocks.50.ada_gss, module.blocks.50.sa.scale_mul_1H11, module.blocks.50.sa.q_bias, '\n"
                    " 'module.blocks.50.sa.v_bias, module.blocks.50.sa.proj.bias, module.blocks.50.ca.v_bias, module.blocks.50.ca.mat_q.bias, module.blocks.50.ca.proj.bias, module.blocks.50.ffn.fc1.bias, '\n"
                    " 'module.blocks.50.ffn.fc2.bias, module.blocks.50.ca_norm.bias, module.blocks.51.ada_gss, module.blocks.51.sa.scale_mul_1H11, module.blocks.51.sa.q_bias, module.blocks.51.sa.v_bias, '\n"
                    " 'module.blocks.51.sa.proj.bias, module.blocks.51.ca.v_bias, module.blocks.51.ca.mat_q.bias, module.blocks.51.ca.proj.bias, module.blocks.51.ffn.fc1.bias, module.blocks.51.ffn.fc2.bias, '\n"
                    " 'module.blocks.51.ca_norm.bias, module.blocks.52.ada_gss, module.blocks.52.sa.scale_mul_1H11, module.blocks.52.sa.q_bias, module.blocks.52.sa.v_bias, module.blocks.52.sa.proj.bias, '\n"
                    " 'module.blocks.52.ca.v_bias, module.blocks.52.ca.mat_q.bias, module.blocks.52.ca.proj.bias, module.blocks.52.ffn.fc1.bias, module.blocks.52.ffn.fc2.bias, module.blocks.52.ca_norm.bias, '\n"
                    " 'module.blocks.53.ada_gss, module.blocks.53.sa.scale_mul_1H11, module.blocks.53.sa.q_bias, module.blocks.53.sa.v_bias, module.blocks.53.sa.proj.bias, module.blocks.53.ca.v_bias, '\n"
                    " 'module.blocks.53.ca.mat_q.bias, module.blocks.53.ca.proj.bias, module.blocks.53.ffn.fc1.bias, module.blocks.53.ffn.fc2.bias, module.blocks.53.ca_norm.bias, module.blocks.54.ada_gss, '\n"
                    " 'module.blocks.54.sa.scale_mul_1H11, module.blocks.54.sa.q_bias, module.blocks.54.sa.v_bias, module.blocks.54.sa.proj.bias, module.blocks.54.ca.v_bias, module.blocks.54.ca.mat_q.bias, '\n"
                    " 'module.blocks.54.ca.proj.bias, module.blocks.54.ffn.fc1.bias, module.blocks.54.ffn.fc2.bias, module.blocks.54.ca_norm.bias, module.blocks.55.ada_gss, module.blocks.55.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.55.sa.q_bias, module.blocks.55.sa.v_bias, module.blocks.55.sa.proj.bias, module.blocks.55.ca.v_bias, module.blocks.55.ca.mat_q.bias, module.blocks.55.ca.proj.bias, '\n"
                    " 'module.blocks.55.ffn.fc1.bias, module.blocks.55.ffn.fc2.bias, module.blocks.55.ca_norm.bias')",
          'wd_sc': 0.0}}

[09-07 00:08:00] (nity/utils/lr_control.py, line 116)=> [get_param_groups][rank0] type(model).__name__='NullDDP' count=1085, numel=8573892016
[09-07 00:08:00] (nity/utils/lr_control.py, line 117)=> 
[09-07 00:08:00] (inity_circuit_breaker.py, line 343)=> [vgpt] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.97), fused=True), opt_kw={'lr': 9.375e-05, 'weight_decay': 0}

[09-07 00:08:00] (inity_circuit_breaker.py, line 348)=> Loading T5 from google/flan-t5-xl...
[09-07 00:08:02] (_original/src/trainer.py, line  65)=> self.reweight_loss_by_scale: 1
[09-07 00:08:02] (inity_circuit_breaker.py, line 171)=> global bs=4, local bs=4
[09-07 00:08:02] (inity_circuit_breaker.py, line 172)=> initial args:
{
  local_out_path      : ./local_output
  data_path           : ./data
  bed                 : ./checkpoints
  vae_ckpt            : /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
  exp_name            : experiment
  ds                  : oi
  model               : infinity_8b
  short_cap_prob      : 0.2
  project_name        : infinity_circuit_breaker
  tf32                : True
  auto_resume         : True
  rush_resume         : 
  nowd                : 1
  enable_hybrid_shard : False
  inner_shard_degree  : 1
  zero                : 2
  buck                : chunk
  fsdp_orig           : True
  enable_checkpointing: full-block
  pad_to_multiplier   : 128
  log_every_iter      : False
  checkpoint_type     : torch
  seed                : None
  rand                : True
  task_id             : 123
  trial_id            : 123
  robust_run_id       : 0
  ckpt_trials         : []
  real_trial_id       : 123
  chunk_nodes         : 0
  is_master_node      : None
  log_txt_path        : ./local_output/log.txt
  t5_path             : google/flan-t5-xl
  online_t5           : True
  sdpa_mem            : True
  tfast               : 0
  model_alias         : 8b
  rms                 : False
  aln                 : 0.001
  alng                : 5e-06
  saln                : True
  haln                : True
  nm0                 : False
  tau                 : 1
  cos                 : True
  swi                 : False
  dp                  : -1
  drop                : 0.0
  hd                  : 0
  ca_gamma            : -1
  diva                : 1
  hd0                 : 0.02
  dec                 : 1
  cum                 : 3
  rwe                 : False
  tp                  : 0.0
  tk                  : 0.0
  tini                : 0.010416666666666666
  cfg                 : 0.1
  rand_uncond         : False
  ema                 : 0.9999
  tema                : 0
  fp16                : 2
  fuse                : False
  fused_norm          : False
  flash               : False
  xen                 : False
  use_flex_attn       : True
  stable              : False
  gblr                : 0.0001
  dblr                : 0.0001
  tblr                : 0.006
  glr                 : 1.5625e-06
  dlr                 : 1.5625e-06
  tlr                 : 9.375e-05
  gwd                 : 0.005
  dwd                 : 0.0005
  twd                 : 0.005
  gwde                : 0.005
  dwde                : 0.0005
  twde                : 0.005
  ls                  : 0.0
  lz                  : 0.0
  eq                  : 0
  ep                  : 100
  wp                  : 1e-08
  wp0                 : 0.005
  wpe                 : 1.0
  sche                : lin0
  log_freq            : 50
  gclip               : 6.0
  dclip               : 6.0
  tclip               : 5.0
  cdec                : False
  opt                 : adamw
  ada                 : 0.9_0.97
  dada                : 0.9_0.97
  oeps                : 0
  afuse               : True
  pn                  : 0.06M
  scale_schedule      : None
  patch_size          : None
  resos               : None
  data_load_reso      : None
  workers             : 4
  lbs                 : 4
  bs                  : 4
  batch_size          : 4
  glb_batch_size      : 4
  ac                  : 1
  r_accu              : 1.0
  norm_eps            : 1e-06
  tlen                : 512
  Ct5                 : 2048
  use_bit_label       : 1
  bitloss_type        : mean
  dynamic_resolution_across_gpus: 1
  enable_dynamic_length_prompt: 1
  use_streaming_dataset: 1
  iterable_data_buffersize: 30000
  save_model_iters_freq: 100
  noise_apply_layers  : 13
  noise_apply_strength: 0.3
  noise_apply_requant : 1
  rope2d_each_sa_layer: 1
  rope2d_normalized_by_hw: 2
  use_fsdp_model_ema  : 0
  add_lvl_embeding_only_first_block: 1
  reweight_loss_by_scale: 1
  always_training_scales: 100
  vae_type            : 14
  fake_vae_input      : False
  model_init_device   : cuda
  prefetch_factor     : 2
  apply_spatial_patchify: 1
  debug_bsc           : 0
  task_type           : t2i
  target_layers       : [10, 12, 14, 16, 18, 20]
  transform_layers    : [10, 12, 14, 16, 18, 20]
  lorra_alpha         : 5.0
  trainsets           : ['AlpacaSupervisedDataset', 'HarmfulDataset']
  valsets             : ['AlpacaSupervisedDataset', 'HarmfulDataset']
  adv_string          : Sure here's
  full_layers         : False
  harmful_prompts_path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
  sanitized_prompts_path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
  validation_ratio    : 0.1
  category            : hate
  lora_r              : 8
  lora_alpha          : 16
  lora_dropout        : 0.05
  lora_target_modules : ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
  lora_weight_path    : 
  lora_bias           : none
  q_lora              : False
  model_name_or_path  : meta-llama/Llama-2-7b-chat-hf
  adapter_name_or_path: 
  use_lora            : True
  cache_dir           : 
  optim               : adamw_torch
  model_max_length    : 512
  grouped_to_max_length: False
  use_refusal_retain  : True
  sc_train_subset     : ['']
  log_every           : 10
  sc_train_seq_type   : all_text
  coeff_schedule      : linear_converge
  sc_loss_type        : orig_act_dotprod
  branch              : master
  commit_id           : 
  commit_msg          : 
  cmd                 : --bed ./checkpoints --local_out_path ./local_output --project_name infinity_circuit_breaker --exp_name experiment --ep 100 --opt adamw --cum 3 --sche lin0 --fp16 2 --ada 0.9_0.97 --tini -1 --tclip 5 --flash 0 --alng 5e-06 --saln 1 --cos 1 --enable_checkpointing full-block --tblr 6e-3 --pn 0.06M --lbs 4 --workers 4 --short_cap_prob 0.2 --online_t5 1 --use_streaming_dataset 1 --iterable_data_buffersize 30000 --Ct5 2048 --t5_path google/flan-t5-xl --vae_type 14 --wp 0.00000001 --wpe 1 --dynamic_resolution_across_gpus 1 --enable_dynamic_length_prompt 1 --reweight_loss_by_scale 1 --add_lvl_embeding_only_first_block 1 --rope2d_each_sa_layer 1 --rope2d_normalized_by_hw 2 --use_fsdp_model_ema 0 --always_training_scales 100 --use_bit_label 1 --zero 2 --save_model_iters_freq 100 --log_freq 50 --checkpoint_type torch --prefetch_factor 2 --noise_apply_strength 0.3 --noise_apply_layers 13 --apply_spatial_patchify 1 --use_flex_attn True --pad 128 --data_path ./data --data_load_reso 512 --tlen 512 --workers 4 --prefetch_factor 2 --harmful_prompts_path /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts --sanitized_prompts_path /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts --validation_ratio 0.1 --category hate --target_layers 10,12,14,16,18,20 --transform_layers 10,12,14,16,18,20 --lorra_alpha 5.0 --trainsets AlpacaSupervisedDataset#HarmfulDataset --valsets AlpacaSupervisedDataset#HarmfulDataset --adv_string Sure here's --full_layers False --lora_r 8 --lora_alpha 16 --lora_dropout 0.05 --lora_target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj --lora_weight_path  --lora_bias none --q_lora False --model_name_or_path meta-llama/Llama-2-7b-chat-hf --adapter_name_or_path  --use_lora True --cache_dir  --optim adamw_torch --model_max_length 512 --grouped_to_max_length False --use_refusal_retain True --sc_train_subset  --log_every 10 --sc_train_seq_type all_text --coeff_schedule linear_converge --sc_loss_type orig_act_dotprod
  tag                 : UK
  acc_all             : None
  acc_real            : None
  acc_fake            : None
  last_Lnll           : None
  last_L1             : None
  last_Ld             : None
  last_wei_g          : None
  grad_boom           : None
  diff                : None
  diffs               : 
  diffs_ema           : None
  ca_performance      : 
  cur_phase           : 
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  iter_speed          : None
  img_per_day         : None
  max_nvidia_smi      : 0
  max_memory_allocated: None
  max_memory_reserved : None
  num_alloc_retries   : None
  MFU                 : None
  HFU                 : None
  dbg_modified        : False
  dbg_ks              : False
  dbg_ks_last         : None
  dbg                 : False
  ks                  : False
  nodata              : False
  nodata_tlen         : 320
  nova                : False
  prof                : 0
  prof_freq           : 50
  tos_profiler_file_prefix: vgpt_default/
  profall             : 0
  v_seed              : 0
  g_seed              : 0
}

[09-07 00:08:02] (inity_circuit_breaker.py, line 182)=> start_it=0, iters_train=100
[09-07 00:08:02] (inity_circuit_breaker.py, line 466)=> [PT info]  from ep0 it0, acc_str: [no acc str], diffs: ,    =======>  bed: ./checkpoints  <=======

[09-07 00:08:04] (twise_self_correction.py, line  36)=> [dbg] raw_features: torch.Size([2, 14, 64, 64])
[09-07 00:08:04] (twise_self_correction.py, line  41)=> [dbg] target last scale: (1, 32, 32)
[09-07 00:08:04] (twise_self_correction.py, line  46)=> [dbg] Resizing codes_out from torch.Size([2, 14, 1, 64, 64]) to match target size (32, 32)







=======================================================   RESTART [09-07 00:11:03]   =======================================================
[09-07 00:11:03] (inity_circuit_breaker.py, line 718)=> ============================================================
[09-07 00:11:03] (inity_circuit_breaker.py, line 719)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[09-07 00:11:03] (inity_circuit_breaker.py, line 720)=> Circuit Breaker Alpha: 0.1
[09-07 00:11:03] (inity_circuit_breaker.py, line 721)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[09-07 00:11:03] (inity_circuit_breaker.py, line 722)=> Circuit Breaker Enabled: True
[09-07 00:11:03] (inity_circuit_breaker.py, line 723)=> Number of Examples: 1000
[09-07 00:11:03] (inity_circuit_breaker.py, line 724)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[09-07 00:11:03] (inity_circuit_breaker.py, line 725)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[09-07 00:11:03] (inity_circuit_breaker.py, line 726)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[09-07 00:11:03] (inity_circuit_breaker.py, line 727)=> Validation Ratio: 0.1
[09-07 00:11:03] (inity_circuit_breaker.py, line 728)=> Category: hate
[09-07 00:11:03] (inity_circuit_breaker.py, line 729)=> Batch Size: 4
[09-07 00:11:03] (inity_circuit_breaker.py, line 730)=> Workers: 4
[09-07 00:11:03] (inity_circuit_breaker.py, line 731)=> Device: cuda:0
[09-07 00:11:03] (inity_circuit_breaker.py, line 732)=> Model: infinity_8b
[09-07 00:11:03] (inity_circuit_breaker.py, line 733)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-07 00:11:03] (inity_circuit_breaker.py, line 734)=> Rush Resume: 
[09-07 00:11:03] (inity_circuit_breaker.py, line 735)=> Selective Layers: 0,1,2,3,4,5
[09-07 00:11:03] (inity_circuit_breaker.py, line 736)=> ============================================================
[09-07 00:11:03] (inity_circuit_breaker.py, line 377)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[09-07 00:11:03] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-07 00:11:03] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-07 00:11:03] (rain_dataset_infinity.py, line 205)=> Split 1000 prompts: 900 retain, 100 validation
[09-07 00:11:03] (rain_dataset_infinity.py, line 236)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-07 00:11:03] (rain_dataset_infinity.py, line 245)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-07 00:11:03] (rain_dataset_infinity.py, line  84)=> Loaded 900 retain examples
[09-07 00:11:03] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[09-07 00:11:03] (rain_dataset_infinity.py, line  86)=> Loaded 100 validation examples
[09-07 00:11:03] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[09-07 00:11:03] (inity_circuit_breaker.py, line 395)=> args.batch_size=4, vbs=6
[09-07 00:11:03] (inity_circuit_breaker.py, line 418)=> len(dataloader): 100, len(dataset): 100
[09-07 00:11:03] (inity_circuit_breaker.py, line 420)=> total_samples: 200
[09-07 00:11:03] (inity_circuit_breaker.py, line 421)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[09-07 00:11:03] (inity_circuit_breaker.py, line 140)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[09-07 00:11:03] (inity_circuit_breaker.py, line 142)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[09-07 00:11:03] (inity_circuit_breaker.py, line 146)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-07 00:11:06] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[09-07 00:11:06] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[09-07 00:11:06] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[09-07 00:11:06] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[09-07 00:11:08] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[09-07 00:11:10] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[09-07 00:11:10] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[09-07 00:11:10] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[09-07 00:11:10] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[09-07 00:11:10] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

[09-07 00:11:20] (ity/models/init_param.py, line  15)=> [init_weights] Infinity with std=0.02
[09-07 00:11:20] (inity_circuit_breaker.py, line 246)=> [PT] GPT model = Infinity(
  drop_path_rate=0.2
  (norm0_cond): Identity()
  (text_norm): FastRMSNorm(C=2048, eps=1e-06, elementwise_affine=True)
  (text_proj_for_sos): TextAttentivePool(
    (ca): CrossAttention(
      Cq=3072, Ckv=2048, cos_attn=False
      (mat_kv): Linear(in_features=2048, out_features=6144, bias=False)
      (proj): Linear(in_features=3072, out_features=3072, bias=True)
      (proj_drop): Identity()
    )
  )
  (text_proj_for_ca): Sequential(
    (0): Linear(in_features=2048, out_features=3072, bias=True)
    (1): GELU(approximate='tanh')
    (2): Linear(in_features=3072, out_features=3072, bias=True)
  )
  (lvl_embed): Embedding(15, 3072)
  (norm0_ve): Identity()
  (word_embed): Linear(in_features=56, out_features=3072, bias=True)
  (shared_ada_lin): Sequential(
    (0): SiLU()
    (1): SharedAdaLin(in_features=3072, out_features=18432, bias=True)
  )
  (head_nm): AdaLNBeforeHead(
    (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
    (ada_lin): Sequential(
      (0): SiLU()
      (1): Linear(in_features=3072, out_features=6144, bias=True)
    )
  )
  (head): Linear(in_features=3072, out_features=112, bias=True)
  (blocks): ModuleList(
    (0): CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): Identity()
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
    (1-55): 55 x CrossAttnBlock(
      shared_aln=True, fused_norm=False, ca_gamma=1
      (drop_path): DropPath(...)
      (sa): SelfAttention(
        using_flash=False, tau=1, cos_attn=True
        (mat_qkv): Linear(in_features=3072, out_features=9216, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ca): CrossAttention(
        Cq=3072, Ckv=3072, cos_attn=False
        (mat_q): Linear(in_features=3072, out_features=3072, bias=True)
        (mat_kv): Linear(in_features=3072, out_features=6144, bias=False)
        (proj): Linear(in_features=3072, out_features=3072, bias=True)
        (proj_drop): Identity()
      )
      (ffn): FFN(
        fused_mlp=False
        (fc1): Linear(in_features=3072, out_features=12288, bias=True)
        (act): GELU(approximate='tanh')
        (fc2): Linear(in_features=12288, out_features=3072, bias=True)
        (drop): Identity()
      )
      (ln_wo_grad): LayerNorm((3072,), eps=1e-06, elementwise_affine=False)
      (ca_norm): LayerNorm((3072,), eps=1e-06, elementwise_affine=True)
    )
  )
)


[09-07 00:11:20] (inity_circuit_breaker.py, line 248)=> [PT][#para] VAE=81.62, VAE.quant=0.00
[09-07 00:11:20] (inity_circuit_breaker.py, line 251)=> [PT][#para] GPT=8573.89


[09-07 00:11:21] (nity/utils/lr_control.py, line  71)=> [get_param_groups][lr decay] with_lr_scale=False, lr_scale=0.0
[09-07 00:11:21] (nity/utils/lr_control.py, line 111)=> [get_param_groups] param_groups = 
{ 'D': { 'lr_sc': '[no scale]',
         'params': "('module.text_norm.weight, module.text_proj_for_sos.ca.mat_kv.weight, module.text_proj_for_sos.ca.proj.weight, module.text_proj_for_ca.0.weight, module.text_proj_for_ca.2.weight, '\n"
                   " 'module.word_embed.weight, module.shared_ada_lin.1.weight, module.head_nm.ada_lin.1.weight, module.head.weight, module.blocks.0.sa.mat_qkv.weight, module.blocks.0.sa.proj.weight, '\n"
                   " 'module.blocks.0.ca.mat_q.weight, module.blocks.0.ca.mat_kv.weight, module.blocks.0.ca.proj.weight, module.blocks.0.ffn.fc1.weight, module.blocks.0.ffn.fc2.weight, module.blocks.0.ca_norm.weight, '\n"
                   " 'module.blocks.1.sa.mat_qkv.weight, module.blocks.1.sa.proj.weight, module.blocks.1.ca.mat_q.weight, module.blocks.1.ca.mat_kv.weight, module.blocks.1.ca.proj.weight, '\n"
                   " 'module.blocks.1.ffn.fc1.weight, module.blocks.1.ffn.fc2.weight, module.blocks.1.ca_norm.weight, module.blocks.2.sa.mat_qkv.weight, module.blocks.2.sa.proj.weight, module.blocks.2.ca.mat_q.weight, '\n"
                   " 'module.blocks.2.ca.mat_kv.weight, module.blocks.2.ca.proj.weight, module.blocks.2.ffn.fc1.weight, module.blocks.2.ffn.fc2.weight, module.blocks.2.ca_norm.weight, module.blocks.3.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.3.sa.proj.weight, module.blocks.3.ca.mat_q.weight, module.blocks.3.ca.mat_kv.weight, module.blocks.3.ca.proj.weight, module.blocks.3.ffn.fc1.weight, module.blocks.3.ffn.fc2.weight, '\n"
                   " 'module.blocks.3.ca_norm.weight, module.blocks.4.sa.mat_qkv.weight, module.blocks.4.sa.proj.weight, module.blocks.4.ca.mat_q.weight, module.blocks.4.ca.mat_kv.weight, '\n"
                   " 'module.blocks.4.ca.proj.weight, module.blocks.4.ffn.fc1.weight, module.blocks.4.ffn.fc2.weight, module.blocks.4.ca_norm.weight, module.blocks.5.sa.mat_qkv.weight, module.blocks.5.sa.proj.weight, '\n"
                   " 'module.blocks.5.ca.mat_q.weight, module.blocks.5.ca.mat_kv.weight, module.blocks.5.ca.proj.weight, module.blocks.5.ffn.fc1.weight, module.blocks.5.ffn.fc2.weight, module.blocks.5.ca_norm.weight, '\n"
                   " 'module.blocks.6.sa.mat_qkv.weight, module.blocks.6.sa.proj.weight, module.blocks.6.ca.mat_q.weight, module.blocks.6.ca.mat_kv.weight, module.blocks.6.ca.proj.weight, '\n"
                   " 'module.blocks.6.ffn.fc1.weight, module.blocks.6.ffn.fc2.weight, module.blocks.6.ca_norm.weight, module.blocks.7.sa.mat_qkv.weight, module.blocks.7.sa.proj.weight, module.blocks.7.ca.mat_q.weight, '\n"
                   " 'module.blocks.7.ca.mat_kv.weight, module.blocks.7.ca.proj.weight, module.blocks.7.ffn.fc1.weight, module.blocks.7.ffn.fc2.weight, module.blocks.7.ca_norm.weight, module.blocks.8.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.8.sa.proj.weight, module.blocks.8.ca.mat_q.weight, module.blocks.8.ca.mat_kv.weight, module.blocks.8.ca.proj.weight, module.blocks.8.ffn.fc1.weight, module.blocks.8.ffn.fc2.weight, '\n"
                   " 'module.blocks.8.ca_norm.weight, module.blocks.9.sa.mat_qkv.weight, module.blocks.9.sa.proj.weight, module.blocks.9.ca.mat_q.weight, module.blocks.9.ca.mat_kv.weight, '\n"
                   " 'module.blocks.9.ca.proj.weight, module.blocks.9.ffn.fc1.weight, module.blocks.9.ffn.fc2.weight, module.blocks.9.ca_norm.weight, module.blocks.10.sa.mat_qkv.weight, module.blocks.10.sa.proj.weight, '\n"
                   " 'module.blocks.10.ca.mat_q.weight, module.blocks.10.ca.mat_kv.weight, module.blocks.10.ca.proj.weight, module.blocks.10.ffn.fc1.weight, module.blocks.10.ffn.fc2.weight, '\n"
                   " 'module.blocks.10.ca_norm.weight, module.blocks.11.sa.mat_qkv.weight, module.blocks.11.sa.proj.weight, module.blocks.11.ca.mat_q.weight, module.blocks.11.ca.mat_kv.weight, '\n"
                   " 'module.blocks.11.ca.proj.weight, module.blocks.11.ffn.fc1.weight, module.blocks.11.ffn.fc2.weight, module.blocks.11.ca_norm.weight, module.blocks.12.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.12.sa.proj.weight, module.blocks.12.ca.mat_q.weight, module.blocks.12.ca.mat_kv.weight, module.blocks.12.ca.proj.weight, module.blocks.12.ffn.fc1.weight, '\n"
                   " 'module.blocks.12.ffn.fc2.weight, module.blocks.12.ca_norm.weight, module.blocks.13.sa.mat_qkv.weight, module.blocks.13.sa.proj.weight, module.blocks.13.ca.mat_q.weight, '\n"
                   " 'module.blocks.13.ca.mat_kv.weight, module.blocks.13.ca.proj.weight, module.blocks.13.ffn.fc1.weight, module.blocks.13.ffn.fc2.weight, module.blocks.13.ca_norm.weight, '\n"
                   " 'module.blocks.14.sa.mat_qkv.weight, module.blocks.14.sa.proj.weight, module.blocks.14.ca.mat_q.weight, module.blocks.14.ca.mat_kv.weight, module.blocks.14.ca.proj.weight, '\n"
                   " 'module.blocks.14.ffn.fc1.weight, module.blocks.14.ffn.fc2.weight, module.blocks.14.ca_norm.weight, module.blocks.15.sa.mat_qkv.weight, module.blocks.15.sa.proj.weight, '\n"
                   " 'module.blocks.15.ca.mat_q.weight, module.blocks.15.ca.mat_kv.weight, module.blocks.15.ca.proj.weight, module.blocks.15.ffn.fc1.weight, module.blocks.15.ffn.fc2.weight, '\n"
                   " 'module.blocks.15.ca_norm.weight, module.blocks.16.sa.mat_qkv.weight, module.blocks.16.sa.proj.weight, module.blocks.16.ca.mat_q.weight, module.blocks.16.ca.mat_kv.weight, '\n"
                   " 'module.blocks.16.ca.proj.weight, module.blocks.16.ffn.fc1.weight, module.blocks.16.ffn.fc2.weight, module.blocks.16.ca_norm.weight, module.blocks.17.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.17.sa.proj.weight, module.blocks.17.ca.mat_q.weight, module.blocks.17.ca.mat_kv.weight, module.blocks.17.ca.proj.weight, module.blocks.17.ffn.fc1.weight, '\n"
                   " 'module.blocks.17.ffn.fc2.weight, module.blocks.17.ca_norm.weight, module.blocks.18.sa.mat_qkv.weight, module.blocks.18.sa.proj.weight, module.blocks.18.ca.mat_q.weight, '\n"
                   " 'module.blocks.18.ca.mat_kv.weight, module.blocks.18.ca.proj.weight, module.blocks.18.ffn.fc1.weight, module.blocks.18.ffn.fc2.weight, module.blocks.18.ca_norm.weight, '\n"
                   " 'module.blocks.19.sa.mat_qkv.weight, module.blocks.19.sa.proj.weight, module.blocks.19.ca.mat_q.weight, module.blocks.19.ca.mat_kv.weight, module.blocks.19.ca.proj.weight, '\n"
                   " 'module.blocks.19.ffn.fc1.weight, module.blocks.19.ffn.fc2.weight, module.blocks.19.ca_norm.weight, module.blocks.20.sa.mat_qkv.weight, module.blocks.20.sa.proj.weight, '\n"
                   " 'module.blocks.20.ca.mat_q.weight, module.blocks.20.ca.mat_kv.weight, module.blocks.20.ca.proj.weight, module.blocks.20.ffn.fc1.weight, module.blocks.20.ffn.fc2.weight, '\n"
                   " 'module.blocks.20.ca_norm.weight, module.blocks.21.sa.mat_qkv.weight, module.blocks.21.sa.proj.weight, module.blocks.21.ca.mat_q.weight, module.blocks.21.ca.mat_kv.weight, '\n"
                   " 'module.blocks.21.ca.proj.weight, module.blocks.21.ffn.fc1.weight, module.blocks.21.ffn.fc2.weight, module.blocks.21.ca_norm.weight, module.blocks.22.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.22.sa.proj.weight, module.blocks.22.ca.mat_q.weight, module.blocks.22.ca.mat_kv.weight, module.blocks.22.ca.proj.weight, module.blocks.22.ffn.fc1.weight, '\n"
                   " 'module.blocks.22.ffn.fc2.weight, module.blocks.22.ca_norm.weight, module.blocks.23.sa.mat_qkv.weight, module.blocks.23.sa.proj.weight, module.blocks.23.ca.mat_q.weight, '\n"
                   " 'module.blocks.23.ca.mat_kv.weight, module.blocks.23.ca.proj.weight, module.blocks.23.ffn.fc1.weight, module.blocks.23.ffn.fc2.weight, module.blocks.23.ca_norm.weight, '\n"
                   " 'module.blocks.24.sa.mat_qkv.weight, module.blocks.24.sa.proj.weight, module.blocks.24.ca.mat_q.weight, module.blocks.24.ca.mat_kv.weight, module.blocks.24.ca.proj.weight, '\n"
                   " 'module.blocks.24.ffn.fc1.weight, module.blocks.24.ffn.fc2.weight, module.blocks.24.ca_norm.weight, module.blocks.25.sa.mat_qkv.weight, module.blocks.25.sa.proj.weight, '\n"
                   " 'module.blocks.25.ca.mat_q.weight, module.blocks.25.ca.mat_kv.weight, module.blocks.25.ca.proj.weight, module.blocks.25.ffn.fc1.weight, module.blocks.25.ffn.fc2.weight, '\n"
                   " 'module.blocks.25.ca_norm.weight, module.blocks.26.sa.mat_qkv.weight, module.blocks.26.sa.proj.weight, module.blocks.26.ca.mat_q.weight, module.blocks.26.ca.mat_kv.weight, '\n"
                   " 'module.blocks.26.ca.proj.weight, module.blocks.26.ffn.fc1.weight, module.blocks.26.ffn.fc2.weight, module.blocks.26.ca_norm.weight, module.blocks.27.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.27.sa.proj.weight, module.blocks.27.ca.mat_q.weight, module.blocks.27.ca.mat_kv.weight, module.blocks.27.ca.proj.weight, module.blocks.27.ffn.fc1.weight, '\n"
                   " 'module.blocks.27.ffn.fc2.weight, module.blocks.27.ca_norm.weight, module.blocks.28.sa.mat_qkv.weight, module.blocks.28.sa.proj.weight, module.blocks.28.ca.mat_q.weight, '\n"
                   " 'module.blocks.28.ca.mat_kv.weight, module.blocks.28.ca.proj.weight, module.blocks.28.ffn.fc1.weight, module.blocks.28.ffn.fc2.weight, module.blocks.28.ca_norm.weight, '\n"
                   " 'module.blocks.29.sa.mat_qkv.weight, module.blocks.29.sa.proj.weight, module.blocks.29.ca.mat_q.weight, module.blocks.29.ca.mat_kv.weight, module.blocks.29.ca.proj.weight, '\n"
                   " 'module.blocks.29.ffn.fc1.weight, module.blocks.29.ffn.fc2.weight, module.blocks.29.ca_norm.weight, module.blocks.30.sa.mat_qkv.weight, module.blocks.30.sa.proj.weight, '\n"
                   " 'module.blocks.30.ca.mat_q.weight, module.blocks.30.ca.mat_kv.weight, module.blocks.30.ca.proj.weight, module.blocks.30.ffn.fc1.weight, module.blocks.30.ffn.fc2.weight, '\n"
                   " 'module.blocks.30.ca_norm.weight, module.blocks.31.sa.mat_qkv.weight, module.blocks.31.sa.proj.weight, module.blocks.31.ca.mat_q.weight, module.blocks.31.ca.mat_kv.weight, '\n"
                   " 'module.blocks.31.ca.proj.weight, module.blocks.31.ffn.fc1.weight, module.blocks.31.ffn.fc2.weight, module.blocks.31.ca_norm.weight, module.blocks.32.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.32.sa.proj.weight, module.blocks.32.ca.mat_q.weight, module.blocks.32.ca.mat_kv.weight, module.blocks.32.ca.proj.weight, module.blocks.32.ffn.fc1.weight, '\n"
                   " 'module.blocks.32.ffn.fc2.weight, module.blocks.32.ca_norm.weight, module.blocks.33.sa.mat_qkv.weight, module.blocks.33.sa.proj.weight, module.blocks.33.ca.mat_q.weight, '\n"
                   " 'module.blocks.33.ca.mat_kv.weight, module.blocks.33.ca.proj.weight, module.blocks.33.ffn.fc1.weight, module.blocks.33.ffn.fc2.weight, module.blocks.33.ca_norm.weight, '\n"
                   " 'module.blocks.34.sa.mat_qkv.weight, module.blocks.34.sa.proj.weight, module.blocks.34.ca.mat_q.weight, module.blocks.34.ca.mat_kv.weight, module.blocks.34.ca.proj.weight, '\n"
                   " 'module.blocks.34.ffn.fc1.weight, module.blocks.34.ffn.fc2.weight, module.blocks.34.ca_norm.weight, module.blocks.35.sa.mat_qkv.weight, module.blocks.35.sa.proj.weight, '\n"
                   " 'module.blocks.35.ca.mat_q.weight, module.blocks.35.ca.mat_kv.weight, module.blocks.35.ca.proj.weight, module.blocks.35.ffn.fc1.weight, module.blocks.35.ffn.fc2.weight, '\n"
                   " 'module.blocks.35.ca_norm.weight, module.blocks.36.sa.mat_qkv.weight, module.blocks.36.sa.proj.weight, module.blocks.36.ca.mat_q.weight, module.blocks.36.ca.mat_kv.weight, '\n"
                   " 'module.blocks.36.ca.proj.weight, module.blocks.36.ffn.fc1.weight, module.blocks.36.ffn.fc2.weight, module.blocks.36.ca_norm.weight, module.blocks.37.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.37.sa.proj.weight, module.blocks.37.ca.mat_q.weight, module.blocks.37.ca.mat_kv.weight, module.blocks.37.ca.proj.weight, module.blocks.37.ffn.fc1.weight, '\n"
                   " 'module.blocks.37.ffn.fc2.weight, module.blocks.37.ca_norm.weight, module.blocks.38.sa.mat_qkv.weight, module.blocks.38.sa.proj.weight, module.blocks.38.ca.mat_q.weight, '\n"
                   " 'module.blocks.38.ca.mat_kv.weight, module.blocks.38.ca.proj.weight, module.blocks.38.ffn.fc1.weight, module.blocks.38.ffn.fc2.weight, module.blocks.38.ca_norm.weight, '\n"
                   " 'module.blocks.39.sa.mat_qkv.weight, module.blocks.39.sa.proj.weight, module.blocks.39.ca.mat_q.weight, module.blocks.39.ca.mat_kv.weight, module.blocks.39.ca.proj.weight, '\n"
                   " 'module.blocks.39.ffn.fc1.weight, module.blocks.39.ffn.fc2.weight, module.blocks.39.ca_norm.weight, module.blocks.40.sa.mat_qkv.weight, module.blocks.40.sa.proj.weight, '\n"
                   " 'module.blocks.40.ca.mat_q.weight, module.blocks.40.ca.mat_kv.weight, module.blocks.40.ca.proj.weight, module.blocks.40.ffn.fc1.weight, module.blocks.40.ffn.fc2.weight, '\n"
                   " 'module.blocks.40.ca_norm.weight, module.blocks.41.sa.mat_qkv.weight, module.blocks.41.sa.proj.weight, module.blocks.41.ca.mat_q.weight, module.blocks.41.ca.mat_kv.weight, '\n"
                   " 'module.blocks.41.ca.proj.weight, module.blocks.41.ffn.fc1.weight, module.blocks.41.ffn.fc2.weight, module.blocks.41.ca_norm.weight, module.blocks.42.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.42.sa.proj.weight, module.blocks.42.ca.mat_q.weight, module.blocks.42.ca.mat_kv.weight, module.blocks.42.ca.proj.weight, module.blocks.42.ffn.fc1.weight, '\n"
                   " 'module.blocks.42.ffn.fc2.weight, module.blocks.42.ca_norm.weight, module.blocks.43.sa.mat_qkv.weight, module.blocks.43.sa.proj.weight, module.blocks.43.ca.mat_q.weight, '\n"
                   " 'module.blocks.43.ca.mat_kv.weight, module.blocks.43.ca.proj.weight, module.blocks.43.ffn.fc1.weight, module.blocks.43.ffn.fc2.weight, module.blocks.43.ca_norm.weight, '\n"
                   " 'module.blocks.44.sa.mat_qkv.weight, module.blocks.44.sa.proj.weight, module.blocks.44.ca.mat_q.weight, module.blocks.44.ca.mat_kv.weight, module.blocks.44.ca.proj.weight, '\n"
                   " 'module.blocks.44.ffn.fc1.weight, module.blocks.44.ffn.fc2.weight, module.blocks.44.ca_norm.weight, module.blocks.45.sa.mat_qkv.weight, module.blocks.45.sa.proj.weight, '\n"
                   " 'module.blocks.45.ca.mat_q.weight, module.blocks.45.ca.mat_kv.weight, module.blocks.45.ca.proj.weight, module.blocks.45.ffn.fc1.weight, module.blocks.45.ffn.fc2.weight, '\n"
                   " 'module.blocks.45.ca_norm.weight, module.blocks.46.sa.mat_qkv.weight, module.blocks.46.sa.proj.weight, module.blocks.46.ca.mat_q.weight, module.blocks.46.ca.mat_kv.weight, '\n"
                   " 'module.blocks.46.ca.proj.weight, module.blocks.46.ffn.fc1.weight, module.blocks.46.ffn.fc2.weight, module.blocks.46.ca_norm.weight, module.blocks.47.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.47.sa.proj.weight, module.blocks.47.ca.mat_q.weight, module.blocks.47.ca.mat_kv.weight, module.blocks.47.ca.proj.weight, module.blocks.47.ffn.fc1.weight, '\n"
                   " 'module.blocks.47.ffn.fc2.weight, module.blocks.47.ca_norm.weight, module.blocks.48.sa.mat_qkv.weight, module.blocks.48.sa.proj.weight, module.blocks.48.ca.mat_q.weight, '\n"
                   " 'module.blocks.48.ca.mat_kv.weight, module.blocks.48.ca.proj.weight, module.blocks.48.ffn.fc1.weight, module.blocks.48.ffn.fc2.weight, module.blocks.48.ca_norm.weight, '\n"
                   " 'module.blocks.49.sa.mat_qkv.weight, module.blocks.49.sa.proj.weight, module.blocks.49.ca.mat_q.weight, module.blocks.49.ca.mat_kv.weight, module.blocks.49.ca.proj.weight, '\n"
                   " 'module.blocks.49.ffn.fc1.weight, module.blocks.49.ffn.fc2.weight, module.blocks.49.ca_norm.weight, module.blocks.50.sa.mat_qkv.weight, module.blocks.50.sa.proj.weight, '\n"
                   " 'module.blocks.50.ca.mat_q.weight, module.blocks.50.ca.mat_kv.weight, module.blocks.50.ca.proj.weight, module.blocks.50.ffn.fc1.weight, module.blocks.50.ffn.fc2.weight, '\n"
                   " 'module.blocks.50.ca_norm.weight, module.blocks.51.sa.mat_qkv.weight, module.blocks.51.sa.proj.weight, module.blocks.51.ca.mat_q.weight, module.blocks.51.ca.mat_kv.weight, '\n"
                   " 'module.blocks.51.ca.proj.weight, module.blocks.51.ffn.fc1.weight, module.blocks.51.ffn.fc2.weight, module.blocks.51.ca_norm.weight, module.blocks.52.sa.mat_qkv.weight, '\n"
                   " 'module.blocks.52.sa.proj.weight, module.blocks.52.ca.mat_q.weight, module.blocks.52.ca.mat_kv.weight, module.blocks.52.ca.proj.weight, module.blocks.52.ffn.fc1.weight, '\n"
                   " 'module.blocks.52.ffn.fc2.weight, module.blocks.52.ca_norm.weight, module.blocks.53.sa.mat_qkv.weight, module.blocks.53.sa.proj.weight, module.blocks.53.ca.mat_q.weight, '\n"
                   " 'module.blocks.53.ca.mat_kv.weight, module.blocks.53.ca.proj.weight, module.blocks.53.ffn.fc1.weight, module.blocks.53.ffn.fc2.weight, module.blocks.53.ca_norm.weight, '\n"
                   " 'module.blocks.54.sa.mat_qkv.weight, module.blocks.54.sa.proj.weight, module.blocks.54.ca.mat_q.weight, module.blocks.54.ca.mat_kv.weight, module.blocks.54.ca.proj.weight, '\n"
                   " 'module.blocks.54.ffn.fc1.weight, module.blocks.54.ffn.fc2.weight, module.blocks.54.ca_norm.weight, module.blocks.55.sa.mat_qkv.weight, module.blocks.55.sa.proj.weight, '\n"
                   " 'module.blocks.55.ca.mat_q.weight, module.blocks.55.ca.mat_kv.weight, module.blocks.55.ca.proj.weight, module.blocks.55.ffn.fc1.weight, module.blocks.55.ffn.fc2.weight, '\n"
                   " 'module.blocks.55.ca_norm.weight')",
         'wd_sc': 1.0},
  'ND': { 'lr_sc': '[no scale]',
          'params': "('module.cfg_uncond, module.pos_start, module.text_proj_for_sos.ca.mat_q, module.text_proj_for_sos.ca.v_bias, module.text_proj_for_sos.ca.proj.bias, module.text_proj_for_ca.0.bias, '\n"
                    " 'module.text_proj_for_ca.2.bias, module.lvl_embed.weight, module.word_embed.bias, module.shared_ada_lin.1.bias, module.head_nm.ada_lin.1.bias, module.head.bias, module.blocks.0.ada_gss, '\n"
                    " 'module.blocks.0.sa.scale_mul_1H11, module.blocks.0.sa.q_bias, module.blocks.0.sa.v_bias, module.blocks.0.sa.proj.bias, module.blocks.0.ca.v_bias, module.blocks.0.ca.mat_q.bias, '\n"
                    " 'module.blocks.0.ca.proj.bias, module.blocks.0.ffn.fc1.bias, module.blocks.0.ffn.fc2.bias, module.blocks.0.ca_norm.bias, module.blocks.1.ada_gss, module.blocks.1.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.1.sa.q_bias, module.blocks.1.sa.v_bias, module.blocks.1.sa.proj.bias, module.blocks.1.ca.v_bias, module.blocks.1.ca.mat_q.bias, module.blocks.1.ca.proj.bias, '\n"
                    " 'module.blocks.1.ffn.fc1.bias, module.blocks.1.ffn.fc2.bias, module.blocks.1.ca_norm.bias, module.blocks.2.ada_gss, module.blocks.2.sa.scale_mul_1H11, module.blocks.2.sa.q_bias, '\n"
                    " 'module.blocks.2.sa.v_bias, module.blocks.2.sa.proj.bias, module.blocks.2.ca.v_bias, module.blocks.2.ca.mat_q.bias, module.blocks.2.ca.proj.bias, module.blocks.2.ffn.fc1.bias, '\n"
                    " 'module.blocks.2.ffn.fc2.bias, module.blocks.2.ca_norm.bias, module.blocks.3.ada_gss, module.blocks.3.sa.scale_mul_1H11, module.blocks.3.sa.q_bias, module.blocks.3.sa.v_bias, '\n"
                    " 'module.blocks.3.sa.proj.bias, module.blocks.3.ca.v_bias, module.blocks.3.ca.mat_q.bias, module.blocks.3.ca.proj.bias, module.blocks.3.ffn.fc1.bias, module.blocks.3.ffn.fc2.bias, '\n"
                    " 'module.blocks.3.ca_norm.bias, module.blocks.4.ada_gss, module.blocks.4.sa.scale_mul_1H11, module.blocks.4.sa.q_bias, module.blocks.4.sa.v_bias, module.blocks.4.sa.proj.bias, '\n"
                    " 'module.blocks.4.ca.v_bias, module.blocks.4.ca.mat_q.bias, module.blocks.4.ca.proj.bias, module.blocks.4.ffn.fc1.bias, module.blocks.4.ffn.fc2.bias, module.blocks.4.ca_norm.bias, '\n"
                    " 'module.blocks.5.ada_gss, module.blocks.5.sa.scale_mul_1H11, module.blocks.5.sa.q_bias, module.blocks.5.sa.v_bias, module.blocks.5.sa.proj.bias, module.blocks.5.ca.v_bias, '\n"
                    " 'module.blocks.5.ca.mat_q.bias, module.blocks.5.ca.proj.bias, module.blocks.5.ffn.fc1.bias, module.blocks.5.ffn.fc2.bias, module.blocks.5.ca_norm.bias, module.blocks.6.ada_gss, '\n"
                    " 'module.blocks.6.sa.scale_mul_1H11, module.blocks.6.sa.q_bias, module.blocks.6.sa.v_bias, module.blocks.6.sa.proj.bias, module.blocks.6.ca.v_bias, module.blocks.6.ca.mat_q.bias, '\n"
                    " 'module.blocks.6.ca.proj.bias, module.blocks.6.ffn.fc1.bias, module.blocks.6.ffn.fc2.bias, module.blocks.6.ca_norm.bias, module.blocks.7.ada_gss, module.blocks.7.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.7.sa.q_bias, module.blocks.7.sa.v_bias, module.blocks.7.sa.proj.bias, module.blocks.7.ca.v_bias, module.blocks.7.ca.mat_q.bias, module.blocks.7.ca.proj.bias, '\n"
                    " 'module.blocks.7.ffn.fc1.bias, module.blocks.7.ffn.fc2.bias, module.blocks.7.ca_norm.bias, module.blocks.8.ada_gss, module.blocks.8.sa.scale_mul_1H11, module.blocks.8.sa.q_bias, '\n"
                    " 'module.blocks.8.sa.v_bias, module.blocks.8.sa.proj.bias, module.blocks.8.ca.v_bias, module.blocks.8.ca.mat_q.bias, module.blocks.8.ca.proj.bias, module.blocks.8.ffn.fc1.bias, '\n"
                    " 'module.blocks.8.ffn.fc2.bias, module.blocks.8.ca_norm.bias, module.blocks.9.ada_gss, module.blocks.9.sa.scale_mul_1H11, module.blocks.9.sa.q_bias, module.blocks.9.sa.v_bias, '\n"
                    " 'module.blocks.9.sa.proj.bias, module.blocks.9.ca.v_bias, module.blocks.9.ca.mat_q.bias, module.blocks.9.ca.proj.bias, module.blocks.9.ffn.fc1.bias, module.blocks.9.ffn.fc2.bias, '\n"
                    " 'module.blocks.9.ca_norm.bias, module.blocks.10.ada_gss, module.blocks.10.sa.scale_mul_1H11, module.blocks.10.sa.q_bias, module.blocks.10.sa.v_bias, module.blocks.10.sa.proj.bias, '\n"
                    " 'module.blocks.10.ca.v_bias, module.blocks.10.ca.mat_q.bias, module.blocks.10.ca.proj.bias, module.blocks.10.ffn.fc1.bias, module.blocks.10.ffn.fc2.bias, module.blocks.10.ca_norm.bias, '\n"
                    " 'module.blocks.11.ada_gss, module.blocks.11.sa.scale_mul_1H11, module.blocks.11.sa.q_bias, module.blocks.11.sa.v_bias, module.blocks.11.sa.proj.bias, module.blocks.11.ca.v_bias, '\n"
                    " 'module.blocks.11.ca.mat_q.bias, module.blocks.11.ca.proj.bias, module.blocks.11.ffn.fc1.bias, module.blocks.11.ffn.fc2.bias, module.blocks.11.ca_norm.bias, module.blocks.12.ada_gss, '\n"
                    " 'module.blocks.12.sa.scale_mul_1H11, module.blocks.12.sa.q_bias, module.blocks.12.sa.v_bias, module.blocks.12.sa.proj.bias, module.blocks.12.ca.v_bias, module.blocks.12.ca.mat_q.bias, '\n"
                    " 'module.blocks.12.ca.proj.bias, module.blocks.12.ffn.fc1.bias, module.blocks.12.ffn.fc2.bias, module.blocks.12.ca_norm.bias, module.blocks.13.ada_gss, module.blocks.13.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.13.sa.q_bias, module.blocks.13.sa.v_bias, module.blocks.13.sa.proj.bias, module.blocks.13.ca.v_bias, module.blocks.13.ca.mat_q.bias, module.blocks.13.ca.proj.bias, '\n"
                    " 'module.blocks.13.ffn.fc1.bias, module.blocks.13.ffn.fc2.bias, module.blocks.13.ca_norm.bias, module.blocks.14.ada_gss, module.blocks.14.sa.scale_mul_1H11, module.blocks.14.sa.q_bias, '\n"
                    " 'module.blocks.14.sa.v_bias, module.blocks.14.sa.proj.bias, module.blocks.14.ca.v_bias, module.blocks.14.ca.mat_q.bias, module.blocks.14.ca.proj.bias, module.blocks.14.ffn.fc1.bias, '\n"
                    " 'module.blocks.14.ffn.fc2.bias, module.blocks.14.ca_norm.bias, module.blocks.15.ada_gss, module.blocks.15.sa.scale_mul_1H11, module.blocks.15.sa.q_bias, module.blocks.15.sa.v_bias, '\n"
                    " 'module.blocks.15.sa.proj.bias, module.blocks.15.ca.v_bias, module.blocks.15.ca.mat_q.bias, module.blocks.15.ca.proj.bias, module.blocks.15.ffn.fc1.bias, module.blocks.15.ffn.fc2.bias, '\n"
                    " 'module.blocks.15.ca_norm.bias, module.blocks.16.ada_gss, module.blocks.16.sa.scale_mul_1H11, module.blocks.16.sa.q_bias, module.blocks.16.sa.v_bias, module.blocks.16.sa.proj.bias, '\n"
                    " 'module.blocks.16.ca.v_bias, module.blocks.16.ca.mat_q.bias, module.blocks.16.ca.proj.bias, module.blocks.16.ffn.fc1.bias, module.blocks.16.ffn.fc2.bias, module.blocks.16.ca_norm.bias, '\n"
                    " 'module.blocks.17.ada_gss, module.blocks.17.sa.scale_mul_1H11, module.blocks.17.sa.q_bias, module.blocks.17.sa.v_bias, module.blocks.17.sa.proj.bias, module.blocks.17.ca.v_bias, '\n"
                    " 'module.blocks.17.ca.mat_q.bias, module.blocks.17.ca.proj.bias, module.blocks.17.ffn.fc1.bias, module.blocks.17.ffn.fc2.bias, module.blocks.17.ca_norm.bias, module.blocks.18.ada_gss, '\n"
                    " 'module.blocks.18.sa.scale_mul_1H11, module.blocks.18.sa.q_bias, module.blocks.18.sa.v_bias, module.blocks.18.sa.proj.bias, module.blocks.18.ca.v_bias, module.blocks.18.ca.mat_q.bias, '\n"
                    " 'module.blocks.18.ca.proj.bias, module.blocks.18.ffn.fc1.bias, module.blocks.18.ffn.fc2.bias, module.blocks.18.ca_norm.bias, module.blocks.19.ada_gss, module.blocks.19.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.19.sa.q_bias, module.blocks.19.sa.v_bias, module.blocks.19.sa.proj.bias, module.blocks.19.ca.v_bias, module.blocks.19.ca.mat_q.bias, module.blocks.19.ca.proj.bias, '\n"
                    " 'module.blocks.19.ffn.fc1.bias, module.blocks.19.ffn.fc2.bias, module.blocks.19.ca_norm.bias, module.blocks.20.ada_gss, module.blocks.20.sa.scale_mul_1H11, module.blocks.20.sa.q_bias, '\n"
                    " 'module.blocks.20.sa.v_bias, module.blocks.20.sa.proj.bias, module.blocks.20.ca.v_bias, module.blocks.20.ca.mat_q.bias, module.blocks.20.ca.proj.bias, module.blocks.20.ffn.fc1.bias, '\n"
                    " 'module.blocks.20.ffn.fc2.bias, module.blocks.20.ca_norm.bias, module.blocks.21.ada_gss, module.blocks.21.sa.scale_mul_1H11, module.blocks.21.sa.q_bias, module.blocks.21.sa.v_bias, '\n"
                    " 'module.blocks.21.sa.proj.bias, module.blocks.21.ca.v_bias, module.blocks.21.ca.mat_q.bias, module.blocks.21.ca.proj.bias, module.blocks.21.ffn.fc1.bias, module.blocks.21.ffn.fc2.bias, '\n"
                    " 'module.blocks.21.ca_norm.bias, module.blocks.22.ada_gss, module.blocks.22.sa.scale_mul_1H11, module.blocks.22.sa.q_bias, module.blocks.22.sa.v_bias, module.blocks.22.sa.proj.bias, '\n"
                    " 'module.blocks.22.ca.v_bias, module.blocks.22.ca.mat_q.bias, module.blocks.22.ca.proj.bias, module.blocks.22.ffn.fc1.bias, module.blocks.22.ffn.fc2.bias, module.blocks.22.ca_norm.bias, '\n"
                    " 'module.blocks.23.ada_gss, module.blocks.23.sa.scale_mul_1H11, module.blocks.23.sa.q_bias, module.blocks.23.sa.v_bias, module.blocks.23.sa.proj.bias, module.blocks.23.ca.v_bias, '\n"
                    " 'module.blocks.23.ca.mat_q.bias, module.blocks.23.ca.proj.bias, module.blocks.23.ffn.fc1.bias, module.blocks.23.ffn.fc2.bias, module.blocks.23.ca_norm.bias, module.blocks.24.ada_gss, '\n"
                    " 'module.blocks.24.sa.scale_mul_1H11, module.blocks.24.sa.q_bias, module.blocks.24.sa.v_bias, module.blocks.24.sa.proj.bias, module.blocks.24.ca.v_bias, module.blocks.24.ca.mat_q.bias, '\n"
                    " 'module.blocks.24.ca.proj.bias, module.blocks.24.ffn.fc1.bias, module.blocks.24.ffn.fc2.bias, module.blocks.24.ca_norm.bias, module.blocks.25.ada_gss, module.blocks.25.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.25.sa.q_bias, module.blocks.25.sa.v_bias, module.blocks.25.sa.proj.bias, module.blocks.25.ca.v_bias, module.blocks.25.ca.mat_q.bias, module.blocks.25.ca.proj.bias, '\n"
                    " 'module.blocks.25.ffn.fc1.bias, module.blocks.25.ffn.fc2.bias, module.blocks.25.ca_norm.bias, module.blocks.26.ada_gss, module.blocks.26.sa.scale_mul_1H11, module.blocks.26.sa.q_bias, '\n"
                    " 'module.blocks.26.sa.v_bias, module.blocks.26.sa.proj.bias, module.blocks.26.ca.v_bias, module.blocks.26.ca.mat_q.bias, module.blocks.26.ca.proj.bias, module.blocks.26.ffn.fc1.bias, '\n"
                    " 'module.blocks.26.ffn.fc2.bias, module.blocks.26.ca_norm.bias, module.blocks.27.ada_gss, module.blocks.27.sa.scale_mul_1H11, module.blocks.27.sa.q_bias, module.blocks.27.sa.v_bias, '\n"
                    " 'module.blocks.27.sa.proj.bias, module.blocks.27.ca.v_bias, module.blocks.27.ca.mat_q.bias, module.blocks.27.ca.proj.bias, module.blocks.27.ffn.fc1.bias, module.blocks.27.ffn.fc2.bias, '\n"
                    " 'module.blocks.27.ca_norm.bias, module.blocks.28.ada_gss, module.blocks.28.sa.scale_mul_1H11, module.blocks.28.sa.q_bias, module.blocks.28.sa.v_bias, module.blocks.28.sa.proj.bias, '\n"
                    " 'module.blocks.28.ca.v_bias, module.blocks.28.ca.mat_q.bias, module.blocks.28.ca.proj.bias, module.blocks.28.ffn.fc1.bias, module.blocks.28.ffn.fc2.bias, module.blocks.28.ca_norm.bias, '\n"
                    " 'module.blocks.29.ada_gss, module.blocks.29.sa.scale_mul_1H11, module.blocks.29.sa.q_bias, module.blocks.29.sa.v_bias, module.blocks.29.sa.proj.bias, module.blocks.29.ca.v_bias, '\n"
                    " 'module.blocks.29.ca.mat_q.bias, module.blocks.29.ca.proj.bias, module.blocks.29.ffn.fc1.bias, module.blocks.29.ffn.fc2.bias, module.blocks.29.ca_norm.bias, module.blocks.30.ada_gss, '\n"
                    " 'module.blocks.30.sa.scale_mul_1H11, module.blocks.30.sa.q_bias, module.blocks.30.sa.v_bias, module.blocks.30.sa.proj.bias, module.blocks.30.ca.v_bias, module.blocks.30.ca.mat_q.bias, '\n"
                    " 'module.blocks.30.ca.proj.bias, module.blocks.30.ffn.fc1.bias, module.blocks.30.ffn.fc2.bias, module.blocks.30.ca_norm.bias, module.blocks.31.ada_gss, module.blocks.31.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.31.sa.q_bias, module.blocks.31.sa.v_bias, module.blocks.31.sa.proj.bias, module.blocks.31.ca.v_bias, module.blocks.31.ca.mat_q.bias, module.blocks.31.ca.proj.bias, '\n"
                    " 'module.blocks.31.ffn.fc1.bias, module.blocks.31.ffn.fc2.bias, module.blocks.31.ca_norm.bias, module.blocks.32.ada_gss, module.blocks.32.sa.scale_mul_1H11, module.blocks.32.sa.q_bias, '\n"
                    " 'module.blocks.32.sa.v_bias, module.blocks.32.sa.proj.bias, module.blocks.32.ca.v_bias, module.blocks.32.ca.mat_q.bias, module.blocks.32.ca.proj.bias, module.blocks.32.ffn.fc1.bias, '\n"
                    " 'module.blocks.32.ffn.fc2.bias, module.blocks.32.ca_norm.bias, module.blocks.33.ada_gss, module.blocks.33.sa.scale_mul_1H11, module.blocks.33.sa.q_bias, module.blocks.33.sa.v_bias, '\n"
                    " 'module.blocks.33.sa.proj.bias, module.blocks.33.ca.v_bias, module.blocks.33.ca.mat_q.bias, module.blocks.33.ca.proj.bias, module.blocks.33.ffn.fc1.bias, module.blocks.33.ffn.fc2.bias, '\n"
                    " 'module.blocks.33.ca_norm.bias, module.blocks.34.ada_gss, module.blocks.34.sa.scale_mul_1H11, module.blocks.34.sa.q_bias, module.blocks.34.sa.v_bias, module.blocks.34.sa.proj.bias, '\n"
                    " 'module.blocks.34.ca.v_bias, module.blocks.34.ca.mat_q.bias, module.blocks.34.ca.proj.bias, module.blocks.34.ffn.fc1.bias, module.blocks.34.ffn.fc2.bias, module.blocks.34.ca_norm.bias, '\n"
                    " 'module.blocks.35.ada_gss, module.blocks.35.sa.scale_mul_1H11, module.blocks.35.sa.q_bias, module.blocks.35.sa.v_bias, module.blocks.35.sa.proj.bias, module.blocks.35.ca.v_bias, '\n"
                    " 'module.blocks.35.ca.mat_q.bias, module.blocks.35.ca.proj.bias, module.blocks.35.ffn.fc1.bias, module.blocks.35.ffn.fc2.bias, module.blocks.35.ca_norm.bias, module.blocks.36.ada_gss, '\n"
                    " 'module.blocks.36.sa.scale_mul_1H11, module.blocks.36.sa.q_bias, module.blocks.36.sa.v_bias, module.blocks.36.sa.proj.bias, module.blocks.36.ca.v_bias, module.blocks.36.ca.mat_q.bias, '\n"
                    " 'module.blocks.36.ca.proj.bias, module.blocks.36.ffn.fc1.bias, module.blocks.36.ffn.fc2.bias, module.blocks.36.ca_norm.bias, module.blocks.37.ada_gss, module.blocks.37.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.37.sa.q_bias, module.blocks.37.sa.v_bias, module.blocks.37.sa.proj.bias, module.blocks.37.ca.v_bias, module.blocks.37.ca.mat_q.bias, module.blocks.37.ca.proj.bias, '\n"
                    " 'module.blocks.37.ffn.fc1.bias, module.blocks.37.ffn.fc2.bias, module.blocks.37.ca_norm.bias, module.blocks.38.ada_gss, module.blocks.38.sa.scale_mul_1H11, module.blocks.38.sa.q_bias, '\n"
                    " 'module.blocks.38.sa.v_bias, module.blocks.38.sa.proj.bias, module.blocks.38.ca.v_bias, module.blocks.38.ca.mat_q.bias, module.blocks.38.ca.proj.bias, module.blocks.38.ffn.fc1.bias, '\n"
                    " 'module.blocks.38.ffn.fc2.bias, module.blocks.38.ca_norm.bias, module.blocks.39.ada_gss, module.blocks.39.sa.scale_mul_1H11, module.blocks.39.sa.q_bias, module.blocks.39.sa.v_bias, '\n"
                    " 'module.blocks.39.sa.proj.bias, module.blocks.39.ca.v_bias, module.blocks.39.ca.mat_q.bias, module.blocks.39.ca.proj.bias, module.blocks.39.ffn.fc1.bias, module.blocks.39.ffn.fc2.bias, '\n"
                    " 'module.blocks.39.ca_norm.bias, module.blocks.40.ada_gss, module.blocks.40.sa.scale_mul_1H11, module.blocks.40.sa.q_bias, module.blocks.40.sa.v_bias, module.blocks.40.sa.proj.bias, '\n"
                    " 'module.blocks.40.ca.v_bias, module.blocks.40.ca.mat_q.bias, module.blocks.40.ca.proj.bias, module.blocks.40.ffn.fc1.bias, module.blocks.40.ffn.fc2.bias, module.blocks.40.ca_norm.bias, '\n"
                    " 'module.blocks.41.ada_gss, module.blocks.41.sa.scale_mul_1H11, module.blocks.41.sa.q_bias, module.blocks.41.sa.v_bias, module.blocks.41.sa.proj.bias, module.blocks.41.ca.v_bias, '\n"
                    " 'module.blocks.41.ca.mat_q.bias, module.blocks.41.ca.proj.bias, module.blocks.41.ffn.fc1.bias, module.blocks.41.ffn.fc2.bias, module.blocks.41.ca_norm.bias, module.blocks.42.ada_gss, '\n"
                    " 'module.blocks.42.sa.scale_mul_1H11, module.blocks.42.sa.q_bias, module.blocks.42.sa.v_bias, module.blocks.42.sa.proj.bias, module.blocks.42.ca.v_bias, module.blocks.42.ca.mat_q.bias, '\n"
                    " 'module.blocks.42.ca.proj.bias, module.blocks.42.ffn.fc1.bias, module.blocks.42.ffn.fc2.bias, module.blocks.42.ca_norm.bias, module.blocks.43.ada_gss, module.blocks.43.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.43.sa.q_bias, module.blocks.43.sa.v_bias, module.blocks.43.sa.proj.bias, module.blocks.43.ca.v_bias, module.blocks.43.ca.mat_q.bias, module.blocks.43.ca.proj.bias, '\n"
                    " 'module.blocks.43.ffn.fc1.bias, module.blocks.43.ffn.fc2.bias, module.blocks.43.ca_norm.bias, module.blocks.44.ada_gss, module.blocks.44.sa.scale_mul_1H11, module.blocks.44.sa.q_bias, '\n"
                    " 'module.blocks.44.sa.v_bias, module.blocks.44.sa.proj.bias, module.blocks.44.ca.v_bias, module.blocks.44.ca.mat_q.bias, module.blocks.44.ca.proj.bias, module.blocks.44.ffn.fc1.bias, '\n"
                    " 'module.blocks.44.ffn.fc2.bias, module.blocks.44.ca_norm.bias, module.blocks.45.ada_gss, module.blocks.45.sa.scale_mul_1H11, module.blocks.45.sa.q_bias, module.blocks.45.sa.v_bias, '\n"
                    " 'module.blocks.45.sa.proj.bias, module.blocks.45.ca.v_bias, module.blocks.45.ca.mat_q.bias, module.blocks.45.ca.proj.bias, module.blocks.45.ffn.fc1.bias, module.blocks.45.ffn.fc2.bias, '\n"
                    " 'module.blocks.45.ca_norm.bias, module.blocks.46.ada_gss, module.blocks.46.sa.scale_mul_1H11, module.blocks.46.sa.q_bias, module.blocks.46.sa.v_bias, module.blocks.46.sa.proj.bias, '\n"
                    " 'module.blocks.46.ca.v_bias, module.blocks.46.ca.mat_q.bias, module.blocks.46.ca.proj.bias, module.blocks.46.ffn.fc1.bias, module.blocks.46.ffn.fc2.bias, module.blocks.46.ca_norm.bias, '\n"
                    " 'module.blocks.47.ada_gss, module.blocks.47.sa.scale_mul_1H11, module.blocks.47.sa.q_bias, module.blocks.47.sa.v_bias, module.blocks.47.sa.proj.bias, module.blocks.47.ca.v_bias, '\n"
                    " 'module.blocks.47.ca.mat_q.bias, module.blocks.47.ca.proj.bias, module.blocks.47.ffn.fc1.bias, module.blocks.47.ffn.fc2.bias, module.blocks.47.ca_norm.bias, module.blocks.48.ada_gss, '\n"
                    " 'module.blocks.48.sa.scale_mul_1H11, module.blocks.48.sa.q_bias, module.blocks.48.sa.v_bias, module.blocks.48.sa.proj.bias, module.blocks.48.ca.v_bias, module.blocks.48.ca.mat_q.bias, '\n"
                    " 'module.blocks.48.ca.proj.bias, module.blocks.48.ffn.fc1.bias, module.blocks.48.ffn.fc2.bias, module.blocks.48.ca_norm.bias, module.blocks.49.ada_gss, module.blocks.49.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.49.sa.q_bias, module.blocks.49.sa.v_bias, module.blocks.49.sa.proj.bias, module.blocks.49.ca.v_bias, module.blocks.49.ca.mat_q.bias, module.blocks.49.ca.proj.bias, '\n"
                    " 'module.blocks.49.ffn.fc1.bias, module.blocks.49.ffn.fc2.bias, module.blocks.49.ca_norm.bias, module.blocks.50.ada_gss, module.blocks.50.sa.scale_mul_1H11, module.blocks.50.sa.q_bias, '\n"
                    " 'module.blocks.50.sa.v_bias, module.blocks.50.sa.proj.bias, module.blocks.50.ca.v_bias, module.blocks.50.ca.mat_q.bias, module.blocks.50.ca.proj.bias, module.blocks.50.ffn.fc1.bias, '\n"
                    " 'module.blocks.50.ffn.fc2.bias, module.blocks.50.ca_norm.bias, module.blocks.51.ada_gss, module.blocks.51.sa.scale_mul_1H11, module.blocks.51.sa.q_bias, module.blocks.51.sa.v_bias, '\n"
                    " 'module.blocks.51.sa.proj.bias, module.blocks.51.ca.v_bias, module.blocks.51.ca.mat_q.bias, module.blocks.51.ca.proj.bias, module.blocks.51.ffn.fc1.bias, module.blocks.51.ffn.fc2.bias, '\n"
                    " 'module.blocks.51.ca_norm.bias, module.blocks.52.ada_gss, module.blocks.52.sa.scale_mul_1H11, module.blocks.52.sa.q_bias, module.blocks.52.sa.v_bias, module.blocks.52.sa.proj.bias, '\n"
                    " 'module.blocks.52.ca.v_bias, module.blocks.52.ca.mat_q.bias, module.blocks.52.ca.proj.bias, module.blocks.52.ffn.fc1.bias, module.blocks.52.ffn.fc2.bias, module.blocks.52.ca_norm.bias, '\n"
                    " 'module.blocks.53.ada_gss, module.blocks.53.sa.scale_mul_1H11, module.blocks.53.sa.q_bias, module.blocks.53.sa.v_bias, module.blocks.53.sa.proj.bias, module.blocks.53.ca.v_bias, '\n"
                    " 'module.blocks.53.ca.mat_q.bias, module.blocks.53.ca.proj.bias, module.blocks.53.ffn.fc1.bias, module.blocks.53.ffn.fc2.bias, module.blocks.53.ca_norm.bias, module.blocks.54.ada_gss, '\n"
                    " 'module.blocks.54.sa.scale_mul_1H11, module.blocks.54.sa.q_bias, module.blocks.54.sa.v_bias, module.blocks.54.sa.proj.bias, module.blocks.54.ca.v_bias, module.blocks.54.ca.mat_q.bias, '\n"
                    " 'module.blocks.54.ca.proj.bias, module.blocks.54.ffn.fc1.bias, module.blocks.54.ffn.fc2.bias, module.blocks.54.ca_norm.bias, module.blocks.55.ada_gss, module.blocks.55.sa.scale_mul_1H11, '\n"
                    " 'module.blocks.55.sa.q_bias, module.blocks.55.sa.v_bias, module.blocks.55.sa.proj.bias, module.blocks.55.ca.v_bias, module.blocks.55.ca.mat_q.bias, module.blocks.55.ca.proj.bias, '\n"
                    " 'module.blocks.55.ffn.fc1.bias, module.blocks.55.ffn.fc2.bias, module.blocks.55.ca_norm.bias')",
          'wd_sc': 0.0}}

[09-07 00:11:21] (nity/utils/lr_control.py, line 116)=> [get_param_groups][rank0] type(model).__name__='NullDDP' count=1085, numel=8573892016
[09-07 00:11:21] (nity/utils/lr_control.py, line 117)=> 
[09-07 00:11:21] (inity_circuit_breaker.py, line 343)=> [vgpt] optim=functools.partial(<class 'torch.optim.adamw.AdamW'>, betas=(0.9, 0.97), fused=True), opt_kw={'lr': 9.375e-05, 'weight_decay': 0}

[09-07 00:11:21] (inity_circuit_breaker.py, line 348)=> Loading T5 from google/flan-t5-xl...
[09-07 00:11:22] (_original/src/trainer.py, line  65)=> self.reweight_loss_by_scale: 1
[09-07 00:11:22] (inity_circuit_breaker.py, line 171)=> global bs=4, local bs=4
[09-07 00:11:22] (inity_circuit_breaker.py, line 172)=> initial args:
{
  local_out_path      : ./local_output
  data_path           : ./data
  bed                 : ./checkpoints
  vae_ckpt            : /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
  exp_name            : experiment
  ds                  : oi
  model               : infinity_8b
  short_cap_prob      : 0.2
  project_name        : infinity_circuit_breaker
  tf32                : True
  auto_resume         : True
  rush_resume         : 
  nowd                : 1
  enable_hybrid_shard : False
  inner_shard_degree  : 1
  zero                : 2
  buck                : chunk
  fsdp_orig           : True
  enable_checkpointing: full-block
  pad_to_multiplier   : 128
  log_every_iter      : False
  checkpoint_type     : torch
  seed                : None
  rand                : True
  task_id             : 123
  trial_id            : 123
  robust_run_id       : 0
  ckpt_trials         : []
  real_trial_id       : 123
  chunk_nodes         : 0
  is_master_node      : None
  log_txt_path        : ./local_output/log.txt
  t5_path             : google/flan-t5-xl
  online_t5           : True
  sdpa_mem            : True
  tfast               : 0
  model_alias         : 8b
  rms                 : False
  aln                 : 0.001
  alng                : 5e-06
  saln                : True
  haln                : True
  nm0                 : False
  tau                 : 1
  cos                 : True
  swi                 : False
  dp                  : -1
  drop                : 0.0
  hd                  : 0
  ca_gamma            : -1
  diva                : 1
  hd0                 : 0.02
  dec                 : 1
  cum                 : 3
  rwe                 : False
  tp                  : 0.0
  tk                  : 0.0
  tini                : 0.010416666666666666
  cfg                 : 0.1
  rand_uncond         : False
  ema                 : 0.9999
  tema                : 0
  fp16                : 2
  fuse                : False
  fused_norm          : False
  flash               : False
  xen                 : False
  use_flex_attn       : True
  stable              : False
  gblr                : 0.0001
  dblr                : 0.0001
  tblr                : 0.006
  glr                 : 1.5625e-06
  dlr                 : 1.5625e-06
  tlr                 : 9.375e-05
  gwd                 : 0.005
  dwd                 : 0.0005
  twd                 : 0.005
  gwde                : 0.005
  dwde                : 0.0005
  twde                : 0.005
  ls                  : 0.0
  lz                  : 0.0
  eq                  : 0
  ep                  : 100
  wp                  : 1e-08
  wp0                 : 0.005
  wpe                 : 1.0
  sche                : lin0
  log_freq            : 50
  gclip               : 6.0
  dclip               : 6.0
  tclip               : 5.0
  cdec                : False
  opt                 : adamw
  ada                 : 0.9_0.97
  dada                : 0.9_0.97
  oeps                : 0
  afuse               : True
  pn                  : 0.06M
  scale_schedule      : None
  patch_size          : None
  resos               : None
  data_load_reso      : None
  workers             : 4
  lbs                 : 4
  bs                  : 4
  batch_size          : 4
  glb_batch_size      : 4
  ac                  : 1
  r_accu              : 1.0
  norm_eps            : 1e-06
  tlen                : 512
  Ct5                 : 2048
  use_bit_label       : 1
  bitloss_type        : mean
  dynamic_resolution_across_gpus: 1
  enable_dynamic_length_prompt: 1
  use_streaming_dataset: 1
  iterable_data_buffersize: 30000
  save_model_iters_freq: 100
  noise_apply_layers  : 13
  noise_apply_strength: 0.3
  noise_apply_requant : 1
  rope2d_each_sa_layer: 1
  rope2d_normalized_by_hw: 2
  use_fsdp_model_ema  : 0
  add_lvl_embeding_only_first_block: 1
  reweight_loss_by_scale: 1
  always_training_scales: 100
  vae_type            : 14
  fake_vae_input      : False
  model_init_device   : cuda
  prefetch_factor     : 2
  apply_spatial_patchify: 1
  debug_bsc           : 0
  task_type           : t2i
  target_layers       : [10, 12, 14, 16, 18, 20]
  transform_layers    : [10, 12, 14, 16, 18, 20]
  lorra_alpha         : 5.0
  trainsets           : ['AlpacaSupervisedDataset', 'HarmfulDataset']
  valsets             : ['AlpacaSupervisedDataset', 'HarmfulDataset']
  adv_string          : Sure here's
  full_layers         : False
  harmful_prompts_path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
  sanitized_prompts_path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
  validation_ratio    : 0.1
  category            : hate
  lora_r              : 8
  lora_alpha          : 16
  lora_dropout        : 0.05
  lora_target_modules : ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']
  lora_weight_path    : 
  lora_bias           : none
  q_lora              : False
  model_name_or_path  : meta-llama/Llama-2-7b-chat-hf
  adapter_name_or_path: 
  use_lora            : True
  cache_dir           : 
  optim               : adamw_torch
  model_max_length    : 512
  grouped_to_max_length: False
  use_refusal_retain  : True
  sc_train_subset     : ['']
  log_every           : 10
  sc_train_seq_type   : all_text
  coeff_schedule      : linear_converge
  sc_loss_type        : orig_act_dotprod
  branch              : master
  commit_id           : 
  commit_msg          : 
  cmd                 : --bed ./checkpoints --local_out_path ./local_output --project_name infinity_circuit_breaker --exp_name experiment --ep 100 --opt adamw --cum 3 --sche lin0 --fp16 2 --ada 0.9_0.97 --tini -1 --tclip 5 --flash 0 --alng 5e-06 --saln 1 --cos 1 --enable_checkpointing full-block --tblr 6e-3 --pn 0.06M --lbs 4 --workers 4 --short_cap_prob 0.2 --online_t5 1 --use_streaming_dataset 1 --iterable_data_buffersize 30000 --Ct5 2048 --t5_path google/flan-t5-xl --vae_type 14 --wp 0.00000001 --wpe 1 --dynamic_resolution_across_gpus 1 --enable_dynamic_length_prompt 1 --reweight_loss_by_scale 1 --add_lvl_embeding_only_first_block 1 --rope2d_each_sa_layer 1 --rope2d_normalized_by_hw 2 --use_fsdp_model_ema 0 --always_training_scales 100 --use_bit_label 1 --zero 2 --save_model_iters_freq 100 --log_freq 50 --checkpoint_type torch --prefetch_factor 2 --noise_apply_strength 0.3 --noise_apply_layers 13 --apply_spatial_patchify 1 --use_flex_attn True --pad 128 --data_path ./data --data_load_reso 512 --tlen 512 --workers 4 --prefetch_factor 2 --harmful_prompts_path /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts --sanitized_prompts_path /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts --validation_ratio 0.1 --category hate --target_layers 10,12,14,16,18,20 --transform_layers 10,12,14,16,18,20 --lorra_alpha 5.0 --trainsets AlpacaSupervisedDataset#HarmfulDataset --valsets AlpacaSupervisedDataset#HarmfulDataset --adv_string Sure here's --full_layers False --lora_r 8 --lora_alpha 16 --lora_dropout 0.05 --lora_target_modules q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj --lora_weight_path  --lora_bias none --q_lora False --model_name_or_path meta-llama/Llama-2-7b-chat-hf --adapter_name_or_path  --use_lora True --cache_dir  --optim adamw_torch --model_max_length 512 --grouped_to_max_length False --use_refusal_retain True --sc_train_subset  --log_every 10 --sc_train_seq_type all_text --coeff_schedule linear_converge --sc_loss_type orig_act_dotprod
  tag                 : UK
  acc_all             : None
  acc_real            : None
  acc_fake            : None
  last_Lnll           : None
  last_L1             : None
  last_Ld             : None
  last_wei_g          : None
  grad_boom           : None
  diff                : None
  diffs               : 
  diffs_ema           : None
  ca_performance      : 
  cur_phase           : 
  cur_it              : 
  cur_ep              : 
  remain_time         : 
  finish_time         : 
  iter_speed          : None
  img_per_day         : None
  max_nvidia_smi      : 0
  max_memory_allocated: None
  max_memory_reserved : None
  num_alloc_retries   : None
  MFU                 : None
  HFU                 : None
  dbg_modified        : False
  dbg_ks              : False
  dbg_ks_last         : None
  dbg                 : False
  ks                  : False
  nodata              : False
  nodata_tlen         : 320
  nova                : False
  prof                : 0
  prof_freq           : 50
  tos_profiler_file_prefix: vgpt_default/
  profall             : 0
  v_seed              : 0
  g_seed              : 0
}

[09-07 00:11:22] (inity_circuit_breaker.py, line 182)=> start_it=0, iters_train=100
[09-07 00:11:23] (inity_circuit_breaker.py, line 466)=> [PT info]  from ep0 it0, acc_str: [no acc str], diffs: ,    =======>  bed: ./checkpoints  <=======

[09-07 00:11:25] (twise_self_correction.py, line  36)=> [dbg] raw_features: torch.Size([2, 14, 64, 64])
[09-07 00:11:25] (twise_self_correction.py, line  41)=> [dbg] target last scale: (1, 32, 32)
[09-07 00:11:25] (twise_self_correction.py, line  46)=> [dbg] Resizing codes_out from torch.Size([2, 14, 1, 64, 64]) to match target size (1, 32, 32)
[09-07 00:11:25] (twise_self_correction.py, line  48)=> [dbg] Resized codes_out shape: torch.Size([2, 14, 1, 32, 32])
[09-07 00:11:25] (inity/models/infinity.py, line 400)=> [Infinity] x_BLC shape: torch.Size([2, 521, 3072]), cond_BD_or_gss shape: torch.Size([2, 1, 6, 3072]), ca_kv shape: torch.Size([114, 3072]), scale_schedule: [(1, 1, 1), (1, 2, 2), (1, 4, 4), (1, 6, 6), (1, 8, 8), (1, 12, 12), (1, 16, 16)]







=======================================================   RESTART [09-07 10:27:14]   =======================================================
[09-07 10:27:14] (inity_circuit_breaker.py, line 718)=> ============================================================
[09-07 10:27:14] (inity_circuit_breaker.py, line 719)=> INFINITY CIRCUIT BREAKER TRAINING - SELECTIVE LAYER FINE-TUNING
[09-07 10:27:14] (inity_circuit_breaker.py, line 720)=> Circuit Breaker Alpha: 0.1
[09-07 10:27:14] (inity_circuit_breaker.py, line 721)=> Circuit Breaker Target Layers: 0,1,2,3,4,5
[09-07 10:27:14] (inity_circuit_breaker.py, line 722)=> Circuit Breaker Enabled: True
[09-07 10:27:14] (inity_circuit_breaker.py, line 723)=> Number of Examples: 1000
[09-07 10:27:14] (inity_circuit_breaker.py, line 724)=> Model Name/Path: meta-llama/Llama-2-7b-chat-hf
[09-07 10:27:14] (inity_circuit_breaker.py, line 725)=> Harmful Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts
[09-07 10:27:14] (inity_circuit_breaker.py, line 726)=> Sanitized Prompts Path: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts
[09-07 10:27:14] (inity_circuit_breaker.py, line 727)=> Validation Ratio: 0.1
[09-07 10:27:14] (inity_circuit_breaker.py, line 728)=> Category: hate
[09-07 10:27:14] (inity_circuit_breaker.py, line 729)=> Batch Size: 4
[09-07 10:27:14] (inity_circuit_breaker.py, line 730)=> Workers: 4
[09-07 10:27:14] (inity_circuit_breaker.py, line 731)=> Device: cuda:0
[09-07 10:27:14] (inity_circuit_breaker.py, line 732)=> Model: infinity_8b
[09-07 10:27:14] (inity_circuit_breaker.py, line 733)=> VAE Checkpoint: /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-07 10:27:14] (inity_circuit_breaker.py, line 734)=> Rush Resume: 
[09-07 10:27:14] (inity_circuit_breaker.py, line 735)=> Selective Layers: 0,1,2,3,4,5
[09-07 10:27:14] (inity_circuit_breaker.py, line 736)=> ============================================================
[09-07 10:27:14] (inity_circuit_breaker.py, line 377)=> Using data paths: harmful=/home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts, sanitized=/home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts, category=hate
[09-07 10:27:14] (rain_dataset_infinity.py, line 131)=> Found sanitized prompts file: /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-07 10:27:14] (rain_dataset_infinity.py, line 140)=> Loading sanitized prompts from /home/gs285/VAR/my_model/prompt_generation/output/sanitized_prompts/hate_sanitized_prompts.json
[09-07 10:27:14] (rain_dataset_infinity.py, line 205)=> Split 1000 prompts: 900 retain, 100 validation
[09-07 10:27:14] (rain_dataset_infinity.py, line 236)=> Found harmful prompts file: /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-07 10:27:14] (rain_dataset_infinity.py, line 245)=> Loading harmful prompts from /home/gs285/VAR/my_model/prompt_generation/output/harmful_prompts/hate_prompts.json
[09-07 10:27:15] (rain_dataset_infinity.py, line  84)=> Loaded 900 retain examples
[09-07 10:27:15] (rain_dataset_infinity.py, line  85)=> Loaded 1000 circuit breaker examples
[09-07 10:27:15] (rain_dataset_infinity.py, line  86)=> Loaded 100 validation examples
[09-07 10:27:15] (rain_dataset_infinity.py, line  87)=> num_replicas: 1, rank: 0, dataloader_workers: 2, seed:0
[09-07 10:27:15] (inity_circuit_breaker.py, line 395)=> args.batch_size=4, vbs=6
[09-07 10:27:15] (inity_circuit_breaker.py, line 418)=> len(dataloader): 100, len(dataset): 100
[09-07 10:27:15] (inity_circuit_breaker.py, line 420)=> total_samples: 200
[09-07 10:27:15] (inity_circuit_breaker.py, line 421)=> [dataloader] batch_size=4, iters_train=100, type(train_set)=InfinityCircuitBreakerDataset
[09-07 10:27:15] (inity_circuit_breaker.py, line 140)=> raw_keys=['1:1', '4:3', '3:4', '16:9', '9:16']
[09-07 10:27:15] (inity_circuit_breaker.py, line 142)=> train_h_div_w_list=[1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625]
[09-07 10:27:15] (inity_circuit_breaker.py, line 146)=> Load vae form /home/gs285/VAR/my_model/weights/infinity_vae_d56_f8_14_patchify.pth
[09-07 10:27:17] (l/infinity/utils/load.py, line  72)=> [create gpt_wo_ddp] constructor kw={'pretrained': False, 'global_pool': '', 'text_channels': 2048, 'text_maxlen': 512, 'norm_eps': 1e-06, 'rms_norm': False, 'shared_aln': True, 'head_aln': True, 'cond_drop_rate': 0.1, 'rand_uncond': False, 'drop_rate': 0.0, 'cross_attn_layer_scale': -1, 'nm0': False, 'tau': 1, 'cos_attn': True, 'swiglu': False, 'raw_scale_schedule': None, 'head_depth': 1, 'top_p': 0.0, 'top_k': 0.0, 'customized_flash_attn': False, 'fused_mlp': False, 'fused_norm': False, 'checkpointing': 'full-block', 'pad_to_multiplier': 128, 'use_flex_attn': True, 'batch_size': 4, 'add_lvl_embeding_only_first_block': 1, 'use_bit_label': 1, 'rope2d_each_sa_layer': 1, 'rope2d_normalized_by_hw': 2, 'pn': '0.06M', 'train_h_div_w_list': [1.0, 1.3333333333333333, 0.75, 1.7777777777777777, 0.5625], 'always_training_scales': 100, 'apply_spatial_patchify': 1}

[09-07 10:27:17] (l/infinity/utils/load.py, line  76)=> model_str='infinity_8b'
[09-07 10:27:17] (inity/models/infinity.py, line 136)=> self.codebook_dim: 56, self.add_lvl_embeding_only_first_block: 1,             self.use_bit_label: 1, self.rope2d_each_sa_layer: 1, self.rope2d_normalized_by_hw: 2
[09-07 10:27:17] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.0 scales: 7 ======
[09-07 10:27:20] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.3333333333333333 scales: 7 ======
[09-07 10:27:21] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.75 scales: 7 ======
[09-07 10:27:21] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 1.7777777777777777 scales: 7 ======
[09-07 10:27:21] (inity/models/infinity.py, line 307)=> ====== apply flex attn hdivw: 0.5625 scales: 7 ======
[09-07 10:27:21] (inity/models/infinity.py, line 277)=> self.num_blocks_in_a_chunk=56, depth=56, block_chunks=1
[09-07 10:27:21] (inity/models/infinity.py, line 285)=> 
[constructor]  ==== customized_flash_attn=False (using_flash=0/56), fused_mlp=False (fused_mlp=0/56) ==== 
    [Infinity config ] embed_dim=3072, num_heads=24, depth=56, mlp_ratio=4, swiglu=False num_blocks_in_a_chunk=56
    [drop ratios] drop_rate=0.0, drop_path_rate=0.2 (tensor([0.0000, 0.0036, 0.0073, 0.0109, 0.0145, 0.0182, 0.0218, 0.0255, 0.0291,
        0.0327, 0.0364, 0.0400, 0.0436, 0.0473, 0.0509, 0.0545, 0.0582, 0.0618,
        0.0655, 0.0691, 0.0727, 0.0764, 0.0800, 0.0836, 0.0873, 0.0909, 0.0945,
        0.0982, 0.1018, 0.1055, 0.1091, 0.1127, 0.1164, 0.1200, 0.1236, 0.1273,
        0.1309, 0.1345, 0.1382, 0.1418, 0.1455, 0.1491, 0.1527, 0.1564, 0.1600,
        0.1636, 0.1673, 0.1709, 0.1745, 0.1782, 0.1818, 0.1855, 0.1891, 0.1927,
        0.1964, 0.2000]))

